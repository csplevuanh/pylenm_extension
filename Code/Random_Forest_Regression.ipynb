{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olxOIIAvUIS1"
      },
      "source": [
        "# **SET UP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2JbZiOxdYE9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.colors as mcolors\n",
        "import os\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import t, linregress\n",
        "from matplotlib.dates import date2num, num2date\n",
        "import datetime\n",
        "from datetime import datetime, date, timedelta\n",
        "import folium\n",
        "import branca.colormap\n",
        "from geopy.geocoders import Nominatim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxqxoBV4LQb4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGr_NB3ZcK3Y"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyOVU1hv7iuo"
      },
      "source": [
        "# **DataAnalyzer class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77cuKlNQY6Cq"
      },
      "outputs": [],
      "source": [
        "class DataAnalyzer:\n",
        "    def __init__(self, data_or_url):\n",
        "        if isinstance(data_or_url, str):  # Assuming file path is a string\n",
        "            self.data_url = data_or_url\n",
        "            self.data = self.load_data()\n",
        "        elif isinstance(data_or_url, pd.DataFrame):  # Assuming DataFrame\n",
        "            self.data = data_or_url\n",
        "        else:\n",
        "            raise ValueError(\"Input must be a file path or a pandas DataFrame\")\n",
        "\n",
        "    def load_data(self):\n",
        "        file_extension = self.data_url.split('.')[-1]\n",
        "        if file_extension == 'csv':\n",
        "            return pd.read_csv(self.data_url)\n",
        "        elif file_extension in ['xlsx', 'xlsm', 'xltx', 'xltm']:  # Add more excel file extensions if needed\n",
        "            return pd.read_excel(self.data_url)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "    def print_column_names(self):\n",
        "        print(\"Column names in the dataset:\", self.data.columns.tolist())\n",
        "\n",
        "    def preprocess_data(self, well_name_column, analyte_name_column, well_name, analyte_name):\n",
        "        filtered_data = self.data[(self.data[well_name_column] == well_name) & (self.data[analyte_name_column] == analyte_name)]\n",
        "        filtered_data = filtered_data.dropna(subset=['COLLECTION_DATE', 'RESULT'])\n",
        "        filtered_data['COLLECTION_DATE'] = pd.to_datetime(filtered_data['COLLECTION_DATE']).dt.date\n",
        "        filtered_data['RESULT_LOG'] = np.log10(filtered_data['RESULT'])\n",
        "        return filtered_data\n",
        "\n",
        "    def remove_outliers(self, data, result_column='RESULT_LOG', z_threshold=4):\n",
        "        z = np.abs(stats.zscore(data[result_column]))\n",
        "        return data[(z < z_threshold)]\n",
        "\n",
        "    def predict_time_at_MCL(self, slope, intercept, log_MCL, earliest_date):\n",
        "        if slope >= 0:\n",
        "            return None\n",
        "        time_at_MCL = (log_MCL - intercept) / slope\n",
        "        try:\n",
        "            time_at_MCL_date = num2date(time_at_MCL).replace(tzinfo=None).date()\n",
        "            if earliest_date.year < 1 or time_at_MCL_date.year >= 10000 or time_at_MCL_date <= earliest_date:\n",
        "                return None\n",
        "            return time_at_MCL_date\n",
        "        except ValueError:\n",
        "            return None\n",
        "\n",
        "    def adjust_date_based_on_confidence(self, base_date, slope, confidence_interval, days_per_unit):\n",
        "        if base_date is None or slope == 0:\n",
        "            return None, None, None\n",
        "        # Ensure base_date is a date object, not a string or other type\n",
        "        days_adjustment = confidence_interval / abs(slope) * days_per_unit\n",
        "        av_date = base_date - timedelta(days=days_adjustment) if base_date else None\n",
        "        upper_date = base_date + timedelta(days=days_adjustment) if base_date else None\n",
        "        # Calculate lower_date only if both av_date and upper_date are not None\n",
        "        lower_date = av_date + (upper_date - av_date) / 2 if av_date and upper_date else None\n",
        "        return av_date, upper_date, lower_date\n",
        "\n",
        "    def generate_analyte_summary(self, analyte_name_column, analyte_name, well_name_column, MCL):\n",
        "        unique_wells = self.data[self.data[analyte_name_column] == analyte_name][well_name_column].unique()\n",
        "        summary = []\n",
        "\n",
        "        for well in unique_wells:\n",
        "            query = self.preprocess_data(well_name_column, analyte_name_column, well, analyte_name)\n",
        "            if query.empty or len(query['COLLECTION_DATE'].unique()) < 2:\n",
        "                continue\n",
        "\n",
        "            x = date2num(query['COLLECTION_DATE'])\n",
        "            y = query['RESULT_LOG']\n",
        "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "\n",
        "            if slope > 0:\n",
        "                trend = 'Increase'\n",
        "            elif slope < 0:\n",
        "                trend = 'Decrease'\n",
        "            else:\n",
        "                trend = 'Neutral'\n",
        "\n",
        "            if trend == 'Neutral':\n",
        "                continue\n",
        "\n",
        "            # Initialize your variables\n",
        "            time_at_MCL = None\n",
        "            total_decrease_duration = None  # This will represent the new av_time@MCL\n",
        "            upper_date = None\n",
        "            lower_date = None\n",
        "            min_time_at_MCL = None\n",
        "            max_time_at_MCL = None\n",
        "            mean_time_at_MCL = None\n",
        "\n",
        "            if trend == 'Decrease':\n",
        "                log_MCL = np.log10(MCL)\n",
        "                time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, min(query['COLLECTION_DATE']))\n",
        "                min_time_at_MCL = time_at_MCL if isinstance(time_at_MCL, date) else None\n",
        "\n",
        "                earliest_measurement = min(query['COLLECTION_DATE'])\n",
        "                if min_time_at_MCL:\n",
        "                    total_decrease_duration = (min_time_at_MCL - earliest_measurement).days  # Duration in days\n",
        "\n",
        "                max_concentration_index = query['RESULT'].idxmax()\n",
        "                max_time_at_MCL = query.loc[max_concentration_index, 'COLLECTION_DATE']\n",
        "\n",
        "                if min_time_at_MCL and max_time_at_MCL:\n",
        "                    avg_timestamp = (date2num(max_time_at_MCL) + date2num(min_time_at_MCL)) / 2\n",
        "                    mean_time_at_MCL = num2date(avg_timestamp).date()\n",
        "\n",
        "            # Calculate the confidence interval adjustments only if you have time_at_MCL and it's a date\n",
        "            df = len(x) - 2\n",
        "            ci_slope = std_err * t.ppf(1 - 0.025, df) if df > 0 else 0\n",
        "            if time_at_MCL and isinstance(time_at_MCL, date):\n",
        "                _, upper_date, lower_date = self.adjust_date_based_on_confidence(time_at_MCL, slope, ci_slope, 365.25)\n",
        "\n",
        "            summary.append({\n",
        "                'well_name': well,\n",
        "                'slope': slope,\n",
        "                'trend': trend,\n",
        "                'time@MCL': time_at_MCL,\n",
        "                'total_decrease_duration': total_decrease_duration,  # Renamed from av_time@MCL\n",
        "                'upper_time@MCL': upper_date,\n",
        "                'lower_time@MCL': lower_date,\n",
        "                'min_time@MCL': min_time_at_MCL,\n",
        "                'max_time@MCL': max_time_at_MCL,\n",
        "                'mean_time_at_MCL': mean_time_at_MCL,\n",
        "                'slope_confidence_interval': ci_slope,\n",
        "                'intercept_confidence_interval': intercept if slope else None,\n",
        "            })\n",
        "\n",
        "        summary_df = pd.DataFrame(summary)\n",
        "        return summary_df\n",
        "\n",
        "    def compile_results(self, well_analyte_pairs, well_name_column, analyte_name_column, MCL):\n",
        "        results = []\n",
        "        for well_name, analyte_name in well_analyte_pairs:\n",
        "            time_at_MCL = None\n",
        "            query = self.preprocess_data(well_name_column, analyte_name_column, well_name, analyte_name)\n",
        "            if query.empty:\n",
        "                continue\n",
        "\n",
        "            query = self.remove_outliers(query)\n",
        "            x = date2num(query['COLLECTION_DATE'])\n",
        "            y = query['RESULT_LOG']\n",
        "\n",
        "            if len(x) < 2:\n",
        "                continue\n",
        "\n",
        "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "            df = len(x) - 2\n",
        "            tinv = lambda p, df: abs(t.ppf(p/2, df))\n",
        "            ts = tinv(0.05, df)\n",
        "\n",
        "            trend = \"Increase\" if slope > 0 else \"Decrease\" if slope < 0 else \"Neutral\"\n",
        "            log_MCL = np.log10(MCL) if MCL else None\n",
        "            earliest_date = min(query['COLLECTION_DATE'])\n",
        "            time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, earliest_date) if MCL else None\n",
        "\n",
        "            results.append({\n",
        "                'well_name': well_name,\n",
        "                'slope': slope,\n",
        "                'trend': trend,\n",
        "                'time@MCL': time_at_MCL,\n",
        "                'slope_confidence_interval': ts * std_err,\n",
        "                'intercept_confidence_interval': ts * intercept\n",
        "            })\n",
        "\n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_df.to_csv('time2MCL_results.csv', index=False)\n",
        "        return results_df\n",
        "\n",
        "    def plot_MCL(self, well_name_column, analyte_name_column, well_name, analyte_name, year_interval=5, save_dir='plot_MCL', MCL=None, extrapolate_until_year=2025):\n",
        "        query = self.preprocess_data(well_name_column, analyte_name_column, well_name, analyte_name)\n",
        "        if query.empty:\n",
        "            print(f'No results found for {well_name} and {analyte_name}')\n",
        "            return\n",
        "\n",
        "        query = self.remove_outliers(query, result_column='RESULT_LOG')\n",
        "        x = date2num(query['COLLECTION_DATE'])\n",
        "        y = query['RESULT_LOG']\n",
        "\n",
        "        if len(x) < 2:  # Not enough data points to perform regression\n",
        "            print(\"Not enough data for regression.\")\n",
        "            return\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "        df = len(x) - 2  # degrees of freedom for t-distribution\n",
        "        tinv = lambda p, df: abs(stats.t.ppf(p/2, df))  # two-sided t-test\n",
        "        ts = tinv(0.05, df)  # 95% confidence interval multiplier\n",
        "        slope_confidence_interval = ts * std_err\n",
        "\n",
        "        log_MCL = np.log10(MCL) if MCL else None\n",
        "        time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, min(query['COLLECTION_DATE'])) if MCL and slope < 0 else 'Not available'\n",
        "\n",
        "        # Prepare for extrapolation\n",
        "        extrapolate_date = np.datetime64(f'{extrapolate_until_year}-12-31')\n",
        "        x_extrapolate = np.linspace(x.min(), date2num(extrapolate_date), 100)\n",
        "        y_extrapolate = slope * x_extrapolate + intercept\n",
        "\n",
        "        # Categorize and label the trend based on the slope\n",
        "        trend_label = 'Increase (+)' if slope > 0 else 'Decrease (-)' if slope < 0 else 'Neutral (/)'\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(query['COLLECTION_DATE'], y, label='Data Points')\n",
        "        for i, point in query.iterrows():\n",
        "            plt.text(point['COLLECTION_DATE'], point['RESULT_LOG'], str(i), fontsize=6, ha='center', va='center')\n",
        "        plt.plot(num2date(x_extrapolate), y_extrapolate, 'r--', label=f'Extrapolated Regression ({trend_label})')\n",
        "\n",
        "        if log_MCL is not None:\n",
        "            plt.axhline(y=log_MCL, color='green', linestyle='--', label=f'MCL (log10): {log_MCL:.2f}')\n",
        "\n",
        "        plt.figtext(0.15, 0.05, f\"Slope: {slope:.2e} +/- {slope_confidence_interval:.2e}\\nTime@MCL: {time_at_MCL}\", fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.xlabel('Collection Date')\n",
        "        plt.ylabel('Log-Concentration (pCi/mL)')\n",
        "        plt.title(f'{well_name} - {analyte_name}')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(year_interval))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.gcf().autofmt_xdate()\n",
        "\n",
        "        plt.xlim([num2date(x.min()), num2date(x_extrapolate.max())])\n",
        "\n",
        "        figure_path = f'{save_dir}/{well_name}-{analyte_name}.png'\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        plt.savefig(figure_path)\n",
        "        #plt.close()  # Close the plot after saving to avoid displaying it\n",
        "        return figure_path\n",
        "\n",
        "    def generate_analysis_dataframe(self, analyte_name_column, well_name_column, analyte_name, MCL):\n",
        "        analyte_summary_df = self.generate_analyte_summary(analyte_name_column, analyte_name, well_name_column, MCL)\n",
        "        analysis_data = []\n",
        "\n",
        "        for index, row in analyte_summary_df.iterrows():\n",
        "            if row['trend'] == 'Neutral':\n",
        "                continue\n",
        "\n",
        "            plot_path = self.plot_MCL(well_name_column, analyte_name_column, row['well_name'], analyte_name, year_interval=5, save_dir='plot_MCL', MCL=MCL)\n",
        "\n",
        "            analysis_data.append({\n",
        "                'analyte_name': analyte_name,\n",
        "                'well_name': row['well_name'],\n",
        "                'slope': row['slope'],\n",
        "                'trend': row['trend'],\n",
        "                'total_decrease_duration': row['total_decrease_duration'],\n",
        "                'upper_time@MCL': row['upper_time@MCL'],\n",
        "                'lower_time@MCL': row['lower_time@MCL'],\n",
        "                'path_to_figure': plot_path\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(analysis_data)\n",
        "\n",
        "    def generate_decreasing_well_list(self, analyte_name_column, analyte_name, well_name_column, MCL):\n",
        "        analyte_summary_df = self.generate_analyte_summary(analyte_name_column, analyte_name, well_name_column, MCL)\n",
        "        decreasing_well_list = analyte_summary_df[analyte_summary_df['trend'] == 'Decrease']['well_name'].tolist()\n",
        "        return decreasing_well_list\n",
        "\n",
        "    def extract_location_data(self, well_name_column, decreasing_well_list):\n",
        "        location_data = self.data[self.data[well_name_column].isin(decreasing_well_list)]\n",
        "        return location_data\n",
        "\n",
        "    def forecast_average_concentration(self, analyte_name_column, well_name_column, year, analyte_name, MCL):\n",
        "        decreasing_wells = self.generate_decreasing_well_list(analyte_name_column, analyte_name, well_name_column, MCL)\n",
        "\n",
        "        forecasted_concentration = []\n",
        "\n",
        "        unique_wells = self.data[self.data[well_name_column].isin(decreasing_wells)][well_name_column].unique()\n",
        "\n",
        "        for well in unique_wells:\n",
        "            query = self.preprocess_data(well_name_column, analyte_name_column, well, analyte_name)\n",
        "            if query.empty or len(query['COLLECTION_DATE'].unique()) < 2:\n",
        "                continue\n",
        "\n",
        "            x = date2num(query['COLLECTION_DATE'])\n",
        "            y = query['RESULT_LOG']\n",
        "\n",
        "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "            forecasted_log_concentration = slope * date2num(pd.to_datetime(f'{year}-01-01')) + intercept\n",
        "\n",
        "            # Create the custom key for the predicted concentration\n",
        "            concentration_key = f\"concentration_of_{analyte_name}_in_the_year_{year}\"\n",
        "\n",
        "            total_decrease_duration = None\n",
        "            if slope < 0:\n",
        "                log_MCL = np.log10(MCL)\n",
        "                time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, min(query['COLLECTION_DATE']))\n",
        "                if time_at_MCL:\n",
        "                    total_decrease_duration = (time_at_MCL - min(query['COLLECTION_DATE'])).days\n",
        "\n",
        "            forecasted_concentration.append({\n",
        "                'STATION_ID': well,\n",
        "                concentration_key: 10**forecasted_log_concentration,  # Convert log concentration back to original scale\n",
        "                'total_decrease_duration': total_decrease_duration\n",
        "            })\n",
        "\n",
        "        forecasted_df = pd.DataFrame(forecasted_concentration)\n",
        "        return forecasted_df\n",
        "\n",
        "    def merge_with_location_data(self, location_data, average_concentration):\n",
        "        # Ensure both keys are strings\n",
        "        location_data['station_id'] = location_data['station_id'].astype(str).str.strip().str.upper()\n",
        "        average_concentration['STATION_ID'] = average_concentration['STATION_ID'].astype(str).str.strip().str.upper()\n",
        "\n",
        "        # Identify common wells\n",
        "        common_wells = set(location_data['station_id']).intersection(set(average_concentration['STATION_ID']))\n",
        "\n",
        "        # Filter location_data to retain only common wells\n",
        "        filtered_location_data = location_data[location_data['station_id'].isin(common_wells)]\n",
        "\n",
        "        # Merge the data\n",
        "        merged_data = pd.merge(filtered_location_data, average_concentration, left_on='station_id', right_on='STATION_ID', how='left')\n",
        "\n",
        "        # Rename the 'total_decrease_duration' column to 'time@MCL'\n",
        "        if 'total_decrease_duration' in merged_data.columns:\n",
        "            merged_data.rename(columns={'total_decrease_duration': 'time@MCL'}, inplace=True)\n",
        "\n",
        "        return merged_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBGXsgoMK8ZH"
      },
      "outputs": [],
      "source": [
        "data_analyzer = DataAnalyzer('https://raw.githubusercontent.com/ALTEMIS-DOE/pylenm/master/notebooks/data/FASB_Data_thru_3Q2015_Reduced_Demo.csv')\n",
        "location_analyzer = DataAnalyzer('/content/gdrive/MyDrive/MIT Project - VAL @HW_CEE/FASB Well Construction Info.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZOJs32nPQ4C"
      },
      "source": [
        "# **Gather variables for calculation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHAy-J3oNv8i"
      },
      "outputs": [],
      "source": [
        "average_concentration_2024_sr_90 = data_analyzer.forecast_average_concentration(analyte_name_column='ANALYTE_NAME', well_name_column='STATION_ID', year=2024, analyte_name='STRONTIUM-90', MCL=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKBB-NqpLw5w"
      },
      "outputs": [],
      "source": [
        "print(average_concentration_2024_sr_90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fJd7FW3XTly"
      },
      "outputs": [],
      "source": [
        "average_concentration_2024_i_129 = data_analyzer.forecast_average_concentration(analyte_name_column='ANALYTE_NAME', well_name_column='STATION_ID', year=2024, analyte_name='IODINE-129', MCL=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maF7O7A2XgI4"
      },
      "outputs": [],
      "source": [
        "print(average_concentration_2024_i_129)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH68nYT4AV6F"
      },
      "outputs": [],
      "source": [
        "location_data = location_analyzer.data\n",
        "location_data['SZ_AV'] = (location_data['SZ_TOP(ft MSL)'] + location_data['SZ_BOT(ft MSL)']) / 2\n",
        "location_data['SZ_window'] = location_data['SZ_TOP(ft MSL)'] - location_data['SZ_BOT(ft MSL)']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZM32cdvAdqJ"
      },
      "outputs": [],
      "source": [
        "print(location_data[['SZ_TOP(ft MSL)', 'SZ_BOT(ft MSL)', 'SZ_AV', 'SZ_window']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXKO2VIgPH96"
      },
      "outputs": [],
      "source": [
        "merged_data_sr_90 = data_analyzer.merge_with_location_data(location_data, average_concentration_2024_sr_90)\n",
        "merged_data_i_129 = data_analyzer.merge_with_location_data(location_data, average_concentration_2024_i_129)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7LBTUMxeQNQ"
      },
      "outputs": [],
      "source": [
        "print(merged_data_sr_90)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_data_sr_90.columns)"
      ],
      "metadata": {
        "id": "e49_D2-6JYSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data_sr_90 = merged_data_sr_90.rename(columns={\n",
        "    'concentration_of_STRONTIUM-90_in_the_year_2024': 'Sr_90_initial_concentration',\n",
        "    'SZ_AV': 'screen_zone_average',\n",
        "    'SZ_window': 'screen_zone_window',\n",
        "})"
      ],
      "metadata": {
        "id": "b-2UAtbCJnA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data_sr_90['aquifer'] = merged_data_sr_90['aquifer'].replace({\n",
        "    'UAZ_UTRAU': 'UUTRA',\n",
        "    'LAZ_UTRAU': 'LUTRA'\n",
        "})"
      ],
      "metadata": {
        "id": "bzNvyC5IKilZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwvzrzUqX2nY"
      },
      "outputs": [],
      "source": [
        "print(merged_data_i_129)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_data_i_129.columns)"
      ],
      "metadata": {
        "id": "E9gI6dQ9OQcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data_i_129 = merged_data_i_129.rename(columns={\n",
        "    'concentration_of_IODINE-129_in_the_year_2024': 'I_129_initial_concentration',\n",
        "    'SZ_AV': 'screen_zone_average',\n",
        "    'SZ_window': 'screen_zone_window',\n",
        "})"
      ],
      "metadata": {
        "id": "hHyBRtVpLe2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data_i_129['aquifer'] = merged_data_i_129['aquifer'].replace({\n",
        "    'UAZ_UTRAU': 'UUTRA',\n",
        "    'LAZ_UTRAU': 'LUTRA'\n",
        "})"
      ],
      "metadata": {
        "id": "fmWHS8bELpUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71iHQhQtTEvR"
      },
      "source": [
        "# **Random forest regression**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scienceplots"
      ],
      "metadata": {
        "id": "c4oKXvmFPK8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scienceplots"
      ],
      "metadata": {
        "id": "F5tTTnHpO1Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "w0Aj5eh3r6Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3BZZU8oPe2b"
      },
      "outputs": [],
      "source": [
        "def random_forest_regression(merged_data, target_column, feature_columns, categorical_column=None):\n",
        "    # Data preparation (same as before)\n",
        "    for col in feature_columns:\n",
        "        if categorical_column is None or col != categorical_column:\n",
        "            merged_data[col] = pd.to_numeric(merged_data[col], errors='coerce')\n",
        "\n",
        "    merged_data[target_column] = pd.to_numeric(merged_data[target_column], errors='coerce')\n",
        "    merged_data = merged_data.dropna(subset=[target_column])\n",
        "\n",
        "    if categorical_column is not None:\n",
        "        merged_data[categorical_column] = merged_data[categorical_column].astype('category')\n",
        "        categorical_data = pd.get_dummies(merged_data[categorical_column], drop_first=True)\n",
        "        merged_data = pd.concat([merged_data[feature_columns].drop(columns=[categorical_column]), categorical_data, merged_data[target_column]], axis=1)\n",
        "    else:\n",
        "        merged_data = merged_data[feature_columns + [target_column]]\n",
        "\n",
        "    # Drop rows with missing values\n",
        "    merged_data = merged_data.dropna()\n",
        "\n",
        "    X = merged_data.drop(columns=[target_column])\n",
        "    y = merged_data[target_column]\n",
        "\n",
        "    # Ensure enough data\n",
        "    if X.shape[0] < 2:\n",
        "        print(\"Not enough data for training. Ensure there are more samples.\")\n",
        "        return\n",
        "\n",
        "    # Use a pipeline to standardize the data and apply RandomForestRegressor\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('rf', RandomForestRegressor(n_estimators=100, random_state=42))\n",
        "    ])\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
        "    print(f'Cross-validated R^2 scores: {cv_scores}')\n",
        "    print(f'Mean R^2 score: {cv_scores.mean()}')\n",
        "\n",
        "    # Train the model\n",
        "    pipeline.fit(X, y)\n",
        "    y_pred = pipeline.predict(X)\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    r2 = r2_score(y, y_pred)\n",
        "\n",
        "    print(f'Mean Squared Error: {mse}')\n",
        "    print(f'R^2 Score: {r2}')\n",
        "\n",
        "    # Feature importance\n",
        "    rf = pipeline.named_steps['rf']\n",
        "    feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "    feature_importances = feature_importances.sort_values(ascending=False)\n",
        "\n",
        "    print(\"Feature Importances:\")\n",
        "    print(feature_importances)\n",
        "\n",
        "    return pipeline, feature_importances, r2, mse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature_importances(feature_importances, analyte, concentration_status):\n",
        "    plt.style.use(['science', 'bright'])\n",
        "    plt.rcParams.update({\n",
        "        \"text.usetex\": False,\n",
        "        \"font.size\": 16\n",
        "    })\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    colors = plt.colormaps.get_cmap('tab10')(np.linspace(0, 1, len(feature_importances)))\n",
        "    explode = [0.05 if importance < 0.05 else 0 for importance in feature_importances]\n",
        "    wedges, texts = ax.pie(\n",
        "        feature_importances, labels=None, colors=colors, startangle=90, explode=explode\n",
        "    )\n",
        "    distance_levels = np.linspace(1.00, 1.2, len(feature_importances))  # Bring labels closer to the chart\n",
        "    label_shift_factor = 0.035  # Minimize the separation between labels\n",
        "\n",
        "    for i, wedge in enumerate(wedges):\n",
        "        angle = (wedge.theta2 + wedge.theta1) / 2  # Middle angle of the wedge\n",
        "        x = np.cos(np.deg2rad(angle)) * (distance_levels[i] + i * label_shift_factor)\n",
        "        y = np.sin(np.deg2rad(angle)) * (distance_levels[i] + i * label_shift_factor)\n",
        "\n",
        "        ax.text(x, y, f'{feature_importances[i]*100:.1f}%', ha='center', fontsize=14, color='black')\n",
        "\n",
        "    feature_labels = feature_importances.index\n",
        "    handles = [plt.Rectangle((0, 0), 1, 1, color=color) for color in colors]\n",
        "    legend = ax.legend(handles, feature_labels, title=\"Features\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "    fig.subplots_adjust(top=0.85, right=0.75)\n",
        "\n",
        "    # Modify the title to include 'with' or 'without' based on the argument\n",
        "    fig.suptitle(f'Feature Importances for {analyte}\\n{concentration_status} the analyte concentration', fontsize=18, x=0.9, y=0.92)\n",
        "\n",
        "    ax.axis('equal')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sDF-wbv7PbLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It1pZ9WMLP8V"
      },
      "outputs": [],
      "source": [
        "categorical_column_1 = 'aquifer'\n",
        "target_column = 'time@MCL'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41-OgwJUH_lI"
      },
      "outputs": [],
      "source": [
        "# Sr-90 with analyte concentration\n",
        "feature_columns_1 = [\n",
        "    'Sr_90_initial_concentration',\n",
        "    'screen_zone_average', 'screen_zone_window', 'latitude', 'longitude',\n",
        "    'easting', 'northing', 'ground_elevation',\n",
        "]\n",
        "\n",
        "pipeline_sr_90_1, feature_importances_sr_90_1, r2_sr90_with, mse_sr90_with = random_forest_regression(\n",
        "    merged_data_sr_90, target_column, feature_columns_1\n",
        ")\n",
        "\n",
        "if feature_importances_sr_90_1 is not None:\n",
        "    plot_feature_importances(feature_importances_sr_90_1, 'Sr-90', 'with')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o10vjiBy8Qb"
      },
      "outputs": [],
      "source": [
        "# Sr-90 without analyte concentration\n",
        "feature_columns_3 = [\n",
        "    'screen_zone_average', 'screen_zone_window', 'latitude', 'longitude',\n",
        "    'easting', 'northing', 'ground_elevation',\n",
        "]\n",
        "\n",
        "pipeline_sr_90_2, feature_importances_sr_90_2, r2_sr90_without, mse_sr90_without = random_forest_regression(\n",
        "    merged_data_sr_90, target_column, feature_columns_3\n",
        ")\n",
        "\n",
        "if feature_importances_sr_90_2 is not None:\n",
        "    plot_feature_importances(feature_importances_sr_90_2, 'Sr-90', 'without')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3OHCY6uzmhU"
      },
      "outputs": [],
      "source": [
        "# Prepare variables for I-129 with analyte concentration\n",
        "feature_columns_2 = [\n",
        "    'I_129_initial_concentration',\n",
        "    'screen_zone_average', 'screen_zone_window', 'latitude', 'longitude',\n",
        "    'easting', 'northing', 'ground_elevation',\n",
        "]\n",
        "\n",
        "pipeline_i_129_1, feature_importances_i_129_1, r2_i129_with, mse_i129_with = random_forest_regression(\n",
        "    merged_data_i_129, target_column, feature_columns_2\n",
        ")\n",
        "\n",
        "if feature_importances_i_129_1 is not None:\n",
        "    plot_feature_importances(feature_importances_i_129_1, 'I-129', 'with')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9omk_4QY0TNT"
      },
      "outputs": [],
      "source": [
        "# I-129 without analyte concentration\n",
        "feature_columns_4 = [\n",
        "    'screen_zone_average', 'screen_zone_window', 'latitude', 'longitude',\n",
        "    'easting', 'northing', 'ground_elevation',\n",
        "]\n",
        "\n",
        "pipeline_i_129_2, feature_importances_i_129_2, r2_i129_without, mse_i129_without = random_forest_regression(\n",
        "    merged_data_i_129, target_column, feature_columns_4\n",
        ")\n",
        "\n",
        "if feature_importances_i_129_2 is not None:\n",
        "    plot_feature_importances(feature_importances_i_129_2, 'I-129', 'without')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxUGRi-o2wrx"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYhNhShf0xo_"
      },
      "outputs": [],
      "source": [
        "# Enable science style from SciencePlots package\n",
        "plt.style.use(['science', 'bright'])\n",
        "\n",
        "# Disable LaTeX rendering for text elements\n",
        "plt.rcParams.update({\n",
        "    \"text.usetex\": False,      # Disable LaTeX\n",
        "    \"font.size\": 16            # Increase font size\n",
        "})\n",
        "\n",
        "# Prepare data\n",
        "def get_performance_metrics(pipeline, X, y):\n",
        "    y_pred = pipeline.predict(X)\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    r2 = r2_score(y, y_pred)\n",
        "    return r2, mse\n",
        "\n",
        "# Data for plotting\n",
        "metrics = ['R² Score', 'Mean Squared Error']\n",
        "sr90_with = [r2_sr90_with, mse_sr90_with]\n",
        "sr90_without = [r2_sr90_without, mse_sr90_without]\n",
        "i129_with = [r2_i129_with, mse_i129_with]\n",
        "i129_without = [r2_i129_without, mse_i129_without]\n",
        "\n",
        "# Set-up\n",
        "labels = ['R² Score', 'Mean Squared Error']\n",
        "groups = ['Sr-90 with Analyte', 'Sr-90 without Analyte', 'I-129 with Analyte', 'I-129 without Analyte']\n",
        "colors = cm.jet(np.linspace(0, 1, len(groups)))\n",
        "\n",
        "# Create a bar chart for R² Score\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.bar(groups, [r2_sr90_with, r2_sr90_without, r2_i129_with, r2_i129_without], color=colors)\n",
        "ax.set_ylabel('R² Score')\n",
        "ax.set_title('R² Score by Model and Analyte')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Improve layout\n",
        "fig.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Create a bar chart for Mean Squared Error\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.bar(groups, [mse_sr90_with, mse_sr90_without, mse_i129_with, mse_i129_without], color=colors)\n",
        "ax.set_ylabel('Mean Squared Error')\n",
        "ax.set_title('Mean Squared Error by Model and Analyte')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Improve layout\n",
        "fig.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CF using analytical methods**"
      ],
      "metadata": {
        "id": "mE5LlhEvvAwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import t\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils import resample"
      ],
      "metadata": {
        "id": "xhWPvxBJ9cTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analytical_prediction_intervals(pipeline, X, y, alpha=0.05):\n",
        "    # Fit the model\n",
        "    pipeline.fit(X, y)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = pipeline.predict(X)\n",
        "\n",
        "    # Residuals\n",
        "    residuals = y - y_pred\n",
        "\n",
        "    # Degrees of freedom\n",
        "    n = len(y)\n",
        "    p = X.shape[1]\n",
        "    dof = n - p - 1\n",
        "\n",
        "    # Mean Squared Error\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "\n",
        "    # Standard Error\n",
        "    try:\n",
        "        X = X.astype(np.float64)  # Ensure all values in X are float64\n",
        "        XTX_inv = np.linalg.inv(np.dot(X.T, X))\n",
        "        se = np.sqrt(mse * (1 + np.diagonal(np.dot(X, np.dot(XTX_inv, X.T)))))\n",
        "    except np.linalg.LinAlgError:\n",
        "        se = np.full_like(y_pred, np.nan)  # Set SE to NaN if the matrix is singular\n",
        "    except ValueError:\n",
        "        se = np.full_like(y_pred, np.nan)  # Set SE to NaN if conversion fails\n",
        "\n",
        "    # t-value for the confidence interval\n",
        "    t_value = t.ppf(1 - alpha / 2, dof)\n",
        "\n",
        "    # Prediction Interval\n",
        "    lower_bound = y_pred - t_value * se\n",
        "    upper_bound = y_pred + t_value * se\n",
        "\n",
        "    return lower_bound, upper_bound, y_pred\n",
        "\n",
        "ci_results = {}\n",
        "\n",
        "datasets = [\n",
        "    ('Sr-90 with analyte concentration', RandomForestRegressor(), merged_data_sr_90, feature_columns_1),\n",
        "    ('Sr-90 without analyte concentration', RandomForestRegressor(), merged_data_sr_90, feature_columns_3),\n",
        "    ('I-129 with analyte concentration', RandomForestRegressor(), merged_data_i_129, feature_columns_2),\n",
        "    ('I-129 without analyte concentration', RandomForestRegressor(), merged_data_i_129, feature_columns_4)\n",
        "]\n",
        "\n",
        "for name, model, data, feature_columns in datasets:\n",
        "    X = data[feature_columns]\n",
        "    y = data[target_column]\n",
        "\n",
        "    # Ensure categorical encoding\n",
        "    categorical_column = 'aquifer'  # Assuming 'aquifer' is the categorical column\n",
        "    if categorical_column in X.columns:\n",
        "        X = pd.get_dummies(X, columns=[categorical_column], drop_first=True)\n",
        "\n",
        "    # Ensure all data is numerical and handle NaNs\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    y = pd.to_numeric(y, errors='coerce')\n",
        "\n",
        "    # Drop rows with NaN values in X or y\n",
        "    valid_mask = ~X.isna().any(axis=1) & ~y.isna()\n",
        "    X = X[valid_mask]\n",
        "    y = y[valid_mask]\n",
        "\n",
        "    # Use StandardScaler and LinearRegression for the analytical approach\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', LinearRegression())\n",
        "    ])\n",
        "\n",
        "    lower_bound, upper_bound, predictions = analytical_prediction_intervals(pipeline, X, y)\n",
        "    ci_results[name] = {'lower_bound': lower_bound, 'upper_bound': upper_bound, 'predictions': predictions}\n",
        "\n",
        "# Generating the tables for each trial with well names included, converting day units to year units, and adding confidence interval as percentage\n",
        "for name, ci_data in ci_results.items():\n",
        "    data_for_current_trial = datasets[[ds[0] for ds in datasets].index(name)][2]  # Get the dataset corresponding to the current trial\n",
        "    well_names = data_for_current_trial['STATION_ID']\n",
        "\n",
        "    X = data_for_current_trial[feature_columns]\n",
        "    y = data_for_current_trial[target_column]\n",
        "\n",
        "    # Ensure categorical encoding\n",
        "    categorical_column = 'aquifer'  # Assuming 'aquifer' is the categorical column\n",
        "    if categorical_column in X.columns:\n",
        "        X = pd.get_dummies(X, columns=[categorical_column], drop_first=True)\n",
        "\n",
        "    # Ensure all data is numerical and handle NaNs\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    y = pd.to_numeric(y, errors='coerce')\n",
        "\n",
        "    # Drop rows with NaN values in X or y\n",
        "    valid_mask = ~X.isna().any(axis=1) & ~y.isna()\n",
        "    well_names = well_names[valid_mask]  # Apply valid_mask to well_names\n",
        "    X = X[valid_mask]\n",
        "    y = y[valid_mask]\n",
        "\n",
        "    # Convert results from days to years\n",
        "    predictions_in_years = ci_data['predictions'] / 365.25\n",
        "    lower_bound_in_years = ci_data['lower_bound'] / 365.25\n",
        "    upper_bound_in_years = ci_data['upper_bound'] / 365.25\n",
        "    confidence_interval_in_years = (ci_data['upper_bound'] - ci_data['lower_bound']) / 365.25\n",
        "    confidence_interval_percentage = (confidence_interval_in_years / predictions_in_years) * 100\n",
        "\n",
        "    # Replace negative values with '[0]'\n",
        "    predictions_in_years = np.where(predictions_in_years < 0, '[0]', predictions_in_years)\n",
        "    lower_bound_in_years = np.where(lower_bound_in_years < 0, '[0]', lower_bound_in_years)\n",
        "    upper_bound_in_years = np.where(upper_bound_in_years < 0, '[0]', upper_bound_in_years)\n",
        "    confidence_interval_in_years = np.where(confidence_interval_in_years < 0, '[0]', confidence_interval_in_years)\n",
        "    confidence_interval_percentage = np.where(confidence_interval_percentage < 0, '[0]', confidence_interval_percentage)\n",
        "\n",
        "    table = pd.DataFrame({\n",
        "        'Station ID': well_names,\n",
        "        'Prediction (in years)': predictions_in_years,\n",
        "        'Lower Bound (in years)': lower_bound_in_years,\n",
        "        'Upper Bound (in years)': upper_bound_in_years,\n",
        "        'Confidence Interval (in years)': confidence_interval_in_years,\n",
        "        'Confidence Interval (%)': confidence_interval_percentage\n",
        "    })\n",
        "    print(f'Table for {name}:')\n",
        "    print(table)\n",
        "    table.to_csv(f'{name}_confidence_intervals_in_years.csv', index=False)"
      ],
      "metadata": {
        "id": "i2x02dwNwOfr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "39aaf84606e21579959f93fb3c45e9e3d37c51d84ce27b2546e2ee0530e04c07"
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit ('elev': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}