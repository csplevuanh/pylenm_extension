{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olxOIIAvUIS1"
      },
      "source": [
        "# **SET UP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2JbZiOxdYE9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.colors as mcolors\n",
        "import os\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import t, linregress\n",
        "from matplotlib.dates import date2num, num2date\n",
        "import datetime\n",
        "from datetime import datetime, date, timedelta\n",
        "import folium\n",
        "import branca.colormap\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.colors as pcolors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxqxoBV4LQb4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scienceplots"
      ],
      "metadata": {
        "id": "BHYf9yPoCZ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scienceplots"
      ],
      "metadata": {
        "id": "piF5ZHjqCjfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyOVU1hv7iuo"
      },
      "source": [
        "# **DataAnalyzer class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77cuKlNQY6Cq"
      },
      "outputs": [],
      "source": [
        "class DataAnalyzer:\n",
        "    def __init__(self, data_or_url):\n",
        "        if isinstance(data_or_url, str):\n",
        "            self.data_url = data_or_url\n",
        "            self.data = self.load_data()\n",
        "        elif isinstance(data_or_url, pd.DataFrame):\n",
        "            self.data = data_or_url\n",
        "        else:\n",
        "            raise ValueError(\"Input must be a file path or a pandas DataFrame\")\n",
        "\n",
        "    def load_data(self):\n",
        "        file_extension = self.data_url.split('.')[-1]\n",
        "        if file_extension == 'csv':\n",
        "            return pd.read_csv(self.data_url)\n",
        "        elif file_extension in ['xlsx', 'xlsm', 'xltx', 'xltm']:\n",
        "            return pd.read_excel(self.data_url)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "    def print_column_names(self):\n",
        "        print(\"Column names in the dataset:\", self.data.columns.tolist())\n",
        "\n",
        "    def preprocess_data(self, well_name_column, analyte_name_column, well_name, analyte_name):\n",
        "        filtered_data = self.data[(self.data[well_name_column] == well_name) & (self.data[analyte_name_column] == analyte_name)]\n",
        "        filtered_data = filtered_data.dropna(subset=['COLLECTION_DATE', 'RESULT'])\n",
        "        filtered_data['COLLECTION_DATE'] = pd.to_datetime(filtered_data['COLLECTION_DATE']).dt.date\n",
        "        filtered_data['RESULT_LOG'] = np.log10(filtered_data['RESULT'])\n",
        "        return filtered_data\n",
        "\n",
        "    def remove_outliers(self, data, result_column='RESULT_LOG', z_threshold=4):\n",
        "        z = np.abs(stats.zscore(data[result_column]))\n",
        "        return data[(z < z_threshold)]\n",
        "\n",
        "    def predict_time_at_MCL(self, slope, intercept, log_MCL, earliest_date):\n",
        "        if slope >= 0:\n",
        "            return None\n",
        "        time_at_MCL = (log_MCL - intercept) / slope\n",
        "        try:\n",
        "            time_at_MCL_date = num2date(time_at_MCL).replace(tzinfo=None).date()\n",
        "            if earliest_date.year < 1 or time_at_MCL_date.year >= 10000 or time_at_MCL_date <= earliest_date:\n",
        "                return None\n",
        "            return time_at_MCL_date\n",
        "        except ValueError:\n",
        "            return None\n",
        "\n",
        "    def adjust_date_based_on_confidence(self, base_date, slope, confidence_interval, days_per_unit):\n",
        "        if base_date is None or slope == 0:\n",
        "            return None, None, None\n",
        "        days_adjustment = confidence_interval / abs(slope) * days_per_unit\n",
        "        upper_date = base_date + timedelta(days=days_adjustment) if base_date else None\n",
        "        lower_date = base_date - timedelta(days=days_adjustment) if base_date else None\n",
        "        return lower_date, upper_date\n",
        "\n",
        "    def generate_analyte_summary(self, analyte_name_column, analyte_name, well_name_column, MCL):\n",
        "        unique_wells = self.data[self.data[analyte_name_column] == analyte_name][well_name_column].unique()\n",
        "        summary = []\n",
        "\n",
        "        for well in unique_wells:\n",
        "            query = self.preprocess_data(well_name_column, analyte_name_column, well, analyte_name)\n",
        "            if query.empty or len(query['COLLECTION_DATE'].unique()) < 2:\n",
        "                continue\n",
        "\n",
        "            x = date2num(query['COLLECTION_DATE'])\n",
        "            y = query['RESULT_LOG']\n",
        "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "\n",
        "            if slope > 0:\n",
        "                trend = 'Increase'\n",
        "            elif slope < 0:\n",
        "                trend = 'Decrease'\n",
        "            else:\n",
        "                trend = 'Neutral'\n",
        "\n",
        "            if trend == 'Neutral':\n",
        "                continue\n",
        "\n",
        "            log_MCL = np.log10(MCL)\n",
        "            time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, min(query['COLLECTION_DATE']))\n",
        "\n",
        "            total_decrease_duration = None\n",
        "            if trend == 'Decrease' and time_at_MCL:\n",
        "                total_decrease_duration = (time_at_MCL - min(query['COLLECTION_DATE'])).days\n",
        "\n",
        "            df = len(x) - 2\n",
        "            ts = t.ppf(1 - 0.025, df)\n",
        "            ci_slope = ts * std_err\n",
        "\n",
        "            lower_date, upper_date = None, None\n",
        "            if time_at_MCL and isinstance(time_at_MCL, date):\n",
        "                lower_date, upper_date = self.adjust_date_based_on_confidence(time_at_MCL, slope, ci_slope, 365.25)\n",
        "\n",
        "            summary.append({\n",
        "                'well_name': well,\n",
        "                'slope': slope,\n",
        "                'p_value': p_value,\n",
        "                'trend': trend,\n",
        "                'time@MCL': time_at_MCL,\n",
        "                'lower_time@MCL': lower_date,\n",
        "                'upper_time@MCL': upper_date,\n",
        "                'total_decrease_duration': total_decrease_duration\n",
        "            })\n",
        "\n",
        "        summary_df = pd.DataFrame(summary)\n",
        "        return summary_df\n",
        "\n",
        "    def compile_results(self, well_analyte_pairs, well_name_column, analyte_name_column, MCL):\n",
        "        results = []\n",
        "        for well_name, analyte_name in well_analyte_pairs:\n",
        "            time_at_MCL = None\n",
        "            query = self.preprocess_data(well_name_column, analyte_name_column, well_name, analyte_name)\n",
        "            if query.empty:\n",
        "                continue\n",
        "\n",
        "            query = self.remove_outliers(query)\n",
        "            x = date2num(query['COLLECTION_DATE'])\n",
        "            y = query['RESULT_LOG']\n",
        "\n",
        "            if len(x) < 2:\n",
        "                continue\n",
        "\n",
        "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "            df = len(x) - 2\n",
        "            ts = t.ppf(1 - 0.025, df)\n",
        "\n",
        "            trend = \"Increase\" if slope > 0 else \"Decrease\" if slope < 0 else \"Neutral\"\n",
        "            log_MCL = np.log10(MCL) if MCL else None\n",
        "            earliest_date = min(query['COLLECTION_DATE'])\n",
        "            time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, earliest_date) if MCL else None\n",
        "\n",
        "            total_decrease_duration = None\n",
        "            if trend == 'Decrease' and time_at_MCL:\n",
        "                total_decrease_duration = (time_at_MCL - min(query['COLLECTION_DATE'])).days\n",
        "\n",
        "            lower_date, upper_date = None, None\n",
        "            if time_at_MCL and isinstance(time_at_MCL, date):\n",
        "                lower_date, upper_date = self.adjust_date_based_on_confidence(time_at_MCL, slope, ts * std_err, 365.25)\n",
        "\n",
        "            results.append({\n",
        "                'well_name': well_name,\n",
        "                'slope': slope,\n",
        "                'p_value': p_value,\n",
        "                'trend': trend,\n",
        "                'time@MCL': time_at_MCL,\n",
        "                'lower_time@MCL': lower_date,\n",
        "                'upper_time@MCL': upper_date,\n",
        "                'total_decrease_duration': total_decrease_duration\n",
        "            })\n",
        "\n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_df.to_csv('time2MCL_results.csv', index=False)\n",
        "        return results_df\n",
        "\n",
        "    def plot_MCL(self, well_name_column, analyte_name_column, well_name, analyte_name, year_interval=20, save_dir='plot_MCL', MCL=None, extrapolate_until_year=2025):\n",
        "        plt.style.use(['science', 'no-latex'])\n",
        "\n",
        "        plt.rcParams.update({\n",
        "            'font.size': 18,\n",
        "            'axes.titlesize': 18,\n",
        "            'axes.labelsize': 18,\n",
        "            'xtick.labelsize': 18,\n",
        "            'ytick.labelsize': 18,\n",
        "            'legend.fontsize': 18,\n",
        "            'figure.titlesize': 18\n",
        "        })\n",
        "\n",
        "        query = self.preprocess_data(well_name_column, analyte_name_column, well_name, analyte_name)\n",
        "        if query.empty:\n",
        "            print(f'No results found for {well_name} and {analyte_name}')\n",
        "            return\n",
        "\n",
        "        query = self.remove_outliers(query, result_column='RESULT_LOG')\n",
        "        x = date2num(query['COLLECTION_DATE'])\n",
        "        y = query['RESULT_LOG']\n",
        "\n",
        "        if len(x) < 2:\n",
        "            print(\"Not enough data for regression.\")\n",
        "            return\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "        df = len(x) - 2\n",
        "        ts = t.ppf(1 - 0.025, df)\n",
        "\n",
        "        log_MCL = np.log10(MCL) if MCL else None\n",
        "        time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, min(query['COLLECTION_DATE'])) if MCL and slope < 0 else 'Not available'\n",
        "\n",
        "        lower_date, upper_date = None, None\n",
        "        if time_at_MCL and isinstance(time_at_MCL, date):\n",
        "            lower_date, upper_date = self.adjust_date_based_on_confidence(time_at_MCL, slope, ts * std_err, 365.25)\n",
        "\n",
        "        extrapolate_date = np.datetime64(f'{extrapolate_until_year}-12-31')\n",
        "        x_extrapolate = np.linspace(x.min(), date2num(extrapolate_date), 100)\n",
        "        y_extrapolate = slope * x_extrapolate + intercept\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.grid(False)\n",
        "\n",
        "        data_points = plt.scatter(query['COLLECTION_DATE'], y, color='blue', label='Data Points')\n",
        "        plt.xlim([num2date(x.min()), num2date(x_extrapolate.max())])\n",
        "        plt.ylim([0.0, max(y.max(), y_extrapolate.max())])\n",
        "\n",
        "        extrap_regression = plt.plot(num2date(x_extrapolate), y_extrapolate, 'r--', label=f'Extrapolated Regression')\n",
        "\n",
        "        if log_MCL is not None:\n",
        "            mcl_line = plt.axhline(y=log_MCL, color='green', linestyle='--', label=f'MCL (log10): {log_MCL:.2f}')\n",
        "\n",
        "        if lower_date and upper_date:\n",
        "            lower_intercept = intercept + slope * (date2num(lower_date) - date2num(time_at_MCL))\n",
        "            upper_intercept = intercept + slope * (date2num(upper_date) - date2num(time_at_MCL))\n",
        "            lower_ci = plt.plot(num2date(x_extrapolate), slope * x_extrapolate + lower_intercept, 'b--', label='Lower CI Line')\n",
        "            upper_ci = plt.plot(num2date(x_extrapolate), slope * x_extrapolate + upper_intercept, 'orange', label='Upper CI Line')\n",
        "\n",
        "        plt.xlabel('Collection Date')\n",
        "        plt.ylabel('Log-Concentration (pCi/mL)')\n",
        "        plt.title(f'{well_name} - {analyte_name}')\n",
        "\n",
        "        handles, labels = plt.gca().get_legend_handles_labels()\n",
        "        fig_legend = plt.legend(handles=handles, loc='upper center', bbox_to_anchor=(0.3, -0.2), fontsize=18, frameon=True, shadow=True)\n",
        "\n",
        "        result_str = (\n",
        "            f\"Results\\n\"\n",
        "            f\"- Slope value: {slope:.2e}\\n\"\n",
        "            f\"- Time@MCL value: {time_at_MCL}\\n\"\n",
        "            f\"- Lower CI value: {lower_date}\\n\"\n",
        "            f\"- Upper CI value: {upper_date}\\n\"\n",
        "            f\"- p-value: {p_value:.4f}\"\n",
        "        )\n",
        "\n",
        "        plt.figtext(0.55, -0.2, result_str, fontsize=18, ha=\"left\", bbox=dict(facecolor='white', alpha=0.5))\n",
        "        plt.tight_layout(rect=[0, 0.31, 1, 0.95])\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.gcf().autofmt_xdate()\n",
        "\n",
        "        plt.xlim([num2date(x.min()), num2date(x_extrapolate.max())])\n",
        "\n",
        "        figure_path = f'{save_dir}/{well_name}-{analyte_name}.png'\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        plt.savefig(figure_path)\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "        return figure_path\n",
        "\n",
        "    def generate_analysis_dataframe(self, analyte_name_column, well_name_column, analyte_name, MCL):\n",
        "        analyte_summary_df = self.generate_analyte_summary(analyte_name_column, analyte_name, well_name_column, MCL)\n",
        "        analysis_data = []\n",
        "\n",
        "        for index, row in analyte_summary_df.iterrows():\n",
        "            if row['trend'] == 'Neutral':\n",
        "                continue\n",
        "\n",
        "            plot_path = self.plot_MCL(well_name_column, analyte_name_column, row['well_name'], analyte_name, year_interval=5, save_dir='plot_MCL', MCL=MCL)\n",
        "\n",
        "            analysis_data.append({\n",
        "                'analyte_name': analyte_name,\n",
        "                'well_name': row['well_name'],\n",
        "                'slope': row['slope'],\n",
        "                'p_value': row['p_value'],\n",
        "                'trend': row['trend'],\n",
        "                'time@MCL': row['time@MCL'],\n",
        "                'lower_time@MCL': row['lower_time@MCL'],\n",
        "                'upper_time@MCL': row['upper_time@MCL'],\n",
        "                'total_decrease_duration': row['total_decrease_duration'],\n",
        "                'path_to_figure': plot_path\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(analysis_data)\n",
        "\n",
        "    def generate_decreasing_well_list(self, analyte_name_column, analyte_name, well_name_column, MCL):\n",
        "        analyte_summary_df = self.generate_analyte_summary(analyte_name_column, analyte_name, well_name_column, MCL)\n",
        "        decreasing_well_list = analyte_summary_df[analyte_summary_df['trend'] == 'Decrease']['well_name'].tolist()\n",
        "        return decreasing_well_list\n",
        "\n",
        "    def extract_location_data(self, well_name_column, decreasing_well_list):\n",
        "        location_data = self.data[self.data[well_name_column].isin(decreasing_well_list)]\n",
        "        return location_data\n",
        "\n",
        "    def forecast_average_concentration(self, analyte_name_column, well_name_column, year, analyte_name, MCL):\n",
        "        decreasing_wells = self.generate_decreasing_well_list(analyte_name_column, analyte_name, well_name_column, MCL)\n",
        "\n",
        "        forecasted_concentration = []\n",
        "\n",
        "        unique_wells = self.data[self.data[well_name_column].isin(decreasing_wells)][well_name_column].unique()\n",
        "\n",
        "        for well in unique_wells:\n",
        "            query = self.preprocess_data(well_name_column, analyte_name_column, well, analyte_name)\n",
        "            if query.empty or len(query['COLLECTION_DATE'].unique()) < 2:\n",
        "                continue\n",
        "\n",
        "            x = date2num(query['COLLECTION_DATE'])\n",
        "            y = query['RESULT_LOG']\n",
        "\n",
        "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "            forecasted_log_concentration = slope * date2num(pd.to_datetime(f'{year}-01-01')) + intercept\n",
        "\n",
        "            concentration_key = f\"concentration_of_{analyte_name}_in_the_year_{year}\"\n",
        "\n",
        "            total_decrease_duration = None\n",
        "            if slope < 0:\n",
        "                log_MCL = np.log10(MCL)\n",
        "                time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, min(query['COLLECTION_DATE']))\n",
        "                if time_at_MCL:\n",
        "                    total_decrease_duration = (time_at_MCL - min(query['COLLECTION_DATE'])).days\n",
        "\n",
        "            forecasted_concentration.append({\n",
        "                'STATION_ID': well,\n",
        "                concentration_key: 10**forecasted_log_concentration,\n",
        "                'total_decrease_duration': total_decrease_duration\n",
        "            })\n",
        "\n",
        "        forecasted_df = pd.DataFrame(forecasted_concentration)\n",
        "        return forecasted_df\n",
        "\n",
        "    def merge_with_location_data(self, location_data, average_concentration):\n",
        "        location_data['station_id'] = location_data['station_id'].astype(str).str.strip().str.upper()\n",
        "        average_concentration['STATION_ID'] = average_concentration['STATION_ID'].astype(str).str.strip().str.upper()\n",
        "\n",
        "        common_wells = set(location_data['station_id']).intersection(set(average_concentration['STATION_ID']))\n",
        "\n",
        "        filtered_location_data = location_data[location_data['station_id'].isin(common_wells)]\n",
        "\n",
        "        merged_data = pd.merge(filtered_location_data, average_concentration, left_on='station_id', right_on='STATION_ID', how='left')\n",
        "\n",
        "        if 'total_decrease_duration' in merged_data.columns:\n",
        "            merged_data.rename(columns={'total_decrease_duration': 'time@MCL'}, inplace=True)\n",
        "\n",
        "        return merged_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBGXsgoMK8ZH"
      },
      "outputs": [],
      "source": [
        "data_analyzer = DataAnalyzer('https://raw.githubusercontent.com/ALTEMIS-DOE/pylenm/master/notebooks/data/FASB_Data_thru_3Q2015_Reduced_Demo.csv')\n",
        "location_analyzer = DataAnalyzer('/content/gdrive/MyDrive/MIT Project - VAL @HW_CEE/FASB Well Construction Info.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZOJs32nPQ4C"
      },
      "source": [
        "# **Gather variables for calculation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHAy-J3oNv8i"
      },
      "outputs": [],
      "source": [
        "average_concentration_2024_sr_90 = data_analyzer.forecast_average_concentration(analyte_name_column='ANALYTE_NAME', well_name_column='STATION_ID', year=2024, analyte_name='STRONTIUM-90', MCL=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fJd7FW3XTly"
      },
      "outputs": [],
      "source": [
        "average_concentration_2024_i_129 = data_analyzer.forecast_average_concentration(analyte_name_column='ANALYTE_NAME', well_name_column='STATION_ID', year=2024, analyte_name='IODINE-129', MCL=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH68nYT4AV6F"
      },
      "outputs": [],
      "source": [
        "location_data = location_analyzer.data\n",
        "location_data['SZ_AV'] = (location_data['SZ_TOP(ft MSL)'] + location_data['SZ_BOT(ft MSL)']) / 2\n",
        "location_data['SZ_window'] = location_data['SZ_TOP(ft MSL)'] - location_data['SZ_BOT(ft MSL)']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyte_summary_df_sr_90 = data_analyzer.generate_analyte_summary(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='STRONTIUM-90',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "print(analyte_summary_df_sr_90)"
      ],
      "metadata": {
        "id": "c-uUWtPWCqBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyte_summary_df_i_129 = data_analyzer.generate_analyte_summary(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='IODINE-129',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "print(analyte_summary_df_i_129)"
      ],
      "metadata": {
        "id": "Kn6zXDGLC0kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXKO2VIgPH96"
      },
      "outputs": [],
      "source": [
        "merged_data_sr_90 = data_analyzer.merge_with_location_data(location_data, average_concentration_2024_sr_90)\n",
        "merged_data_i_129 = data_analyzer.merge_with_location_data(location_data, average_concentration_2024_i_129)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data_sr_90 = pd.merge(\n",
        "    merged_data_sr_90,\n",
        "    analyte_summary_df_sr_90,\n",
        "    left_on='STATION_ID',\n",
        "    right_on='well_name',\n",
        "    how='inner'\n",
        ")"
      ],
      "metadata": {
        "id": "TzEwWG2CDSFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data_i_129 = pd.merge(\n",
        "    merged_data_i_129,\n",
        "    analyte_summary_df_i_129,\n",
        "    left_on='STATION_ID',\n",
        "    right_on='well_name',\n",
        "    how='inner'\n",
        ")\n"
      ],
      "metadata": {
        "id": "jKXNf9eaDS2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data_sr_90 = merged_data_sr_90.rename(columns={\n",
        "    'concentration_of_STRONTIUM-90_in_the_year_2024': 'Sr_90_initial_concentration',\n",
        "    'SZ_AV': 'screen_zone_average',\n",
        "    'SZ_window': 'screen_zone_window',\n",
        "})"
      ],
      "metadata": {
        "id": "b-2UAtbCJnA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data_sr_90['aquifer'] = merged_data_sr_90['aquifer'].replace({\n",
        "    'UAZ_UTRAU': 'UUTRA',\n",
        "    'LAZ_UTRAU': 'LUTRA'\n",
        "})"
      ],
      "metadata": {
        "id": "bzNvyC5IKilZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data_i_129 = merged_data_i_129.rename(columns={\n",
        "    'concentration_of_IODINE-129_in_the_year_2024': 'I_129_initial_concentration',\n",
        "    'SZ_AV': 'screen_zone_average',\n",
        "    'SZ_window': 'screen_zone_window',\n",
        "})"
      ],
      "metadata": {
        "id": "hHyBRtVpLe2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data_i_129['aquifer'] = merged_data_i_129['aquifer'].replace({\n",
        "    'UAZ_UTRAU': 'UUTRA',\n",
        "    'LAZ_UTRAU': 'LUTRA'\n",
        "})"
      ],
      "metadata": {
        "id": "fmWHS8bELpUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71iHQhQtTEvR"
      },
      "source": [
        "# **Random forest regression**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "w0Aj5eh3r6Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3BZZU8oPe2b"
      },
      "outputs": [],
      "source": [
        "def random_forest_regression(merged_data, target_column, feature_columns, categorical_column=None):\n",
        "    for col in feature_columns:\n",
        "        if categorical_column is None or col != categorical_column:\n",
        "            merged_data[col] = pd.to_numeric(merged_data[col], errors='coerce')\n",
        "\n",
        "    merged_data[target_column] = pd.to_numeric(merged_data[target_column], errors='coerce')\n",
        "    merged_data = merged_data.dropna(subset=[target_column])\n",
        "\n",
        "    if categorical_column is not None:\n",
        "        merged_data[categorical_column] = merged_data[categorical_column].astype('category')\n",
        "        categorical_data = pd.get_dummies(merged_data[categorical_column], drop_first=True)\n",
        "        merged_data = pd.concat([merged_data[feature_columns].drop(columns=[categorical_column]), categorical_data, merged_data[target_column]], axis=1)\n",
        "    else:\n",
        "        merged_data = merged_data[feature_columns + [target_column]]\n",
        "\n",
        "    merged_data = merged_data.dropna()\n",
        "\n",
        "    X = merged_data.drop(columns=[target_column])\n",
        "    y = merged_data[target_column]\n",
        "\n",
        "    if X.shape[0] < 2:\n",
        "        print(\"Not enough data for training. Ensure there are more samples.\")\n",
        "        return\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('rf', RandomForestRegressor(n_estimators=100, random_state=42))\n",
        "    ])\n",
        "\n",
        "    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
        "    print(f'Cross-validated R^2 scores: {cv_scores}')\n",
        "    print(f'Mean R^2 score: {cv_scores.mean()}')\n",
        "\n",
        "    pipeline.fit(X, y)\n",
        "    y_pred = pipeline.predict(X)\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    r2 = r2_score(y, y_pred)\n",
        "\n",
        "    print(f'Mean Squared Error: {mse}')\n",
        "    print(f'R^2 Score: {r2}')\n",
        "\n",
        "    rf = pipeline.named_steps['rf']\n",
        "    feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "    feature_importances = feature_importances.sort_values(ascending=False)\n",
        "\n",
        "    print(\"Feature Importances:\")\n",
        "    print(feature_importances)\n",
        "\n",
        "    return pipeline, feature_importances, r2, mse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature_importances(feature_importances, analyte, concentration_status):\n",
        "    plt.style.use(['science', 'bright'])\n",
        "    plt.rcParams.update({\n",
        "        \"text.usetex\": False,\n",
        "        \"font.size\": 16\n",
        "    })\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    colors = plt.colormaps.get_cmap('tab10')(np.linspace(0, 1, len(feature_importances)))\n",
        "    explode = [0.05 if importance < 0.05 else 0 for importance in feature_importances]\n",
        "    wedges, texts = ax.pie(\n",
        "        feature_importances, labels=None, colors=colors, startangle=90, explode=explode\n",
        "    )\n",
        "    distance_levels = np.linspace(1.00, 1.2, len(feature_importances))\n",
        "    label_shift_factor = 0.035\n",
        "\n",
        "    for i, wedge in enumerate(wedges):\n",
        "        angle = (wedge.theta2 + wedge.theta1) / 2\n",
        "        x = np.cos(np.deg2rad(angle)) * (distance_levels[i] + i * label_shift_factor)\n",
        "        y = np.sin(np.deg2rad(angle)) * (distance_levels[i] + i * label_shift_factor)\n",
        "\n",
        "        ax.text(x, y, f'{feature_importances[i]*100:.1f}%', ha='center', fontsize=14, color='black')\n",
        "\n",
        "    feature_labels = feature_importances.index\n",
        "    handles = [plt.Rectangle((0, 0), 1, 1, color=color) for color in colors]\n",
        "    legend = ax.legend(handles, feature_labels, title=\"Features\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "    fig.subplots_adjust(top=0.85, right=0.75)\n",
        "\n",
        "    fig.suptitle(f'Feature Importances for {analyte}\\n{concentration_status} the analyte concentration', fontsize=18, x=0.9, y=0.92)\n",
        "\n",
        "    ax.axis('equal')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sDF-wbv7PbLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It1pZ9WMLP8V"
      },
      "outputs": [],
      "source": [
        "categorical_column_1 = 'aquifer'\n",
        "target_column = 'slope'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41-OgwJUH_lI"
      },
      "outputs": [],
      "source": [
        "feature_columns_1 = [\n",
        "    'Sr_90_initial_concentration',\n",
        "    'screen_zone_average', 'screen_zone_window',\n",
        "    'easting', 'northing', 'ground_elevation',\n",
        "]\n",
        "\n",
        "pipeline_sr_90_1, feature_importances_sr_90_1, r2_sr90_with, mse_sr90_with = random_forest_regression(\n",
        "    merged_data_sr_90, target_column, feature_columns_1\n",
        ")\n",
        "\n",
        "if feature_importances_sr_90_1 is not None:\n",
        "    plot_feature_importances(feature_importances_sr_90_1, 'Sr-90', 'with')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o10vjiBy8Qb"
      },
      "outputs": [],
      "source": [
        "feature_columns_3 = [\n",
        "    'screen_zone_average', 'screen_zone_window',\n",
        "    'easting', 'northing', 'ground_elevation',\n",
        "]\n",
        "\n",
        "pipeline_sr_90_2, feature_importances_sr_90_2, r2_sr90_without, mse_sr90_without = random_forest_regression(\n",
        "    merged_data_sr_90, target_column, feature_columns_3\n",
        ")\n",
        "\n",
        "if feature_importances_sr_90_2 is not None:\n",
        "    plot_feature_importances(feature_importances_sr_90_2, 'Sr-90', 'without')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3OHCY6uzmhU"
      },
      "outputs": [],
      "source": [
        "feature_columns_2 = [\n",
        "    'I_129_initial_concentration',\n",
        "    'screen_zone_average', 'screen_zone_window',\n",
        "    'easting', 'northing', 'ground_elevation',\n",
        "]\n",
        "\n",
        "pipeline_i_129_1, feature_importances_i_129_1, r2_i129_with, mse_i129_with = random_forest_regression(\n",
        "    merged_data_i_129, target_column, feature_columns_2\n",
        ")\n",
        "\n",
        "if feature_importances_i_129_1 is not None:\n",
        "    plot_feature_importances(feature_importances_i_129_1, 'I-129', 'with')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9omk_4QY0TNT"
      },
      "outputs": [],
      "source": [
        "feature_columns_4 = [\n",
        "    'screen_zone_average', 'screen_zone_window',\n",
        "    'easting', 'northing', 'ground_elevation',\n",
        "]\n",
        "\n",
        "pipeline_i_129_2, feature_importances_i_129_2, r2_i129_without, mse_i129_without = random_forest_regression(\n",
        "    merged_data_i_129, target_column, feature_columns_4\n",
        ")\n",
        "\n",
        "if feature_importances_i_129_2 is not None:\n",
        "    plot_feature_importances(feature_importances_i_129_2, 'I-129', 'without')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_combined_feature_importances(all_feature_importances, analyte_list, concentration_status_list):\n",
        "    all_features = pd.concat(all_feature_importances).index.unique()\n",
        "    initial_concentrations = ['Sr_90_initial_concentration', 'I_129_initial_concentration']\n",
        "    all_features = [f for f in initial_concentrations if f in all_features] + [f for f in all_features if f not in initial_concentrations]\n",
        "    colors = pcolors.qualitative.Plotly\n",
        "    feature_color_map = {feature: colors[i % len(colors)] for i, feature in enumerate(all_features)}\n",
        "\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=['(a)', '(b)', '(c)', '(d)'],\n",
        "        specs=[[{'type': 'domain'}, {'type': 'domain'}], [{'type': 'domain'}, {'type': 'domain'}]],\n",
        "        horizontal_spacing=0.001,\n",
        "        vertical_spacing=0.05\n",
        "    )\n",
        "\n",
        "    for i, (feature_importances, analyte, concentration_status) in enumerate(zip(all_feature_importances, analyte_list, concentration_status_list)):\n",
        "        feature_importances = feature_importances.reindex(all_features).fillna(0).sort_values(ascending=False)\n",
        "        feature_colors = [feature_color_map[feature] for feature in feature_importances.index]\n",
        "        fig.add_trace(go.Pie(\n",
        "            labels=feature_importances.index,\n",
        "            values=feature_importances,\n",
        "            marker=dict(colors=feature_colors),\n",
        "            hoverinfo='label+percent',\n",
        "            textinfo='percent',\n",
        "            textposition='inside',\n",
        "            showlegend=False\n",
        "        ), row=(i//2)+1, col=(i%2)+1)\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text='',\n",
        "        font=dict(family=\"Computer Modern, serif\", size=16),\n",
        "        showlegend=True,\n",
        "        legend=dict(\n",
        "            title=\"Features\",\n",
        "            orientation=\"h\",\n",
        "            y=-0.05,\n",
        "            x=0.5,\n",
        "            xanchor='center',\n",
        "            traceorder='normal'\n",
        "        ),\n",
        "        height=1100,\n",
        "        width=1000,\n",
        "        margin=dict(t=100, b=50, l=50, r=50),\n",
        "        paper_bgcolor='white',\n",
        "        plot_bgcolor='white'\n",
        "    )\n",
        "\n",
        "    fig.update_xaxes(showgrid=False, zeroline=False, showticklabels=False)\n",
        "    fig.update_yaxes(showgrid=False, zeroline=False, showticklabels=False)\n",
        "\n",
        "    for annotation in fig['layout']['annotations']:\n",
        "        annotation['font'] = dict(size=26, color='black', family=\"Times New Roman, serif\", weight=\"bold\")  # Larger font, bold\n",
        "\n",
        "    unique_handles = {feature: go.Scatter(\n",
        "        x=[None], y=[None], mode='markers',\n",
        "        marker=dict(size=10, color=feature_color_map[feature]),\n",
        "        legendgroup=feature, showlegend=True, name=feature\n",
        "    ) for feature in all_features}\n",
        "\n",
        "    for handle in unique_handles.values():\n",
        "        fig.add_trace(handle)\n",
        "\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "1dl0ZB8q6UiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_feature_importances = [\n",
        "    feature_importances_sr_90_1,\n",
        "    feature_importances_sr_90_2,\n",
        "    feature_importances_i_129_1,\n",
        "    feature_importances_i_129_2\n",
        "]\n",
        "\n",
        "analyte_list = ['Sr-90', 'Sr-90', 'I-129', 'I-129']\n",
        "concentration_status_list = ['with concentration', 'without concentration', 'with concentration', 'without concentration']\n",
        "\n",
        "plot_combined_feature_importances(all_feature_importances, analyte_list, concentration_status_list)"
      ],
      "metadata": {
        "id": "WXb-9AqF6fuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxUGRi-o2wrx"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYhNhShf0xo_"
      },
      "outputs": [],
      "source": [
        "plt.style.use(['science', 'bright'])\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"text.usetex\": False,\n",
        "    \"font.size\": 16\n",
        "})\n",
        "\n",
        "def get_performance_metrics(pipeline, X, y):\n",
        "    y_pred = pipeline.predict(X)\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    r2 = r2_score(y, y_pred)\n",
        "    return r2, mse\n",
        "\n",
        "metrics = ['R² Score', 'Mean Squared Error']\n",
        "sr90_with = [r2_sr90_with, mse_sr90_with]\n",
        "sr90_without = [r2_sr90_without, mse_sr90_without]\n",
        "i129_with = [r2_i129_with, mse_i129_with]\n",
        "i129_without = [r2_i129_without, mse_i129_without]\n",
        "\n",
        "labels = ['R² Score', 'Mean Squared Error']\n",
        "groups = ['Sr-90 with Analyte', 'Sr-90 without Analyte', 'I-129 with Analyte', 'I-129 without Analyte']\n",
        "colors = cm.jet(np.linspace(0, 1, len(groups)))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.bar(groups, [r2_sr90_with, r2_sr90_without, r2_i129_with, r2_i129_without], color=colors)\n",
        "ax.set_ylabel('R² Score')\n",
        "ax.set_title('R² Score by Model and Analyte')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.bar(groups, [mse_sr90_with, mse_sr90_without, mse_i129_with, mse_i129_without], color=colors)\n",
        "ax.set_ylabel('Mean Squared Error')\n",
        "ax.set_title('Mean Squared Error by Model and Analyte')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CF using analytical methods**"
      ],
      "metadata": {
        "id": "mE5LlhEvvAwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import t\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils import resample"
      ],
      "metadata": {
        "id": "xhWPvxBJ9cTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analytical_prediction_intervals(pipeline, X, y, alpha=0.05):\n",
        "    pipeline.fit(X, y)\n",
        "    y_pred = pipeline.predict(X)\n",
        "    residuals = y - y_pred\n",
        "\n",
        "    n = len(y)\n",
        "    p = X.shape[1]\n",
        "    dof = n - p - 1\n",
        "\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "\n",
        "    try:\n",
        "        X = X.astype(np.float64)\n",
        "        XTX_inv = np.linalg.inv(np.dot(X.T, X))\n",
        "        se = np.sqrt(mse * (1 + np.diagonal(np.dot(X, np.dot(XTX_inv, X.T)))))\n",
        "    except np.linalg.LinAlgError:\n",
        "        se = np.full_like(y_pred, np.nan)\n",
        "    except ValueError:\n",
        "        se = np.full_like(y_pred, np.nan)\n",
        "\n",
        "    t_value = t.ppf(1 - alpha / 2, dof)\n",
        "\n",
        "    lower_bound = y_pred - t_value * se\n",
        "    upper_bound = y_pred + t_value * se\n",
        "\n",
        "    return lower_bound, upper_bound, y_pred\n",
        "\n",
        "ci_results = {}\n",
        "\n",
        "datasets = [\n",
        "    ('Sr-90 with analyte concentration', RandomForestRegressor(), merged_data_sr_90, feature_columns_1),\n",
        "    ('Sr-90 without analyte concentration', RandomForestRegressor(), merged_data_sr_90, feature_columns_3),\n",
        "    ('I-129 with analyte concentration', RandomForestRegressor(), merged_data_i_129, feature_columns_2),\n",
        "    ('I-129 without analyte concentration', RandomForestRegressor(), merged_data_i_129, feature_columns_4)\n",
        "]\n",
        "\n",
        "for name, model, data, feature_columns in datasets:\n",
        "    X = data[feature_columns]\n",
        "    y = data[target_column]\n",
        "\n",
        "    categorical_column = 'aquifer'\n",
        "    if categorical_column in X.columns:\n",
        "        X = pd.get_dummies(X, columns=[categorical_column], drop_first=True)\n",
        "\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    y = pd.to_numeric(y, errors='coerce')\n",
        "\n",
        "    valid_mask = ~X.isna().any(axis=1) & ~y.isna()\n",
        "    X = X[valid_mask]\n",
        "    y = y[valid_mask]\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', LinearRegression())\n",
        "    ])\n",
        "\n",
        "    lower_bound, upper_bound, predictions = analytical_prediction_intervals(pipeline, X, y)\n",
        "    ci_results[name] = {'lower_bound': lower_bound, 'upper_bound': upper_bound, 'predictions': predictions}\n",
        "\n",
        "for name, ci_data in ci_results.items():\n",
        "    data_for_current_trial = datasets[[ds[0] for ds in datasets].index(name)][2]\n",
        "    well_names = data_for_current_trial['STATION_ID']\n",
        "\n",
        "    X = data_for_current_trial[feature_columns]\n",
        "    y = data_for_current_trial[target_column]\n",
        "\n",
        "    categorical_column = 'aquifer'\n",
        "    if categorical_column in X.columns:\n",
        "        X = pd.get_dummies(X, columns=[categorical_column], drop_first=True)\n",
        "\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    y = pd.to_numeric(y, errors='coerce')\n",
        "\n",
        "    valid_mask = ~X.isna().any(axis=1) & ~y.isna()\n",
        "    well_names = well_names[valid_mask]\n",
        "    X = X[valid_mask]\n",
        "    y = y[valid_mask]\n",
        "\n",
        "    predictions_in_years = ci_data['predictions'] / 365.25\n",
        "    lower_bound_in_years = ci_data['lower_bound'] / 365.25\n",
        "    upper_bound_in_years = ci_data['upper_bound'] / 365.25\n",
        "    confidence_interval_in_years = (ci_data['upper_bound'] - ci_data['lower_bound']) / 365.25\n",
        "    confidence_interval_percentage = (confidence_interval_in_years / predictions_in_years) * 100\n",
        "\n",
        "    predictions_in_years = np.where(predictions_in_years < 0, '[0]', predictions_in_years)\n",
        "    lower_bound_in_years = np.where(lower_bound_in_years < 0, '[0]', lower_bound_in_years)\n",
        "    upper_bound_in_years = np.where(upper_bound_in_years < 0, '[0]', upper_bound_in_years)\n",
        "    confidence_interval_in_years = np.where(confidence_interval_in_years < 0, '[0]', confidence_interval_in_years)\n",
        "    confidence_interval_percentage = np.where(confidence_interval_percentage < 0, '[0]', confidence_interval_percentage)\n",
        "\n",
        "    table = pd.DataFrame({\n",
        "        'Station ID': well_names,\n",
        "        'Prediction (in years)': predictions_in_years,\n",
        "        'Lower Bound (in years)': lower_bound_in_years,\n",
        "        'Upper Bound (in years)': upper_bound_in_years,\n",
        "        'Confidence Interval (in years)': confidence_interval_in_years,\n",
        "        'Confidence Interval (%)': confidence_interval_percentage\n",
        "    })\n",
        "    print(f'Table for {name}:')\n",
        "    print(table)\n",
        "    table.to_csv(f'{name}_confidence_intervals_in_years.csv', index=False)"
      ],
      "metadata": {
        "id": "i2x02dwNwOfr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "39aaf84606e21579959f93fb3c45e9e3d37c51d84ce27b2546e2ee0530e04c07"
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit ('elev': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}