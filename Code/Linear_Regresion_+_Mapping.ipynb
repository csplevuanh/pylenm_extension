{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olxOIIAvUIS1"
      },
      "source": [
        "# **SET UP**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SciencePlots"
      ],
      "metadata": {
        "id": "izPweIMH0eRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2JbZiOxdYE9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.colors as mcolors\n",
        "import scienceplots\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import t, linregress\n",
        "from matplotlib.dates import date2num, num2date\n",
        "import datetime\n",
        "from datetime import datetime, date, timedelta\n",
        "import folium\n",
        "import branca.colormap\n",
        "from geopy.geocoders import Nominatim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyOVU1hv7iuo"
      },
      "source": [
        "# **Newly updated MCL function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyMbDXdB0_95"
      },
      "outputs": [],
      "source": [
        "class DataAnalyzer:\n",
        "    def __init__(self, data_or_url):\n",
        "        if isinstance(data_or_url, str):  # Assuming file path is a string\n",
        "            self.data_url = data_or_url\n",
        "            self.data = self.load_data()\n",
        "        elif isinstance(data_or_url, pd.DataFrame):  # Assuming DataFrame\n",
        "            self.data = data_or_url\n",
        "        else:\n",
        "            raise ValueError(\"Input must be a file path or a pandas DataFrame\")\n",
        "\n",
        "    def load_data(self):\n",
        "        file_extension = self.data_url.split('.')[-1]\n",
        "        if file_extension == 'csv':\n",
        "            return pd.read_csv(self.data_url)\n",
        "        elif file_extension in ['xlsx', 'xlsm', 'xltx', 'xltm']:  # Add more excel file extensions if needed\n",
        "            return pd.read_excel(self.data_url)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "    def print_column_names(self):\n",
        "        print(\"Column names in the dataset:\", self.data.columns.tolist())\n",
        "\n",
        "    def preprocess_data(self, well_name_column, analyte_name_column, well_name, analyte_name):\n",
        "        filtered_data = self.data[(self.data[well_name_column] == well_name) & (self.data[analyte_name_column] == analyte_name)]\n",
        "        filtered_data = filtered_data.dropna(subset=['COLLECTION_DATE', 'RESULT'])\n",
        "        filtered_data['COLLECTION_DATE'] = pd.to_datetime(filtered_data['COLLECTION_DATE']).dt.date\n",
        "        filtered_data['RESULT_LOG'] = np.log10(filtered_data['RESULT'])\n",
        "        return filtered_data\n",
        "\n",
        "    def remove_outliers(self, data, result_column='RESULT_LOG', z_threshold=4):\n",
        "        z = np.abs(stats.zscore(data[result_column]))\n",
        "        return data[(z < z_threshold)]\n",
        "\n",
        "    def predict_time_at_MCL(self, slope, intercept, log_MCL, earliest_date):\n",
        "        if slope >= 0:\n",
        "            return None\n",
        "        time_at_MCL = (log_MCL - intercept) / slope\n",
        "        try:\n",
        "            time_at_MCL_date = num2date(time_at_MCL).replace(tzinfo=None).date()\n",
        "            if earliest_date.year < 1 or time_at_MCL_date.year >= 10000 or time_at_MCL_date <= earliest_date:\n",
        "                return None\n",
        "            return time_at_MCL_date\n",
        "        except ValueError:\n",
        "            return None\n",
        "\n",
        "    def adjust_date_based_on_confidence(self, base_date, slope, confidence_interval, days_per_unit):\n",
        "        if base_date is None or slope == 0:\n",
        "            return None, None, None\n",
        "        days_adjustment = confidence_interval / abs(slope) * days_per_unit\n",
        "        upper_date = base_date + timedelta(days=days_adjustment) if base_date else None\n",
        "        lower_date = base_date - timedelta(days=days_adjustment) if base_date else None\n",
        "        return lower_date, upper_date\n",
        "\n",
        "    def generate_analyte_summary(self, analyte_name_column, analyte_name, well_name_column, MCL):\n",
        "        unique_wells = self.data[self.data[analyte_name_column] == analyte_name][well_name_column].unique()\n",
        "        summary = []\n",
        "\n",
        "        for well in unique_wells:\n",
        "            query = self.preprocess_data(well_name_column, analyte_name_column, well, analyte_name)\n",
        "            if query.empty or len(query['COLLECTION_DATE'].unique()) < 2:\n",
        "                continue\n",
        "\n",
        "            x = date2num(query['COLLECTION_DATE'])\n",
        "            y = query['RESULT_LOG']\n",
        "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "\n",
        "            if slope > 0:\n",
        "                trend = 'Increase'\n",
        "            elif slope < 0:\n",
        "                trend = 'Decrease'\n",
        "            else:\n",
        "                trend = 'Neutral'\n",
        "\n",
        "            if trend == 'Neutral':\n",
        "                continue\n",
        "\n",
        "            log_MCL = np.log10(MCL)\n",
        "            time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, min(query['COLLECTION_DATE']))\n",
        "\n",
        "            # Calculate total decrease duration (for wells with decreasing trends)\n",
        "            total_decrease_duration = None\n",
        "            if trend == 'Decrease' and time_at_MCL:\n",
        "                total_decrease_duration = (time_at_MCL - min(query['COLLECTION_DATE'])).days\n",
        "\n",
        "            df = len(x) - 2\n",
        "            ts = t.ppf(1 - 0.025, df)  # Two-sided t-critical value for 95% CI\n",
        "            ci_slope = ts * std_err\n",
        "\n",
        "            lower_date, upper_date = None, None\n",
        "            if time_at_MCL and isinstance(time_at_MCL, date):\n",
        "                lower_date, upper_date = self.adjust_date_based_on_confidence(time_at_MCL, slope, ci_slope, 365.25)\n",
        "\n",
        "            summary.append({\n",
        "                'well_name': well,\n",
        "                'slope': slope,\n",
        "                'trend': trend,\n",
        "                'time@MCL': time_at_MCL,\n",
        "                'lower_time@MCL': lower_date,\n",
        "                'upper_time@MCL': upper_date,\n",
        "                'total_decrease_duration': total_decrease_duration  # Add total decrease duration\n",
        "            })\n",
        "\n",
        "        summary_df = pd.DataFrame(summary)\n",
        "        return summary_df\n",
        "\n",
        "    def compile_results(self, well_analyte_pairs, well_name_column, analyte_name_column, MCL):\n",
        "        results = []\n",
        "        for well_name, analyte_name in well_analyte_pairs:\n",
        "            time_at_MCL = None\n",
        "            query = self.preprocess_data(well_name_column, analyte_name_column, well_name, analyte_name)\n",
        "            if query.empty:\n",
        "                continue\n",
        "\n",
        "            query = self.remove_outliers(query)\n",
        "            x = date2num(query['COLLECTION_DATE'])\n",
        "            y = query['RESULT_LOG']\n",
        "\n",
        "            if len(x) < 2:\n",
        "                continue\n",
        "\n",
        "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "            df = len(x) - 2\n",
        "            ts = t.ppf(1 - 0.025, df)\n",
        "\n",
        "            trend = \"Increase\" if slope > 0 else \"Decrease\" if slope < 0 else \"Neutral\"\n",
        "            log_MCL = np.log10(MCL) if MCL else None\n",
        "            earliest_date = min(query['COLLECTION_DATE'])\n",
        "            time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, earliest_date) if MCL else None\n",
        "\n",
        "            # Calculate total decrease duration (for wells with decreasing trends)\n",
        "            total_decrease_duration = None\n",
        "            if trend == 'Decrease' and time_at_MCL:\n",
        "                total_decrease_duration = (time_at_MCL - min(query['COLLECTION_DATE'])).days\n",
        "\n",
        "            lower_date, upper_date = None, None\n",
        "            if time_at_MCL and isinstance(time_at_MCL, date):\n",
        "                lower_date, upper_date = self.adjust_date_based_on_confidence(time_at_MCL, slope, ts * std_err, 365.25)\n",
        "\n",
        "            results.append({\n",
        "                'well_name': well_name,\n",
        "                'slope': slope,\n",
        "                'trend': trend,\n",
        "                'time@MCL': time_at_MCL,\n",
        "                'lower_time@MCL': lower_date,\n",
        "                'upper_time@MCL': upper_date,\n",
        "                'total_decrease_duration': total_decrease_duration  # Add total decrease duration\n",
        "            })\n",
        "\n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_df.to_csv('time2MCL_results.csv', index=False)\n",
        "        return results_df\n",
        "\n",
        "    def plot_MCL(self, well_name_column, analyte_name_column, well_name, analyte_name, year_interval=20, save_dir='plot_MCL', MCL=None, extrapolate_until_year=2025):\n",
        "        plt.style.use(['science', 'no-latex'])  # Using the SciencePlots package with 'science' style\n",
        "\n",
        "        # Set the global font size parameters\n",
        "        plt.rcParams.update({\n",
        "            'font.size': 18,           # General font size\n",
        "            'axes.titlesize': 18,      # Title font size\n",
        "            'axes.labelsize': 18,      # Axis labels font size\n",
        "            'xtick.labelsize': 18,     # X-tick label font size\n",
        "            'ytick.labelsize': 18,     # Y-tick label font size\n",
        "            'legend.fontsize': 18,     # Legend font size\n",
        "            'figure.titlesize': 18     # Figure title font size\n",
        "        })\n",
        "\n",
        "        query = self.preprocess_data(well_name_column, analyte_name_column, well_name, analyte_name)\n",
        "        if query.empty:\n",
        "            print(f'No results found for {well_name} and {analyte_name}')\n",
        "            return\n",
        "\n",
        "        query = self.remove_outliers(query, result_column='RESULT_LOG')\n",
        "        x = date2num(query['COLLECTION_DATE'])\n",
        "        y = query['RESULT_LOG']\n",
        "\n",
        "        if len(x) < 2:\n",
        "            print(\"Not enough data for regression.\")\n",
        "            return\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "        df = len(x) - 2  # degrees of freedom for t-distribution\n",
        "        ts = t.ppf(1 - 0.025, df)  # 95% confidence interval multiplier\n",
        "\n",
        "        log_MCL = np.log10(MCL) if MCL else None\n",
        "        time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, min(query['COLLECTION_DATE'])) if MCL and slope < 0 else 'Not available'\n",
        "\n",
        "        lower_date, upper_date = None, None\n",
        "        if time_at_MCL and isinstance(time_at_MCL, date):\n",
        "            lower_date, upper_date = self.adjust_date_based_on_confidence(time_at_MCL, slope, ts * std_err, 365.25)\n",
        "\n",
        "        # Prepare for extrapolation\n",
        "        extrapolate_date = np.datetime64(f'{extrapolate_until_year}-12-31')\n",
        "        x_extrapolate = np.linspace(x.min(), date2num(extrapolate_date), 100)\n",
        "        y_extrapolate = slope * x_extrapolate + intercept\n",
        "\n",
        "        # Ensure all plots are of the same size\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # Disable the grid mode\n",
        "        plt.grid(False)\n",
        "\n",
        "        # Only show the data point dots, no associated numbers\n",
        "        plt.scatter(query['COLLECTION_DATE'], y, label='Data Points')\n",
        "        plt.xlim([num2date(x.min()), num2date(x_extrapolate.max())])\n",
        "        plt.ylim([0.0, max(y.max(), y_extrapolate.max())])\n",
        "        plt.plot(num2date(x_extrapolate), y_extrapolate, 'r--', label=f'Extrapolated Regression (Decrease)')\n",
        "\n",
        "        if log_MCL is not None:\n",
        "            plt.axhline(y=log_MCL, color='green', linestyle='--', label=f'MCL (log10): {log_MCL:.2f}')\n",
        "\n",
        "        # Adjusting position of the parameter box to avoid overlap with axis values\n",
        "        plt.figtext(0.15, 0.02, f\"Slope: {slope:.2e}\\nTime@MCL: {time_at_MCL}\\nLower CI: {lower_date}\\nUpper CI: {upper_date}\", fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.xlabel('Collection Date')\n",
        "        plt.ylabel('Log-Concentration (pCi/mL)')\n",
        "        plt.title(f'{well_name} - {analyte_name}')\n",
        "        plt.legend()\n",
        "\n",
        "        # Set a specific year interval for ticks so that around 10 ticks are shown\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(base=year_interval))  # Adjust year interval (20 years in this case)\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.gcf().autofmt_xdate()\n",
        "\n",
        "        plt.xlim([num2date(x.min()), num2date(x_extrapolate.max())])\n",
        "\n",
        "        figure_path = f'{save_dir}/{well_name}-{analyte_name}.png'\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        plt.savefig(figure_path)\n",
        "\n",
        "        # Close the plot to avoid showing it multiple times\n",
        "        # plt.close()\n",
        "\n",
        "        return figure_path\n",
        "\n",
        "    def generate_analysis_dataframe(self, analyte_name_column, well_name_column, analyte_name, MCL):\n",
        "        analyte_summary_df = self.generate_analyte_summary(analyte_name_column, analyte_name, well_name_column, MCL)\n",
        "        analysis_data = []\n",
        "\n",
        "        for index, row in analyte_summary_df.iterrows():\n",
        "            if row['trend'] == 'Neutral':\n",
        "                continue\n",
        "\n",
        "            plot_path = self.plot_MCL(well_name_column, analyte_name_column, row['well_name'], analyte_name, year_interval=5, save_dir='plot_MCL', MCL=MCL)\n",
        "\n",
        "            analysis_data.append({\n",
        "                'analyte_name': analyte_name,\n",
        "                'well_name': row['well_name'],\n",
        "                'slope': row['slope'],\n",
        "                'trend': row['trend'],\n",
        "                'time@MCL': row['time@MCL'],\n",
        "                'lower_time@MCL': row['lower_time@MCL'],\n",
        "                'upper_time@MCL': row['upper_time@MCL'],\n",
        "                'total_decrease_duration': row['total_decrease_duration'],\n",
        "                'path_to_figure': plot_path\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(analysis_data)\n",
        "\n",
        "    def generate_decreasing_well_list(self, analyte_name_column, analyte_name, well_name_column, MCL):\n",
        "        analyte_summary_df = self.generate_analyte_summary(analyte_name_column, analyte_name, well_name_column, MCL)\n",
        "        decreasing_well_list = analyte_summary_df[analyte_summary_df['trend'] == 'Decrease']['well_name'].tolist()\n",
        "        return decreasing_well_list\n",
        "\n",
        "    def extract_location_data(self, well_name_column, decreasing_well_list):\n",
        "        location_data = self.data[self.data[well_name_column].isin(decreasing_well_list)]\n",
        "        return location_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlV_0Cy2ZB0K"
      },
      "outputs": [],
      "source": [
        "data_analyzer = DataAnalyzer('https://raw.githubusercontent.com/ALTEMIS-DOE/pylenm/master/notebooks/data/FASB_Data_thru_3Q2015_Reduced_Demo.csv')\n",
        "data_analyzer.print_column_names()  # This will print column names, helping you to identify the correct ones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOb4qjWIgn1h"
      },
      "source": [
        "# **Plot MCL for a specific well**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DjW54mrPRDW"
      },
      "outputs": [],
      "source": [
        "data_analyzer.plot_MCL(\n",
        "    well_name_column='STATION_ID',\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    well_name='FPZ  4A',\n",
        "    analyte_name='IODINE-129',\n",
        "    MCL=10,\n",
        "    extrapolate_until_year=2050\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht6HtEJWg8bW"
      },
      "source": [
        "# **Data Frame Table**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch0bHZHYhCi2"
      },
      "source": [
        "# ***Sr-90***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-kg_wJWM8YS"
      },
      "outputs": [],
      "source": [
        "analyte_summary_df_sr_90 = data_analyzer.generate_analyte_summary(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='STRONTIUM-90',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "print(analyte_summary_df_sr_90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbv1KNZ2hMOL"
      },
      "source": [
        "# ***I-129***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkwVLAD1Phlf"
      },
      "outputs": [],
      "source": [
        "analyte_summary_df_i_129 = data_analyzer.generate_analyte_summary(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='IODINE-129',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "print(analyte_summary_df_i_129)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ucizq6khY-a"
      },
      "source": [
        "# **Store Data Frame for a Specific Analyte**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrJ8VEHIkl7A"
      },
      "source": [
        "# ***Sr-90***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqIS5ND3kuPv"
      },
      "outputs": [],
      "source": [
        "sr_90_data_frame = data_analyzer.generate_analysis_dataframe(\n",
        "    'ANALYTE_NAME',\n",
        "    'STATION_ID',\n",
        "    'STRONTIUM-90',\n",
        "    10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qws3XeSJiBg0"
      },
      "source": [
        "# ***I-129***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VRV-4G_SkYN"
      },
      "outputs": [],
      "source": [
        "i_129_data_frame = data_analyzer.generate_analysis_dataframe(\n",
        "    'ANALYTE_NAME',\n",
        "    'STATION_ID',\n",
        "    'IODINE-129',\n",
        "    10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2N3tmzsllod"
      },
      "source": [
        "# **Report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOD4Ymjzl-qI"
      },
      "outputs": [],
      "source": [
        "def report_of_data_frame(df, original_data, analyte_name):\n",
        "    plt.style.use(['science', 'no-latex'])\n",
        "    plt.grid(False)\n",
        "\n",
        "    # Plot histogram\n",
        "    df['total_decrease_duration'].hist(bins=20)\n",
        "    plt.title('Histogram of Total Decrease Duration', fontsize=14) # Increase the font size\n",
        "    plt.xlabel('Total Decrease Duration (days)', fontsize=12) # Increase the font size\n",
        "    plt.ylabel('Frequency', fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "    # Key details\n",
        "    total_decrease_duration = df['total_decrease_duration'].sum()\n",
        "    print(f\"Overall Total Decrease Duration: {total_decrease_duration} days\")\n",
        "\n",
        "    trend_counts = df['trend'].value_counts()\n",
        "    print(f\"\\nTrend counts:\\n{trend_counts}\")\n",
        "\n",
        "    avg_decrease_duration = df[df['trend'] == 'Decrease']['total_decrease_duration'].mean()\n",
        "    print(f\"Average Decrease Duration for Decreasing Wells: {avg_decrease_duration} days\")\n",
        "\n",
        "    all_wells_count = original_data[original_data['ANALYTE_NAME'] == analyte_name]['STATION_ID'].nunique()\n",
        "    print(f\"\\nTotal number of wells in the original dataset for analyte '{analyte_name}': {all_wells_count}\")\n",
        "\n",
        "    decreasing_wells = df[df['trend'] == 'Decrease']\n",
        "    wells_less_than_10_years = decreasing_wells[decreasing_wells['total_decrease_duration'] < 3650].shape[0]\n",
        "    percentage_wells_less_than_10_years = (wells_less_than_10_years / decreasing_wells.shape[0]) * 100\n",
        "    print(f\"\\nNumber of decreasing wells with time-to-MCL less than 10 years: {wells_less_than_10_years}\")\n",
        "    print(f\"Percentage of decreasing wells with time-to-MCL less than 10 years: {percentage_wells_less_than_10_years:.2f}%\")\n",
        "\n",
        "    trends_per_class = df.groupby('well_name')['trend'].value_counts().unstack(fill_value=0)\n",
        "    print(f\"\\nTrend counts per class:\\n{trends_per_class}\")\n",
        "\n",
        "    overall_trend_counts = df['trend'].value_counts()\n",
        "    decreasing_wells_count = overall_trend_counts.get('Decrease', 0)\n",
        "    stable_wells_count = overall_trend_counts.get('Neutral', 0)\n",
        "    increasing_wells_count = overall_trend_counts.get('Increase', 0)\n",
        "    print(f\"\\nFor the whole analyte:\\nDecreasing: {decreasing_wells_count}\\nStable: {stable_wells_count}\\nIncreasing: {increasing_wells_count}\")\n",
        "\n",
        "    median_duration = df['total_decrease_duration'].median()\n",
        "    percentile_25 = np.percentile(df['total_decrease_duration'], 25)\n",
        "    percentile_75 = np.percentile(df['total_decrease_duration'], 75)\n",
        "    print(f\"\\nMedian Decrease Duration: {median_duration} days\")\n",
        "    print(f\"25th Percentile Decrease Duration: {percentile_25} days\")\n",
        "    print(f\"75th Percentile Decrease Duration: {percentile_75} days\")\n",
        "\n",
        "    skewness = stats.skew(df['total_decrease_duration'].dropna())\n",
        "    print(f\"Skewness of Decrease Duration Distribution: {skewness}\")\n",
        "\n",
        "    kurtosis = stats.kurtosis(df['total_decrease_duration'].dropna())\n",
        "    print(f\"Kurtosis of Decrease Duration Distribution: {kurtosis}\")\n",
        "\n",
        "    patterns = {\n",
        "        'FSB**D': 'FSB\\\\d+D$',\n",
        "        'FSB**DR': 'FSB\\\\d+DR$',\n",
        "        'FSB Number Only': '^FSB\\\\d+$',\n",
        "        'FPZ': '^FPZ'\n",
        "    }\n",
        "\n",
        "    for category, pattern in patterns.items():\n",
        "        print(f\"\\nAnalysis for {category}:\")\n",
        "        category_well_count = original_data[original_data['STATION_ID'].str.contains(pattern, regex=True)]['STATION_ID'].nunique()\n",
        "        print(f\"Number of wells for {category}: {category_well_count}\")\n",
        "\n",
        "        category_df = df[df['well_name'].str.contains(pattern, regex=True)]\n",
        "\n",
        "        category_trend_counts = category_df['trend'].value_counts()\n",
        "        print(f\"Trend counts for {category}:\\n{category_trend_counts}\")\n",
        "\n",
        "        below_MCL_count = category_df[category_df['total_decrease_duration'] <= 0].shape[0]\n",
        "        print(f\"Wells already below MCL for {category}: {below_MCL_count}\")\n",
        "\n",
        "        if not category_df.empty:\n",
        "            category_median_duration = category_df['total_decrease_duration'].median()\n",
        "            category_skewness = stats.skew(category_df['total_decrease_duration'].dropna())\n",
        "            print(f\"Median Decrease Duration for {category}: {category_median_duration} days\")\n",
        "            print(f\"Skewness of Decrease Duration Distribution for {category}: {category_skewness}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiGhT0VUs-HE"
      },
      "outputs": [],
      "source": [
        "report_of_data_frame(sr_90_data_frame, data_analyzer.data, 'STRONTIUM-90')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPBAOpITyTZu"
      },
      "outputs": [],
      "source": [
        "report_of_data_frame(i_129_data_frame, data_analyzer.data, 'IODINE-129')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_analytes_histogram(df1, df2, analyte1_name, analyte2_name):\n",
        "    plt.style.use(['science', 'no-latex'])\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 12))  # Changed layout to vertical\n",
        "    df1_filtered = df1['total_decrease_duration'].dropna()\n",
        "    df2_filtered = df2['total_decrease_duration'].dropna()\n",
        "    axes[0].grid(False)\n",
        "    axes[1].grid(False)\n",
        "    hist1, bins1 = np.histogram(df1_filtered, bins=20)\n",
        "    hist2, bins2 = np.histogram(df2_filtered, bins=20)\n",
        "\n",
        "    # Plot for analyte 1\n",
        "    axes[0].hist(df1_filtered, bins=20)\n",
        "    axes[0].set_title(f'(a)', fontsize=14)\n",
        "    axes[0].set_xlabel('Total Decrease Duration (days)', fontsize=12)\n",
        "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "\n",
        "    # Plot for analyte 2\n",
        "    axes[1].hist(df2_filtered, bins=20)\n",
        "    axes[1].set_title(f'(b)', fontsize=14)\n",
        "    axes[1].set_xlabel('Total Decrease Duration (days)', fontsize=12)\n",
        "    axes[1].set_ylabel('Frequency', fontsize=12)\n",
        "\n",
        "    # Other details\n",
        "    def round_up_to_nearest(value, factor):\n",
        "        return np.ceil(value / factor) * factor\n",
        "\n",
        "    max_duration = max(bins1.max(), bins2.max())\n",
        "    max_frequency = max(hist1.max(), hist2.max())\n",
        "    max_duration_rounded = round_up_to_nearest(max_duration, 50000)\n",
        "    max_frequency_rounded = round_up_to_nearest(max_frequency, 5) + 5\n",
        "\n",
        "    # Adjust the axis ranges across both subplots based on the analyte with higher values\n",
        "    axes[0].set_xlim(0, max_duration_rounded)\n",
        "    axes[1].set_xlim(0, max_duration_rounded)\n",
        "    axes[0].set_ylim(0, max_frequency_rounded)\n",
        "    axes[1].set_ylim(0, max_frequency_rounded)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "0gxQ0ErsJB3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_analytes_histogram(sr_90_data_frame, i_129_data_frame, 'STRONTIUM-90', 'IODINE-129')"
      ],
      "metadata": {
        "id": "f2UytbtJJJ07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_log_time_to_MCL(df, analyte_name, font_size=14, title_font_size=18):\n",
        "    valid_mcl_data = df[df['total_decrease_duration'].notnull()]\n",
        "    valid_mcl_data = valid_mcl_data[valid_mcl_data['total_decrease_duration'] > 0]\n",
        "\n",
        "    # Compute the log(total_decrease_duration)\n",
        "    valid_mcl_data['log_time_to_MCL'] = np.log(valid_mcl_data['total_decrease_duration'])\n",
        "\n",
        "    #Plotting\n",
        "    plt.style.use(['science', 'no-latex'])  # Ensure SciencePlots is used\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.hist(valid_mcl_data['log_time_to_MCL'], bins=20, edgecolor=None, alpha=0.7)\n",
        "\n",
        "    plt.title(f'Log(Time-to-MCL) Distribution for {analyte_name}', fontsize=title_font_size)\n",
        "    plt.xlabel('Log(Time-to-MCL in days)', fontsize=font_size)\n",
        "    plt.ylabel('Frequency', fontsize=font_size)\n",
        "\n",
        "    plt.xticks(fontsize=font_size)\n",
        "    plt.yticks(fontsize=font_size)\n",
        "\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "    return valid_mcl_data[['well_name', 'total_decrease_duration', 'log_time_to_MCL']]"
      ],
      "metadata": {
        "id": "6c7RpXLxvVCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time_to_mcl_sr_90 = plot_log_time_to_MCL(sr_90_data_frame, analyte_name='STRONTIUM-90', font_size=14, title_font_size=18)\n",
        "print(log_time_to_mcl_sr_90)"
      ],
      "metadata": {
        "id": "52H_B6Otvi7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time_to_mcl_i_129 = plot_log_time_to_MCL(i_129_data_frame, analyte_name='IODINE-129', font_size=14, title_font_size=18)\n",
        "print(log_time_to_mcl_i_129)"
      ],
      "metadata": {
        "id": "OBm4tJHAyG0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_log_time_to_MCL(df1, analyte1_name, df2, analyte2_name, font_size=14, title_font_size=18):\n",
        "    valid_mcl_data1 = df1[df1['total_decrease_duration'].notnull() & (df1['total_decrease_duration'] > 0)]\n",
        "    valid_mcl_data2 = df2[df2['total_decrease_duration'].notnull() & (df2['total_decrease_duration'] > 0)]\n",
        "\n",
        "    # Compute the log(total_decrease_duration) for both analytes\n",
        "    valid_mcl_data1['log_time_to_MCL'] = np.log(valid_mcl_data1['total_decrease_duration'])\n",
        "    valid_mcl_data2['log_time_to_MCL'] = np.log(valid_mcl_data2['total_decrease_duration'])\n",
        "\n",
        "    # Create vertical subplots\n",
        "    plt.style.use(['science', 'no-latex'])  # Ensure SciencePlots is used\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 12))  # Vertical alignment\n",
        "\n",
        "    # Function to round up to nearest specified factor\n",
        "    def round_up_to_nearest(value, factor):\n",
        "        return np.ceil(value / factor) * factor\n",
        "\n",
        "    # Plotting\n",
        "    hist1, bins1, _ = axes[0].hist(valid_mcl_data1['log_time_to_MCL'], bins=20, edgecolor=None, alpha=0.7)\n",
        "    hist2, bins2, _ = axes[1].hist(valid_mcl_data2['log_time_to_MCL'], bins=20, edgecolor=None, alpha=0.7)\n",
        "\n",
        "    axes[0].set_title(f'(a)', fontsize=title_font_size)\n",
        "    axes[1].set_title(f'(b)', fontsize=title_font_size)\n",
        "    axes[0].set_xlabel('Log(Time-to-MCL in days)', fontsize=font_size)\n",
        "    axes[1].set_xlabel('Log(Time-to-MCL in days)', fontsize=font_size)\n",
        "    axes[0].set_ylabel('Frequency', fontsize=font_size)\n",
        "    axes[1].set_ylabel('Frequency', fontsize=font_size)\n",
        "\n",
        "    axes[0].tick_params(axis='both', which='major', labelsize=font_size)\n",
        "    axes[1].tick_params(axis='both', which='major', labelsize=font_size)\n",
        "\n",
        "    max_duration = max(bins1.max(), bins2.max())\n",
        "    max_frequency = max(hist1.max(), hist2.max())\n",
        "\n",
        "    max_duration_rounded = round_up_to_nearest(max_duration, 0.5)  # Adjust for log-scale bins\n",
        "    max_frequency_rounded = round_up_to_nearest(max_frequency, 5) + 2.5  # Add 2.5 as a buffer\n",
        "\n",
        "    axes[0].set_xlim(0, max_duration_rounded)\n",
        "    axes[1].set_xlim(0, max_duration_rounded)\n",
        "    axes[0].set_ylim(0, max_frequency_rounded)\n",
        "    axes[1].set_ylim(0, max_frequency_rounded)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pKg52z77zbVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time_to_mcl_comparative = compare_log_time_to_MCL(sr_90_data_frame, 'STRONTIUM-90', i_129_data_frame, 'IODINE-129', font_size=14, title_font_size=18)"
      ],
      "metadata": {
        "id": "RV9Y23t0zjWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipcv_FDLzfJj"
      },
      "source": [
        "\n",
        "# **Mapping**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "import branca.colormap as cm\n",
        "from matplotlib import cm, colors as mcolors\n",
        "from folium.plugins import MeasureControl\n",
        "from geopy.distance import geodesic"
      ],
      "metadata": {
        "id": "C_r5UnEaqFQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "gGr_NB3ZcK3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_analyzer = DataAnalyzer('/content/gdrive/MyDrive/MIT Project - VAL @HW_CEE/FASB Well Construction Info.xlsx')"
      ],
      "metadata": {
        "id": "J71ZQU25c1Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_analyzer.print_column_names()"
      ],
      "metadata": {
        "id": "eJbdiT_a4SFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_real_distances(min_latitude, max_latitude, min_longitude, max_longitude):\n",
        "    width = geodesic((min_latitude, min_longitude), (min_latitude, max_longitude)).meters\n",
        "    height = geodesic((min_latitude, min_longitude), (max_latitude, min_longitude)).meters\n",
        "    return width, height"
      ],
      "metadata": {
        "id": "RsSqg4kuzAf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_map(df, duration_column, zoom_level_offset=10, satellite_mode=False, aquifer=None):\n",
        "    if aquifer:\n",
        "        df = df[df['aquifer'] == aquifer]\n",
        "\n",
        "    if df.empty:\n",
        "        print(f\"No wells found for the aquifer: {aquifer}\")\n",
        "        return None\n",
        "\n",
        "    # Center of the bounding box\n",
        "    center_latitude = df['latitude'].mean()\n",
        "    center_longitude = df['longitude'].mean()\n",
        "\n",
        "    # Zoom level\n",
        "    max_latitude = df['latitude'].max()\n",
        "    min_latitude = df['latitude'].min()\n",
        "    max_longitude = df['longitude'].max()\n",
        "    min_longitude = df['longitude'].min()\n",
        "    max_delta = max(max_latitude - min_latitude, max_longitude - min_longitude)\n",
        "    zoom_level = zoom_level_offset - int(np.log2(max_delta))  # Use the provided offset\n",
        "\n",
        "    # Determine map type (satellite or normal)\n",
        "    if satellite_mode:\n",
        "        m = folium.Map(location=[center_latitude, center_longitude], zoom_start=zoom_level, tiles=None)\n",
        "        folium.TileLayer(\n",
        "            tiles='https://{s}.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
        "            attr='Google',\n",
        "            name='Google Satellite',\n",
        "            max_zoom=20,\n",
        "            subdomains=['mt0', 'mt1', 'mt2', 'mt3']\n",
        "        ).add_to(m)\n",
        "    else:\n",
        "        m = folium.Map(location=[center_latitude, center_longitude], zoom_start=zoom_level)\n",
        "\n",
        "    conc_fg = folium.FeatureGroup(name=\"Concentrations\")\n",
        "\n",
        "    cmap_jet = cm.get_cmap('jet')\n",
        "    num_colors = 6  # Optional number of colors\n",
        "    color_list = [mcolors.rgb2hex(cmap_jet(i / num_colors)) for i in range(num_colors)]\n",
        "\n",
        "    df['duration_years'] = df[duration_column] / 365.25\n",
        "\n",
        "    df['log_duration_years'] = np.log(df['duration_years'])\n",
        "\n",
        "    min_val_log = df['log_duration_years'].min()\n",
        "    max_val_log = df['log_duration_years'].max()\n",
        "    df['normalized_log_duration'] = (df['log_duration_years'] - min_val_log) / (max_val_log - min_val_log)\n",
        "\n",
        "    cmap = mcolors.LinearSegmentedColormap.from_list('custom_colormap', color_list)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        coordinates = [row['latitude'], row['longitude']]\n",
        "        point_color = mcolors.rgb2hex(cmap(row['normalized_log_duration']))\n",
        "\n",
        "        # Creating popup content with specified information\n",
        "        popup_content = f\"<div style='max-width:800px;'><ul>\"\n",
        "        popup_content += f\"<li>Well Name: {row['well_name']}</li>\"\n",
        "        popup_content += f\"<li>Time to MCL: {row['total_decrease_duration']} days ({row['duration_years']:.2f} Years)</li>\"\n",
        "        popup_content += f\"<li>Aquifer: {row['aquifer']}</li>\"\n",
        "        popup_content += \"</ul></div>\"\n",
        "\n",
        "        folium.Circle(\n",
        "            radius=15,\n",
        "            location=coordinates,\n",
        "            popup=folium.Popup(popup_content, max_width=800),\n",
        "            fill_color=point_color,\n",
        "            color='#000000',\n",
        "            fill_opacity=1,\n",
        "            stroke=1,\n",
        "            weight=1\n",
        "        ).add_to(conc_fg)\n",
        "\n",
        "    m.add_child(conc_fg)\n",
        "\n",
        "    min_val = df[duration_column].min()\n",
        "    max_val = df[duration_column].max()\n",
        "    min_val_years = min_val / 365.25\n",
        "    max_val_years = max_val / 365.25\n",
        "\n",
        "    colormap_original = folium.LinearColormap(colors=color_list, vmin=min_val_years, vmax=max_val_years, caption=f\"{duration_column} (years)\")\n",
        "    colormap_original.add_to(m)\n",
        "\n",
        "    width_meters, height_meters = calculate_real_distances(min_latitude, max_latitude, min_longitude, max_longitude)\n",
        "\n",
        "    folium.Marker(\n",
        "        [min_latitude, (min_longitude + max_longitude) / 2],\n",
        "        icon=folium.DivIcon(html=f\"<div style='font-size: 12pt'>Width: {width_meters:.2f} meters</div>\")\n",
        "    ).add_to(m)\n",
        "    folium.Marker(\n",
        "        [(min_latitude + max_latitude) / 2, min_longitude],\n",
        "        icon=folium.DivIcon(html=f\"<div style='font-size: 12pt; transform: rotate(90deg)'>Height: {height_meters:.2f} meters</div>\")\n",
        "    ).add_to(m)\n",
        "\n",
        "    bounding_box = [\n",
        "        [min_latitude, min_longitude],\n",
        "        [min_latitude, max_longitude],\n",
        "        [max_latitude, max_longitude],\n",
        "        [max_latitude, min_longitude],\n",
        "        [min_latitude, min_longitude]\n",
        "    ]\n",
        "    folium.PolyLine(bounding_box, color='black', weight=2, opacity=1).add_to(m)\n",
        "\n",
        "    # Add vertical and horizontal lines of the map window\n",
        "    top_horizontal_line = [\n",
        "        [max_latitude, min_longitude],\n",
        "        [max_latitude, max_longitude]\n",
        "    ]\n",
        "    bottom_horizontal_line = [\n",
        "        [min_latitude, min_longitude],\n",
        "        [min_latitude, max_longitude]\n",
        "    ]\n",
        "    left_vertical_line = [\n",
        "        [min_latitude, min_longitude],\n",
        "        [max_latitude, min_longitude]\n",
        "    ]\n",
        "    right_vertical_line = [\n",
        "        [min_latitude, max_longitude],\n",
        "        [max_latitude, max_longitude]\n",
        "    ]\n",
        "\n",
        "    folium.PolyLine(top_horizontal_line, color='blue', weight=2, opacity=1).add_to(m)\n",
        "    folium.PolyLine(bottom_horizontal_line, color='blue', weight=2, opacity=1).add_to(m)\n",
        "    folium.PolyLine(left_vertical_line, color='blue', weight=2, opacity=1).add_to(m)\n",
        "    folium.PolyLine(right_vertical_line, color='blue', weight=2, opacity=1).add_to(m)\n",
        "\n",
        "    return m"
      ],
      "metadata": {
        "id": "IX8IVgEy1Ycz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sr-90**"
      ],
      "metadata": {
        "id": "wLNFZRfVD37G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame with decreasing trends\n",
        "decreasing_well_list = data_analyzer.generate_decreasing_well_list(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='STRONTIUM-90',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "\n",
        "# Extract latitude and longitude\n",
        "location_data = location_analyzer.extract_location_data(\n",
        "    well_name_column='station_id',\n",
        "    decreasing_well_list=decreasing_well_list\n",
        ")\n",
        "\n",
        "# Merge DataFrame with latitude and longitude information\n",
        "merged_df_sr_90 = pd.merge(analyte_summary_df_sr_90, location_data, left_on='well_name', right_on='station_id', how='inner')\n",
        "\n",
        "print(merged_df_sr_90)"
      ],
      "metadata": {
        "id": "6bxNSS5RP0Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_sr_90, 'total_decrease_duration')"
      ],
      "metadata": {
        "id": "XsMKCIXE0_ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_sr_90, 'total_decrease_duration', zoom_level_offset=11)"
      ],
      "metadata": {
        "id": "qVrTu3Q6122W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_sr_90, 'total_decrease_duration', satellite_mode=True)"
      ],
      "metadata": {
        "id": "EcCKzELXUzDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_sr_90, 'total_decrease_duration', aquifer='UAZ_UTRAU')"
      ],
      "metadata": {
        "id": "WWYFpIuE-ke8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_sr_90, 'total_decrease_duration', aquifer='LAZ_UTRAU')"
      ],
      "metadata": {
        "id": "OLuaepwW_VJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **I-129**"
      ],
      "metadata": {
        "id": "HO0hJkD8lBdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame with decreasing trends\n",
        "decreasing_well_list = data_analyzer.generate_decreasing_well_list(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='IODINE-129',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "\n",
        "# Extract latitude and longitude\n",
        "location_data = location_analyzer.extract_location_data(\n",
        "    well_name_column='station_id',\n",
        "    decreasing_well_list=decreasing_well_list\n",
        ")\n",
        "\n",
        "# Merge DataFrame with latitude and longitude information\n",
        "merged_df_i_129 = pd.merge(analyte_summary_df_i_129, location_data, left_on='well_name', right_on='station_id', how='inner')\n",
        "\n",
        "print(merged_df_i_129)"
      ],
      "metadata": {
        "id": "Ry-5iwJZYqeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_i_129, 'total_decrease_duration', zoom_level_offset=9)"
      ],
      "metadata": {
        "id": "_4ArcEyYqwyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_i_129, 'total_decrease_duration', satellite_mode=True)"
      ],
      "metadata": {
        "id": "TOekTzsGq3dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_i_129, 'total_decrease_duration', aquifer='UAZ_UTRAU')"
      ],
      "metadata": {
        "id": "sEWOE5cDAEUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_i_129, 'total_decrease_duration', aquifer='LAZ_UTRAU')"
      ],
      "metadata": {
        "id": "KAczlJkIAJsa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "39aaf84606e21579959f93fb3c45e9e3d37c51d84ce27b2546e2ee0530e04c07"
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit ('elev': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}