{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olxOIIAvUIS1"
      },
      "source": [
        "# **SET UP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2JbZiOxdYE9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import t, linregress\n",
        "from matplotlib.dates import date2num, num2date\n",
        "import datetime\n",
        "from datetime import datetime, date, timedelta\n",
        "import folium\n",
        "import branca.colormap\n",
        "from geopy.geocoders import Nominatim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyOVU1hv7iuo"
      },
      "source": [
        "# **Newly updated MCL function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77cuKlNQY6Cq"
      },
      "outputs": [],
      "source": [
        "class DataAnalyzer:\n",
        "    def __init__(self, data_or_url):\n",
        "        if isinstance(data_or_url, str):\n",
        "            self.data_url = data_or_url\n",
        "            self.data = self.load_data()\n",
        "        elif isinstance(data_or_url, pd.DataFrame):\n",
        "            self.data = data_or_url\n",
        "        else:\n",
        "            raise ValueError(\"Input must be a file path or a pandas DataFrame\")\n",
        "\n",
        "    def load_data(self):\n",
        "        file_extension = self.data_url.split('.')[-1]\n",
        "        if file_extension == 'csv':\n",
        "            return pd.read_csv(self.data_url)\n",
        "        elif file_extension in ['xlsx', 'xlsm', 'xltx', 'xltm']:\n",
        "            return pd.read_excel(self.data_url)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "    def print_column_names(self):\n",
        "        print(\"Column names in the dataset:\", self.data.columns.tolist())\n",
        "\n",
        "    def preprocess_data(self, well_name_column, analyte_name_column, well_name, analyte_name):\n",
        "        filtered_data = self.data[(self.data[well_name_column] == well_name) & (self.data[analyte_name_column] == analyte_name)]\n",
        "        filtered_data = filtered_data.dropna(subset=['COLLECTION_DATE', 'RESULT'])\n",
        "        filtered_data['COLLECTION_DATE'] = pd.to_datetime(filtered_data['COLLECTION_DATE']).dt.date\n",
        "        filtered_data['RESULT_LOG'] = np.log10(filtered_data['RESULT'])\n",
        "        return filtered_data\n",
        "\n",
        "    def remove_outliers(self, data, result_column='RESULT_LOG', z_threshold=4):\n",
        "        z = np.abs(stats.zscore(data[result_column]))\n",
        "        return data[(z < z_threshold)]\n",
        "\n",
        "    def predict_time_at_MCL(self, slope, intercept, log_MCL, earliest_date):\n",
        "        if slope >= 0:\n",
        "            return None\n",
        "        time_at_MCL = (log_MCL - intercept) / slope\n",
        "        try:\n",
        "            time_at_MCL_date = num2date(time_at_MCL).replace(tzinfo=None).date()\n",
        "            if earliest_date.year < 1 or time_at_MCL_date.year >= 10000 or time_at_MCL_date <= earliest_date:\n",
        "                return None\n",
        "            return time_at_MCL_date\n",
        "        except ValueError:\n",
        "            return None\n",
        "\n",
        "    def adjust_date_based_on_confidence(self, base_date, slope, confidence_interval, days_per_unit):\n",
        "        if base_date is None or slope == 0:\n",
        "            return None, None, None\n",
        "        days_adjustment = confidence_interval / abs(slope) * days_per_unit\n",
        "        upper_date = base_date + timedelta(days=days_adjustment) if base_date else None\n",
        "        lower_date = base_date - timedelta(days=days_adjustment) if base_date else None\n",
        "        return lower_date, upper_date\n",
        "\n",
        "    def generate_analyte_summary(self, analyte_name_column, analyte_name, well_name_column, MCL):\n",
        "        unique_wells = self.data[self.data[analyte_name_column] == analyte_name][well_name_column].unique()\n",
        "        summary = []\n",
        "\n",
        "        for well in unique_wells:\n",
        "            query = self.preprocess_data(well_name_column, analyte_name_column, well, analyte_name)\n",
        "            if query.empty or len(query['COLLECTION_DATE'].unique()) < 2:\n",
        "                continue\n",
        "\n",
        "            x = date2num(query['COLLECTION_DATE'])\n",
        "            y = query['RESULT_LOG']\n",
        "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "\n",
        "            if slope > 0:\n",
        "                trend = 'Increase'\n",
        "            elif slope < 0:\n",
        "                trend = 'Decrease'\n",
        "            else:\n",
        "                trend = 'Neutral'\n",
        "\n",
        "            if trend == 'Neutral':\n",
        "                continue\n",
        "\n",
        "            log_MCL = np.log10(MCL)\n",
        "            time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, min(query['COLLECTION_DATE']))\n",
        "            df = len(x) - 2\n",
        "            ts = t.ppf(1 - 0.025, df)\n",
        "            ci_slope = ts * std_err\n",
        "\n",
        "            lower_date, upper_date = None, None\n",
        "            if time_at_MCL and isinstance(time_at_MCL, date):\n",
        "                lower_date, upper_date = self.adjust_date_based_on_confidence(time_at_MCL, slope, ci_slope, 365.25)\n",
        "\n",
        "            summary.append({\n",
        "                'well_name': well,\n",
        "                'slope': slope,\n",
        "                'trend': trend,\n",
        "                'time@MCL': time_at_MCL,\n",
        "                'lower_time@MCL': lower_date,\n",
        "                'upper_time@MCL': upper_date\n",
        "            })\n",
        "\n",
        "        summary_df = pd.DataFrame(summary)\n",
        "        return summary_df\n",
        "\n",
        "    def compile_results(self, well_analyte_pairs, well_name_column, analyte_name_column, MCL):\n",
        "        results = []\n",
        "        for well_name, analyte_name in well_analyte_pairs:\n",
        "            time_at_MCL = None\n",
        "            query = self.preprocess_data(well_name_column, analyte_name_column, well_name, analyte_name)\n",
        "            if query.empty:\n",
        "                continue\n",
        "\n",
        "            query = self.remove_outliers(query)\n",
        "            x = date2num(query['COLLECTION_DATE'])\n",
        "            y = query['RESULT_LOG']\n",
        "\n",
        "            if len(x) < 2:\n",
        "                continue\n",
        "\n",
        "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "            df = len(x) - 2\n",
        "            ts = t.ppf(1 - 0.025, df)\n",
        "\n",
        "            trend = \"Increase\" if slope > 0 else \"Decrease\" if slope < 0 else \"Neutral\"\n",
        "            log_MCL = np.log10(MCL) if MCL else None\n",
        "            earliest_date = min(query['COLLECTION_DATE'])\n",
        "            time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, earliest_date) if MCL else None\n",
        "\n",
        "            lower_date, upper_date = None, None\n",
        "            if time_at_MCL and isinstance(time_at_MCL, date):\n",
        "                lower_date, upper_date = self.adjust_date_based_on_confidence(time_at_MCL, slope, ts * std_err, 365.25)\n",
        "\n",
        "            results.append({\n",
        "                'well_name': well_name,\n",
        "                'slope': slope,\n",
        "                'trend': trend,\n",
        "                'time@MCL': time_at_MCL,\n",
        "                'lower_time@MCL': lower_date,\n",
        "                'upper_time@MCL': upper_date\n",
        "            })\n",
        "\n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_df.to_csv('time2MCL_results.csv', index=False)\n",
        "        return results_df\n",
        "\n",
        "    def plot_MCL(self, well_name_column, analyte_name_column, well_name, analyte_name, year_interval=5, save_dir='plot_MCL', MCL=None, extrapolate_until_year=2025):\n",
        "        query = self.preprocess_data(well_name_column, analyte_name_column, well_name, analyte_name)\n",
        "        if query.empty:\n",
        "            print(f'No results found for {well_name} and {analyte_name}')\n",
        "            return\n",
        "\n",
        "        query = self.remove_outliers(query, result_column='RESULT_LOG')\n",
        "        x = date2num(query['COLLECTION_DATE'])\n",
        "        y = query['RESULT_LOG']\n",
        "\n",
        "        if len(x) < 2:\n",
        "            print(\"Not enough data for regression.\")\n",
        "            return\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "        df = len(x) - 2  # degrees of freedom for t-distribution\n",
        "        ts = t.ppf(1 - 0.025, df)  # 95% confidence interval multiplier\n",
        "\n",
        "        log_MCL = np.log10(MCL) if MCL else None\n",
        "        time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, min(query['COLLECTION_DATE'])) if MCL and slope < 0 else 'Not available'\n",
        "\n",
        "        lower_date, upper_date = None, None\n",
        "        if time_at_MCL and isinstance(time_at_MCL, date):\n",
        "            lower_date, upper_date = self.adjust_date_based_on_confidence(time_at_MCL, slope, ts * std_err, 365.25)\n",
        "\n",
        "        # Prepare for extrapolation\n",
        "        extrapolate_date = np.datetime64(f'{extrapolate_until_year}-12-31')\n",
        "        x_extrapolate = np.linspace(x.min(), date2num(extrapolate_date), 100)\n",
        "        y_extrapolate = slope * x_extrapolate + intercept\n",
        "\n",
        "        # Categorize and label the trend based on the slope\n",
        "        trend_label = 'Increase (+)' if slope > 0 else 'Decrease (-)' if slope < 0 else 'Neutral (/)'\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(query['COLLECTION_DATE'], y, label='Data Points')\n",
        "        for i, point in query.iterrows():\n",
        "            plt.text(point['COLLECTION_DATE'], point['RESULT_LOG'], str(i), fontsize=6, ha='center', va='center')\n",
        "        plt.plot(num2date(x_extrapolate), y_extrapolate, 'r--', label=f'Extrapolated Regression ({trend_label})')\n",
        "\n",
        "        if log_MCL is not None:\n",
        "            plt.axhline(y=log_MCL, color='green', linestyle='--', label=f'MCL (log10): {log_MCL:.2f}')\n",
        "\n",
        "        plt.figtext(0.15, 0.05, f\"Slope: {slope:.2e}\\nTime@MCL: {time_at_MCL}\\nLower CI: {lower_date}\\nUpper CI: {upper_date}\", fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        plt.xlabel('Collection Date')\n",
        "        plt.ylabel('Log-Concentration (pCi/mL)')\n",
        "        plt.title(f'{well_name} - {analyte_name}')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(year_interval))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.gcf().autofmt_xdate()\n",
        "\n",
        "        plt.xlim([num2date(x.min()), num2date(x_extrapolate.max())])\n",
        "\n",
        "        figure_path = f'{save_dir}/{well_name}-{analyte_name}.png'\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        plt.savefig(figure_path)\n",
        "        return figure_path\n",
        "\n",
        "    def generate_analysis_dataframe(self, analyte_name_column, well_name_column, analyte_name, MCL):\n",
        "        analyte_summary_df = self.generate_analyte_summary(analyte_name_column, analyte_name, well_name_column, MCL)\n",
        "        analysis_data = []\n",
        "\n",
        "        for index, row in analyte_summary_df.iterrows():\n",
        "            if row['trend'] == 'Neutral':\n",
        "                continue\n",
        "\n",
        "            plot_path = self.plot_MCL(well_name_column, analyte_name_column, row['well_name'], analyte_name, year_interval=5, save_dir='plot_MCL', MCL=MCL)\n",
        "\n",
        "            analysis_data.append({\n",
        "                'analyte_name': analyte_name,\n",
        "                'well_name': row['well_name'],\n",
        "                'slope': row['slope'],\n",
        "                'trend': row['trend'],\n",
        "                'time@MCL': row['time@MCL'],\n",
        "                'lower_time@MCL': row['lower_time@MCL'],\n",
        "                'upper_time@MCL': row['upper_time@MCL'],\n",
        "                'path_to_figure': plot_path\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(analysis_data)\n",
        "\n",
        "    def generate_decreasing_well_list(self, analyte_name_column, analyte_name, well_name_column, MCL):\n",
        "        analyte_summary_df = self.generate_analyte_summary(analyte_name_column, analyte_name, well_name_column, MCL)\n",
        "        decreasing_well_list = analyte_summary_df[analyte_summary_df['trend'] == 'Decrease']['well_name'].tolist()\n",
        "        return decreasing_well_list\n",
        "\n",
        "    def extract_location_data(self, well_name_column, decreasing_well_list):\n",
        "        location_data = self.data[self.data[well_name_column].isin(decreasing_well_list)]\n",
        "        return location_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlV_0Cy2ZB0K"
      },
      "outputs": [],
      "source": [
        "data_analyzer = DataAnalyzer('https://raw.githubusercontent.com/ALTEMIS-DOE/pylenm/master/notebooks/data/FASB_Data_thru_3Q2015_Reduced_Demo.csv')\n",
        "data_analyzer.print_column_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOb4qjWIgn1h"
      },
      "source": [
        "# **Plot MCL for a specific well**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DjW54mrPRDW"
      },
      "outputs": [],
      "source": [
        "data_analyzer.plot_MCL(\n",
        "    well_name_column='STATION_ID',\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    well_name='FBI 14D',\n",
        "    analyte_name='IODINE-129',\n",
        "    MCL=10,\n",
        "    extrapolate_until_year=2032\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht6HtEJWg8bW"
      },
      "source": [
        "# **Data Frame Table**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch0bHZHYhCi2"
      },
      "source": [
        "# ***Sr-90***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-kg_wJWM8YS"
      },
      "outputs": [],
      "source": [
        "analyte_summary_df_sr_90 = data_analyzer.generate_analyte_summary(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='STRONTIUM-90',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "print(analyte_summary_df_sr_90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbv1KNZ2hMOL"
      },
      "source": [
        "# ***I-129***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkwVLAD1Phlf"
      },
      "outputs": [],
      "source": [
        "analyte_summary_df_i_129 = data_analyzer.generate_analyte_summary(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='IODINE-129',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "print(analyte_summary_df_i_129)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ucizq6khY-a"
      },
      "source": [
        "# **Store Data Frame for a Specific Analyte**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrJ8VEHIkl7A"
      },
      "source": [
        "# ***Sr-90***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqIS5ND3kuPv"
      },
      "outputs": [],
      "source": [
        "sr_90_data_frame = data_analyzer.generate_analysis_dataframe(\n",
        "    'ANALYTE_NAME',\n",
        "    'STATION_ID',\n",
        "    'STRONTIUM-90',\n",
        "    10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qws3XeSJiBg0"
      },
      "source": [
        "# ***I-129***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VRV-4G_SkYN"
      },
      "outputs": [],
      "source": [
        "i_129_data_frame = data_analyzer.generate_analysis_dataframe(\n",
        "    'ANALYTE_NAME',\n",
        "    'STATION_ID',\n",
        "    'IODINE-129',\n",
        "    10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2N3tmzsllod"
      },
      "source": [
        "# **Report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOD4Ymjzl-qI"
      },
      "outputs": [],
      "source": [
        "def report_of_data_frame(df, original_data, analyte_name):\n",
        "    df['total_decrease_duration'].hist(bins=20)\n",
        "    plt.title('Histogram of Total Decrease Duration')\n",
        "    plt.xlabel('Total Decrease Duration (days)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "    total_decrease_duration = df['total_decrease_duration'].sum()\n",
        "    print(f\"Overall Total Decrease Duration: {total_decrease_duration} days\")\n",
        "\n",
        "    trend_counts = df['trend'].value_counts()\n",
        "    print(f\"\\nTrend counts:\\n{trend_counts}\")\n",
        "\n",
        "    avg_decrease_duration = df[df['trend'] == 'Decrease']['total_decrease_duration'].mean()\n",
        "    print(f\"Average Decrease Duration for Decreasing Wells: {avg_decrease_duration} days\")\n",
        "\n",
        "    all_wells_count = original_data[original_data['ANALYTE_NAME'] == analyte_name]['STATION_ID'].nunique()\n",
        "    print(f\"\\nTotal number of wells in the original dataset for analyte '{analyte_name}': {all_wells_count}\")\n",
        "\n",
        "    trends_per_class = df.groupby('well_name')['trend'].value_counts().unstack(fill_value=0)\n",
        "    print(f\"\\nTrend counts per class:\\n{trends_per_class}\")\n",
        "\n",
        "    overall_trend_counts = df['trend'].value_counts()\n",
        "    decreasing_wells_count = overall_trend_counts.get('Decrease', 0)\n",
        "    stable_wells_count = overall_trend_counts.get('Neutral', 0)\n",
        "    increasing_wells_count = overall_trend_counts.get('Increase', 0)\n",
        "\n",
        "    print(f\"\\nFor the whole analyte:\\nDecreasing: {decreasing_wells_count}\\nStable: {stable_wells_count}\\nIncreasing: {increasing_wells_count}\")\n",
        "\n",
        "    patterns = {\n",
        "        'FSB**D': 'FSB\\\\d+D$',\n",
        "        'FSB**DR': 'FSB\\\\d+DR$',\n",
        "        'FSB Number Only': '^FSB\\\\d+$',\n",
        "        'FPZ': '^FPZ'\n",
        "    }\n",
        "\n",
        "    for category, pattern in patterns.items():\n",
        "        print(f\"\\nAnalysis for {category}:\")\n",
        "        category_well_count = original_data[original_data['STATION_ID'].str.contains(pattern, regex=True)]['STATION_ID'].nunique()\n",
        "        print(f\"Number of wells for {category}: {category_well_count}\")\n",
        "\n",
        "        category_df = df[df['well_name'].str.contains(pattern, regex=True)]\n",
        "\n",
        "        category_trend_counts = category_df['trend'].value_counts()\n",
        "        print(f\"Trend counts for {category}:\\n{category_trend_counts}\")\n",
        "\n",
        "        below_MCL_count = category_df[category_df['total_decrease_duration'] <= 0].shape[0]\n",
        "        print(f\"Wells already below MCL for {category}: {below_MCL_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiGhT0VUs-HE"
      },
      "outputs": [],
      "source": [
        "report_of_data_frame(sr_90_data_frame, data_analyzer.data, 'STRONTIUM-90')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPBAOpITyTZu"
      },
      "outputs": [],
      "source": [
        "report_of_data_frame(i_129_data_frame, data_analyzer.data, 'IODINE-129')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipcv_FDLzfJj"
      },
      "source": [
        "\n",
        "# **Mapping**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "import branca.colormap as cm\n",
        "from matplotlib import cm, colors as mcolors\n",
        "from folium.plugins import MeasureControl\n",
        "from geopy.distance import geodesic"
      ],
      "metadata": {
        "id": "C_r5UnEaqFQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "gGr_NB3ZcK3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_analyzer = DataAnalyzer('/content/gdrive/MyDrive/MIT Project - VAL @HW_CEE/FASB Well Construction Info.xlsx')"
      ],
      "metadata": {
        "id": "J71ZQU25c1Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_analyzer.print_column_names()"
      ],
      "metadata": {
        "id": "eJbdiT_a4SFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_real_distances(min_latitude, max_latitude, min_longitude, max_longitude):\n",
        "    width = geodesic((min_latitude, min_longitude), (min_latitude, max_longitude)).meters\n",
        "    height = geodesic((min_latitude, min_longitude), (max_latitude, min_longitude)).meters\n",
        "    return width, height"
      ],
      "metadata": {
        "id": "RsSqg4kuzAf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_map(df, duration_column, zoom_level_offset=10):\n",
        "    center_latitude = df['latitude'].mean()\n",
        "    center_longitude = df['longitude'].mean()\n",
        "\n",
        "    max_latitude = df['latitude'].max()\n",
        "    min_latitude = df['latitude'].min()\n",
        "    max_longitude = df['longitude'].max()\n",
        "    min_longitude = df['longitude'].min()\n",
        "    max_delta = max(max_latitude - min_latitude, max_longitude - min_longitude)\n",
        "    zoom_level = zoom_level_offset - int(np.log2(max_delta))\n",
        "\n",
        "    m = folium.Map(location=[center_latitude, center_longitude], zoom_start=zoom_level)\n",
        "\n",
        "    conc_fg = folium.FeatureGroup(name=\"Concentrations\")\n",
        "\n",
        "    cmap_jet = cm.get_cmap('jet')\n",
        "    num_colors = 6\n",
        "    color_list = [mcolors.rgb2hex(cmap_jet(i / num_colors)) for i in range(num_colors)]\n",
        "\n",
        "    df['duration_years'] = df[duration_column] / 365.25\n",
        "\n",
        "    df['log_duration_years'] = np.log(df['duration_years'])\n",
        "\n",
        "    min_val_log = df['log_duration_years'].min()\n",
        "    max_val_log = df['log_duration_years'].max()\n",
        "    df['normalized_log_duration'] = (df['log_duration_years'] - min_val_log) / (max_val_log - min_val_log)\n",
        "\n",
        "    cmap = mcolors.LinearSegmentedColormap.from_list('custom_colormap', color_list)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        coordinates = [row['latitude'], row['longitude']]\n",
        "        point_color = mcolors.rgb2hex(cmap(row['normalized_log_duration']))\n",
        "        popup_content = f\"<div style='max-width:800px;'><ul>\"\n",
        "        popup_content += f\"<li>Well Name: {row['well_name']}</li>\"\n",
        "        popup_content += f\"<li>Time to MCL: {row['total_decrease_duration']} days ({row['duration_years']:.2f} Years)</li>\"\n",
        "        popup_content += \"</ul></div>\"\n",
        "        folium.Circle(\n",
        "            radius=15,\n",
        "            location=coordinates,\n",
        "            popup=folium.Popup(popup_content, max_width=800),\n",
        "            fill_color=point_color,\n",
        "            color='#000000',\n",
        "            fill_opacity=1,\n",
        "            stroke=1,\n",
        "            weight=1\n",
        "        ).add_to(conc_fg)\n",
        "\n",
        "    m.add_child(conc_fg)\n",
        "\n",
        "    colormap_log = folium.LinearColormap(colors=color_list, vmin=min_val_log, vmax=max_val_log, caption=\"Log Scale\")\n",
        "    colormap_log.add_to(m)\n",
        "\n",
        "    min_val = df[duration_column].min()\n",
        "    max_val = df[duration_column].max()\n",
        "\n",
        "    min_val_years = min_val / 365.25\n",
        "    max_val_years = max_val / 365.25\n",
        "\n",
        "    colormap_original = folium.LinearColormap(colors=color_list, vmin=min_val_years, vmax=max_val_years, caption=f\"{duration_column} (years)\")\n",
        "    colormap_original.add_to(m)\n",
        "\n",
        "    width_meters, height_meters = calculate_real_distances(min_latitude, max_latitude, min_longitude, max_longitude)\n",
        "\n",
        "    folium.Marker(\n",
        "        [min_latitude, (min_longitude + max_longitude) / 2],\n",
        "        icon=folium.DivIcon(html=f\"<div style='font-size: 12pt'>Width: {width_meters:.2f} meters</div>\")\n",
        "    ).add_to(m)\n",
        "    folium.Marker(\n",
        "        [(min_latitude + max_latitude) / 2, min_longitude],\n",
        "        icon=folium.DivIcon(html=f\"<div style='font-size: 12pt; transform: rotate(90deg)'>Height: {height_meters:.2f} meters</div>\")\n",
        "    ).add_to(m)\n",
        "\n",
        "    bounding_box = [\n",
        "        [min_latitude, min_longitude],\n",
        "        [min_latitude, max_longitude],\n",
        "        [max_latitude, max_longitude],\n",
        "        [max_latitude, min_longitude],\n",
        "        [min_latitude, min_longitude]\n",
        "    ]\n",
        "    folium.PolyLine(bounding_box, color='black', weight=2, opacity=1).add_to(m)\n",
        "\n",
        "    top_horizontal_line = [\n",
        "        [max_latitude, min_longitude],\n",
        "        [max_latitude, max_longitude]\n",
        "    ]\n",
        "    bottom_horizontal_line = [\n",
        "        [min_latitude, min_longitude],\n",
        "        [min_latitude, max_longitude]\n",
        "    ]\n",
        "    left_vertical_line = [\n",
        "        [min_latitude, min_longitude],\n",
        "        [max_latitude, min_longitude]\n",
        "    ]\n",
        "    right_vertical_line = [\n",
        "        [min_latitude, max_longitude],\n",
        "        [max_latitude, max_longitude]\n",
        "    ]\n",
        "\n",
        "    folium.PolyLine(top_horizontal_line, color='blue', weight=2, opacity=1).add_to(m)\n",
        "    folium.PolyLine(bottom_horizontal_line, color='blue', weight=2, opacity=1).add_to(m)\n",
        "    folium.PolyLine(left_vertical_line, color='blue', weight=2, opacity=1).add_to(m)\n",
        "    folium.PolyLine(right_vertical_line, color='blue', weight=2, opacity=1).add_to(m)\n",
        "\n",
        "    m.save('map_with_colorbar_and_real_scales.html')\n",
        "\n",
        "    return m"
      ],
      "metadata": {
        "id": "Ee4ozjnzlxVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_satellite_map(df, duration_column, zoom_level_offset=10):\n",
        "    center_latitude = df['latitude'].mean()\n",
        "    center_longitude = df['longitude'].mean()\n",
        "\n",
        "    max_latitude = df['latitude'].max()\n",
        "    min_latitude = df['latitude'].min()\n",
        "    max_longitude = df['longitude'].max()\n",
        "    min_longitude = df['longitude'].min()\n",
        "    max_delta = max(max_latitude - min_latitude, max_longitude - min_longitude)\n",
        "    zoom_level = zoom_level_offset - int(np.log2(max_delta))\n",
        "\n",
        "    m = folium.Map(location=[center_latitude, center_longitude], zoom_start=zoom_level)\n",
        "\n",
        "    folium.TileLayer(\n",
        "        tiles='https://{s}.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
        "        attr='Google',\n",
        "        name='Google Satellite',\n",
        "        max_zoom=20,\n",
        "        subdomains=['mt0', 'mt1', 'mt2', 'mt3']\n",
        "    ).add_to(m)\n",
        "\n",
        "    conc_fg = folium.FeatureGroup(name=\"Concentrations\")\n",
        "\n",
        "    cmap_jet = cm.get_cmap('jet')\n",
        "    num_colors = 6\n",
        "    color_list = [mcolors.rgb2hex(cmap_jet(i / num_colors)) for i in range(num_colors)]\n",
        "\n",
        "    df['duration_years'] = df[duration_column] / 365.25\n",
        "\n",
        "    df['log_duration_years'] = np.log(df['duration_years'])\n",
        "\n",
        "    min_val_log = df['log_duration_years'].min()\n",
        "    max_val_log = df['log_duration_years'].max()\n",
        "    df['normalized_log_duration'] = (df['log_duration_years'] - min_val_log) / (max_val_log - min_val_log)\n",
        "\n",
        "    cmap = mcolors.LinearSegmentedColormap.from_list('custom_colormap', color_list)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        coordinates = [row['latitude'], row['longitude']]\n",
        "        point_color = mcolors.rgb2hex(cmap(row['normalized_log_duration']))\n",
        "        popup_content = f\"<div style='max-width:800px;'><ul>\"\n",
        "        popup_content += f\"<li>Well Name: {row['well_name']}</li>\"\n",
        "        popup_content += f\"<li>Time to MCL: {row['total_decrease_duration']} days ({row['duration_years']:.2f} Years)</li>\"\n",
        "        popup_content += \"</ul></div>\"\n",
        "        folium.Circle(\n",
        "            radius=15,\n",
        "            location=coordinates,\n",
        "            popup=folium.Popup(popup_content, max_width=800),\n",
        "            fill_color=point_color,\n",
        "            color='#000000',\n",
        "            fill_opacity=1,\n",
        "            stroke=1,\n",
        "            weight=1\n",
        "        ).add_to(conc_fg)\n",
        "\n",
        "    m.add_child(conc_fg)\n",
        "\n",
        "    colormap_log = folium.LinearColormap(colors=color_list, vmin=min_val_log, vmax=max_val_log, caption=\"Log Scale\")\n",
        "    colormap_log.add_to(m)\n",
        "\n",
        "    min_val = df[duration_column].min()\n",
        "    max_val = df[duration_column].max()\n",
        "\n",
        "    min_val_years = min_val / 365.25\n",
        "    max_val_years = max_val / 365.25\n",
        "\n",
        "    colormap_original = folium.LinearColormap(colors=color_list, vmin=min_val_years, vmax=max_val_years, caption=f\"{duration_column} (years)\")\n",
        "    colormap_original.add_to(m)\n",
        "\n",
        "    width_meters, height_meters = calculate_real_distances(min_latitude, max_latitude, min_longitude, max_longitude)\n",
        "\n",
        "    folium.Marker(\n",
        "        [min_latitude, (min_longitude + max_longitude) / 2],\n",
        "        icon=folium.DivIcon(html=f\"<div style='font-size: 12pt'>Width: {width_meters:.2f} meters</div>\")\n",
        "    ).add_to(m)\n",
        "    folium.Marker(\n",
        "        [(min_latitude + max_latitude) / 2, min_longitude],\n",
        "        icon=folium.DivIcon(html=f\"<div style='font-size: 12pt; transform: rotate(90deg)'>Height: {height_meters:.2f} meters</div>\")\n",
        "    ).add_to(m)\n",
        "\n",
        "    bounding_box = [\n",
        "        [min_latitude, min_longitude],\n",
        "        [min_latitude, max_longitude],\n",
        "        [max_latitude, max_longitude],\n",
        "        [max_latitude, min_longitude],\n",
        "        [min_latitude, min_longitude]\n",
        "    ]\n",
        "    folium.PolyLine(bounding_box, color='black', weight=2, opacity=1).add_to(m)\n",
        "\n",
        "    top_horizontal_line = [\n",
        "        [max_latitude, min_longitude],\n",
        "        [max_latitude, max_longitude]\n",
        "    ]\n",
        "    bottom_horizontal_line = [\n",
        "        [min_latitude, min_longitude],\n",
        "        [min_latitude, max_longitude]\n",
        "    ]\n",
        "    left_vertical_line = [\n",
        "        [min_latitude, min_longitude],\n",
        "        [max_latitude, min_longitude]\n",
        "    ]\n",
        "    right_vertical_line = [\n",
        "        [min_latitude, max_longitude],\n",
        "        [max_latitude, max_longitude]\n",
        "    ]\n",
        "\n",
        "    folium.PolyLine(top_horizontal_line, color='blue', weight=2, opacity=1).add_to(m)\n",
        "    folium.PolyLine(bottom_horizontal_line, color='blue', weight=2, opacity=1).add_to(m)\n",
        "    folium.PolyLine(left_vertical_line, color='blue', weight=2, opacity=1).add_to(m)\n",
        "    folium.PolyLine(right_vertical_line, color='blue', weight=2, opacity=1).add_to(m)\n",
        "\n",
        "    m.save('map_with_colorbar_and_real_scales.html')\n",
        "\n",
        "    return m"
      ],
      "metadata": {
        "id": "IX8IVgEy1Ycz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sr-90**"
      ],
      "metadata": {
        "id": "xDnd2ZIlp8Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decreasing_well_list = data_analyzer.generate_decreasing_well_list(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='STRONTIUM-90',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "\n",
        "location_data = location_analyzer.extract_location_data(\n",
        "    well_name_column='station_id',\n",
        "    decreasing_well_list=decreasing_well_list\n",
        ")\n",
        "\n",
        "merged_df_sr_90 = pd.merge(analyte_summary_df_sr_90, location_data, left_on='well_name', right_on='station_id', how='inner')\n",
        "\n",
        "print(merged_df_sr_90)"
      ],
      "metadata": {
        "id": "6bxNSS5RP0Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_sr_90, 'total_decrease_duration')"
      ],
      "metadata": {
        "id": "XsMKCIXE0_ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_sr_90, 'total_decrease_duration', zoom_level_offset=11)"
      ],
      "metadata": {
        "id": "qVrTu3Q6122W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_satellite_map(merged_df_sr_90, 'total_decrease_duration')"
      ],
      "metadata": {
        "id": "EcCKzELXUzDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **I-129**"
      ],
      "metadata": {
        "id": "HO0hJkD8lBdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame with decreasing trends\n",
        "decreasing_well_list = data_analyzer.generate_decreasing_well_list(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='IODINE-129',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "\n",
        "# Extract latitude and longitude\n",
        "location_data = location_analyzer.extract_location_data(\n",
        "    well_name_column='station_id',\n",
        "    decreasing_well_list=decreasing_well_list\n",
        ")\n",
        "\n",
        "# Merge DataFrame with latitude and longitude information\n",
        "merged_df_i_129 = pd.merge(analyte_summary_df_i_129, location_data, left_on='well_name', right_on='station_id', how='inner')\n",
        "\n",
        "print(merged_df_i_129)"
      ],
      "metadata": {
        "id": "Ry-5iwJZYqeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_i_129, 'total_decrease_duration', zoom_level_offset=9)"
      ],
      "metadata": {
        "id": "_4ArcEyYqwyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_satellite_map(merged_df_i_129, 'total_decrease_duration')"
      ],
      "metadata": {
        "id": "TOekTzsGq3dZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "39aaf84606e21579959f93fb3c45e9e3d37c51d84ce27b2546e2ee0530e04c07"
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit ('elev': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}