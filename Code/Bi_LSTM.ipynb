{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olxOIIAvUIS1"
      },
      "source": [
        "# **SET UP**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.dates import date2num\n",
        "import os"
      ],
      "metadata": {
        "id": "lgYsUKifbFVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scienceplots"
      ],
      "metadata": {
        "id": "HsaURQXsmtc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scienceplots\n",
        "plt.style.use(['science', 'no-latex'])\n",
        "plt.rcParams.update({'font.size': 16})"
      ],
      "metadata": {
        "id": "59kN_52GmwON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyOVU1hv7iuo"
      },
      "source": [
        "# **DataAnalyzer class**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAnalyzer:\n",
        "    def __init__(self, data_or_url):\n",
        "        if isinstance(data_or_url, str):\n",
        "            self.data_url = data_or_url\n",
        "            self.data = self.load_data()\n",
        "        elif isinstance(data_or_url, pd.DataFrame):\n",
        "            self.data = data_or_url\n",
        "        else:\n",
        "            raise ValueError(\"Input must be a file path or a pandas DataFrame\")\n",
        "\n",
        "    def load_data(self):\n",
        "        file_extension = self.data_url.split('.')[-1]\n",
        "        if file_extension == 'csv':\n",
        "            return pd.read_csv(self.data_url)\n",
        "        elif file_extension in ['xlsx', 'xlsm', 'xltx', 'xltm']:\n",
        "            return pd.read_excel(self.data_url)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "    def preprocess_data(self, well_name_column, analyte_name_column, well_name, analyte_name):\n",
        "        filtered_data = self.data[(self.data[well_name_column] == well_name) & (self.data[analyte_name_column] == analyte_name)].copy()\n",
        "        filtered_data = filtered_data.dropna(subset=['COLLECTION_DATE', 'RESULT'])\n",
        "        filtered_data['COLLECTION_DATE'] = pd.to_datetime(filtered_data['COLLECTION_DATE'], errors='coerce')\n",
        "        filtered_data = filtered_data.dropna(subset=['COLLECTION_DATE'])\n",
        "        filtered_data = filtered_data[filtered_data['RESULT'] > 0]\n",
        "        filtered_data['RESULT_LOG'] = np.log10(filtered_data['RESULT'])\n",
        "        return filtered_data\n",
        "\n",
        "    def create_sequences(self, data, seq_length):\n",
        "        xs = []\n",
        "        ys = []\n",
        "        for i in range(len(data) - seq_length):\n",
        "            x = data[i:i+seq_length]\n",
        "            y = data[i+seq_length]\n",
        "            xs.append(x)\n",
        "            ys.append(y)\n",
        "        return np.array(xs), np.array(ys)\n",
        "\n",
        "    def lstm_slope_forecast(self, well_name_column, analyte_name_column, analyte_name, seq_length=10, n_bootstrap=100):\n",
        "        well_names = self.data[self.data[analyte_name_column] == analyte_name][well_name_column].unique()\n",
        "        results = []\n",
        "\n",
        "        all_mse = []\n",
        "        all_r2 = []\n",
        "\n",
        "        for well_name in well_names:\n",
        "            filtered_data = self.preprocess_data(well_name_column, analyte_name_column, well_name, analyte_name)\n",
        "            print(f'Data size after preprocessing for {well_name} and {analyte_name}: {filtered_data.shape}')\n",
        "\n",
        "            if filtered_data.empty:\n",
        "                print(f'No data for well: {well_name} and analyte: {analyte_name}')\n",
        "                continue\n",
        "\n",
        "            cutoff_date = filtered_data['COLLECTION_DATE'].max() - pd.DateOffset(years=4)\n",
        "            train_data = filtered_data[filtered_data['COLLECTION_DATE'] <= cutoff_date].copy()\n",
        "            test_data = filtered_data[filtered_data['COLLECTION_DATE'] > cutoff_date].copy()\n",
        "\n",
        "            if train_data.empty or test_data.empty:\n",
        "                print(f'Not enough data to split into training and testing sets for well: {well_name} and analyte: {analyte_name}')\n",
        "                continue\n",
        "\n",
        "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "            train_data.loc[:, 'RESULT_LOG'] = scaler.fit_transform(train_data['RESULT_LOG'].values.reshape(-1, 1))\n",
        "            test_data.loc[:, 'RESULT_LOG'] = scaler.transform(test_data['RESULT_LOG'].values.reshape(-1, 1))\n",
        "\n",
        "            train_data.loc[:, 'DELTA_RESULT_LOG'] = train_data['RESULT_LOG'].diff().fillna(0)\n",
        "            test_data.loc[:, 'DELTA_RESULT_LOG'] = test_data['RESULT_LOG'].diff().fillna(0)\n",
        "\n",
        "            X_train, y_train = self.create_sequences(train_data['DELTA_RESULT_LOG'].values, seq_length)\n",
        "            if len(X_train) == 0:\n",
        "                print(f'Not enough training data to create sequences for well: {well_name} and analyte: {analyte_name}')\n",
        "                continue\n",
        "\n",
        "            X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "\n",
        "            model = Sequential()\n",
        "            model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(seq_length, 1)))\n",
        "            model.add(Dropout(0.2))\n",
        "            model.add(Bidirectional(LSTM(64)))\n",
        "            model.add(Dropout(0.2))\n",
        "            model.add(Dense(1))\n",
        "            model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "\n",
        "            if len(X_train) > 5:\n",
        "                validation_split = 0.2\n",
        "            else:\n",
        "                validation_split = 0\n",
        "\n",
        "            model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=2, validation_split=validation_split, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "            lin_reg_model = LinearRegression()\n",
        "            x_lin_reg = np.array([date2num(d) for d in train_data['COLLECTION_DATE']]).reshape(-1, 1)\n",
        "            y_lin_reg = scaler.inverse_transform(train_data['RESULT_LOG'].values.reshape(-1, 1)).flatten()\n",
        "            lin_reg_model.fit(x_lin_reg, y_lin_reg)\n",
        "            trend = lin_reg_model.predict(x_lin_reg)\n",
        "\n",
        "            predictions = []\n",
        "            input_sequence = train_data['DELTA_RESULT_LOG'].values[-seq_length:]\n",
        "\n",
        "            last_value = train_data['RESULT_LOG'].values[-1]\n",
        "            for date in test_data['COLLECTION_DATE']:\n",
        "                input_sequence_reshaped = input_sequence.reshape((1, seq_length, 1))\n",
        "                delta_prediction = model.predict(input_sequence_reshaped)[0, 0]\n",
        "                last_value += delta_prediction\n",
        "                predictions.append(last_value)\n",
        "                input_sequence = np.append(input_sequence[1:], delta_prediction)\n",
        "\n",
        "            predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
        "\n",
        "            boot_predictions = np.zeros((n_bootstrap, len(predictions)))\n",
        "            residuals = y_train - model.predict(X_train).flatten()\n",
        "\n",
        "            for i in range(n_bootstrap):\n",
        "                boot_residuals = np.random.choice(residuals, len(predictions), replace=True)\n",
        "                boot_predictions[i, :] = predictions.flatten() + boot_residuals\n",
        "\n",
        "            lower_bound = np.percentile(boot_predictions, 2.5, axis=0)\n",
        "            upper_bound = np.percentile(boot_predictions, 97.5, axis=0)\n",
        "\n",
        "            x_forecast = np.array([date2num(d) for d in test_data['COLLECTION_DATE']]).reshape(-1, 1)\n",
        "            lin_reg_predictions = lin_reg_model.predict(x_forecast)\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            train_data = train_data.sort_values(by='COLLECTION_DATE')\n",
        "            test_data = test_data.sort_values(by='COLLECTION_DATE')\n",
        "            historical_data_unscaled_train = scaler.inverse_transform(train_data['RESULT_LOG'].values.reshape(-1, 1)).flatten()\n",
        "            historical_data_unscaled_test = scaler.inverse_transform(test_data['RESULT_LOG'].values.reshape(-1, 1)).flatten()\n",
        "            plt.plot(train_data['COLLECTION_DATE'], historical_data_unscaled_train, label='Training Data', color='blue')\n",
        "            plt.scatter(train_data['COLLECTION_DATE'], historical_data_unscaled_train, color='blue')\n",
        "            plt.plot(test_data['COLLECTION_DATE'], historical_data_unscaled_test, label='Testing Data', color='purple')\n",
        "            plt.scatter(test_data['COLLECTION_DATE'], historical_data_unscaled_test, color='purple')\n",
        "            plt.plot(test_data['COLLECTION_DATE'], predictions, label='Bidirectional LSTM Forecast', linestyle='--', color='red')\n",
        "            plt.fill_between(test_data['COLLECTION_DATE'], lower_bound, upper_bound, color='orange', alpha=0.3, label='95% Confidence Interval')\n",
        "            plt.plot(test_data['COLLECTION_DATE'], lin_reg_predictions, label='Linear Regression Forecast', linestyle='--', color='orange')\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('Log-Concentration')\n",
        "            plt.title(f'{well_name} - {analyte_name} Forecast')\n",
        "            plt.legend()\n",
        "\n",
        "            save_dir = 'plots'\n",
        "            if not os.path.exists(save_dir):\n",
        "                os.makedirs(save_dir)\n",
        "            plot_path = os.path.join(save_dir, f'forecast_{well_name}_{analyte_name}_Bidirectional_LSTM.png')\n",
        "            plt.savefig(plot_path)\n",
        "            plt.show()\n",
        "\n",
        "            print(f'Saved plot for well: {well_name} at {plot_path}')\n",
        "            results.append({\n",
        "                'well_name': well_name,\n",
        "                'analyte_name': analyte_name,\n",
        "                'model_name': 'Bidirectional LSTM',\n",
        "                'plot_path': plot_path\n",
        "            })\n",
        "\n",
        "            y_pred = model.predict(X_train)\n",
        "            y_true = y_train\n",
        "            mse = mean_squared_error(y_true, y_pred)\n",
        "            r2 = r2_score(y_true, y_pred)\n",
        "            all_mse.append(mse)\n",
        "            all_r2.append(r2)\n",
        "\n",
        "            print(f'MSE for well {well_name} with Bidirectional LSTM: {mse}')\n",
        "            print(f'R^2 for well {well_name} with Bidirectional LSTM: {r2}')\n",
        "\n",
        "        filtered_mse = [mse for mse in all_mse if not np.isnan(mse) and mse > 0]\n",
        "        filtered_r2 = [r2 for r2 in all_r2 if not np.isnan(r2) and r2 > 0]\n",
        "\n",
        "        if filtered_mse:\n",
        "            avg_mse = np.mean(filtered_mse)\n",
        "        else:\n",
        "            avg_mse = float('nan')\n",
        "\n",
        "        if filtered_r2:\n",
        "            avg_r2 = np.mean(filtered_r2)\n",
        "        else:\n",
        "            avg_r2 = float('nan')\n",
        "\n",
        "        print(f'Average MSE: {avg_mse}')\n",
        "        print(f'Average R^2: {avg_r2}')\n",
        "\n",
        "        return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "gNjB0D81W2YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_analyzer = DataAnalyzer('https://raw.githubusercontent.com/ALTEMIS-DOE/pylenm/master/notebooks/data/FASB_Data_thru_3Q2015_Reduced_Demo.csv')"
      ],
      "metadata": {
        "id": "sPfPnZTGbUJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM model**"
      ],
      "metadata": {
        "id": "SsZNmFE2WSgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_results = data_analyzer.lstm_slope_forecast(\n",
        "    well_name_column='STATION_ID',\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='STRONTIUM-90'\n",
        ")\n",
        "print(forecast_results)"
      ],
      "metadata": {
        "id": "IxhuzpFObW7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_results_2 = data_analyzer.lstm_slope_forecast(\n",
        "    well_name_column='STATION_ID',\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='IODINE-129'\n",
        ")\n",
        "print(forecast_results_2)"
      ],
      "metadata": {
        "id": "C_4v3LcM9Lo9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "interpreter": {
      "hash": "39aaf84606e21579959f93fb3c45e9e3d37c51d84ce27b2546e2ee0530e04c07"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}