{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olxOIIAvUIS1"
      },
      "source": [
        "# **SET UP**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SciencePlots"
      ],
      "metadata": {
        "id": "izPweIMH0eRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2JbZiOxdYE9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.colors as mcolors\n",
        "import scienceplots\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import t, linregress\n",
        "from matplotlib.dates import date2num, num2date\n",
        "import matplotlib.lines as mlines\n",
        "import matplotlib.patches as mpatches\n",
        "import datetime\n",
        "from datetime import datetime, date, timedelta\n",
        "import folium\n",
        "import branca.colormap\n",
        "from geopy.geocoders import Nominatim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyOVU1hv7iuo"
      },
      "source": [
        "# **Newly updated MCL function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyMbDXdB0_95"
      },
      "outputs": [],
      "source": [
        "class DataAnalyzer:\n",
        "    def __init__(self, data_or_url):\n",
        "        if isinstance(data_or_url, str):  # Assuming file path is a string\n",
        "            self.data_url = data_or_url\n",
        "            self.data = self.load_data()\n",
        "        elif isinstance(data_or_url, pd.DataFrame):  # Assuming DataFrame\n",
        "            self.data = data_or_url\n",
        "        else:\n",
        "            raise ValueError(\"Input must be a file path or a pandas DataFrame\")\n",
        "\n",
        "    def load_data(self):\n",
        "        file_extension = self.data_url.split('.')[-1]\n",
        "        if file_extension == 'csv':\n",
        "            return pd.read_csv(self.data_url)\n",
        "        elif file_extension in ['xlsx', 'xlsm', 'xltx', 'xltm']:  # Add more excel file extensions if needed\n",
        "            return pd.read_excel(self.data_url)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "    def print_column_names(self):\n",
        "        print(\"Column names in the dataset:\", self.data.columns.tolist())\n",
        "\n",
        "    def preprocess_data(self, well_name_column, analyte_name_column, well_name, analyte_name):\n",
        "        filtered_data = self.data[(self.data[well_name_column] == well_name) & (self.data[analyte_name_column] == analyte_name)]\n",
        "        filtered_data = filtered_data.dropna(subset=['COLLECTION_DATE', 'RESULT'])\n",
        "        filtered_data['COLLECTION_DATE'] = pd.to_datetime(filtered_data['COLLECTION_DATE']).dt.date\n",
        "        filtered_data['RESULT_LOG'] = np.log10(filtered_data['RESULT'])\n",
        "        return filtered_data\n",
        "\n",
        "    def remove_outliers(self, data, result_column='RESULT_LOG', z_threshold=4):\n",
        "        z = np.abs(stats.zscore(data[result_column]))\n",
        "        return data[(z < z_threshold)]\n",
        "\n",
        "    def predict_time_at_MCL(self, slope, intercept, log_MCL, earliest_date):\n",
        "        if slope >= 0:\n",
        "            return None\n",
        "        time_at_MCL = (log_MCL - intercept) / slope\n",
        "        try:\n",
        "            time_at_MCL_date = num2date(time_at_MCL).replace(tzinfo=None).date()\n",
        "            if earliest_date.year < 1 or time_at_MCL_date.year >= 10000 or time_at_MCL_date <= earliest_date:\n",
        "                return None\n",
        "            return time_at_MCL_date\n",
        "        except ValueError:\n",
        "            return None\n",
        "\n",
        "    def adjust_date_based_on_confidence(self, base_date, slope, confidence_interval, days_per_unit):\n",
        "        if base_date is None or slope == 0:\n",
        "            return None, None, None\n",
        "        days_adjustment = confidence_interval / abs(slope) * days_per_unit\n",
        "        upper_date = base_date + timedelta(days=days_adjustment) if base_date else None\n",
        "        lower_date = base_date - timedelta(days=days_adjustment) if base_date else None\n",
        "        return lower_date, upper_date\n",
        "\n",
        "    def generate_analyte_summary(self, analyte_name_column, analyte_name, well_name_column, MCL):\n",
        "        unique_wells = self.data[self.data[analyte_name_column] == analyte_name][well_name_column].unique()\n",
        "        summary = []\n",
        "\n",
        "        for well in unique_wells:\n",
        "            query = self.preprocess_data(well_name_column, analyte_name_column, well, analyte_name)\n",
        "            if query.empty or len(query['COLLECTION_DATE'].unique()) < 2:\n",
        "                continue\n",
        "\n",
        "            x = date2num(query['COLLECTION_DATE'])\n",
        "            y = query['RESULT_LOG']\n",
        "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "\n",
        "            if slope > 0:\n",
        "                trend = 'Increase'\n",
        "            elif slope < 0:\n",
        "                trend = 'Decrease'\n",
        "            else:\n",
        "                trend = 'Neutral'\n",
        "\n",
        "            if trend == 'Neutral':\n",
        "                continue\n",
        "\n",
        "            log_MCL = np.log10(MCL)\n",
        "            time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, min(query['COLLECTION_DATE']))\n",
        "\n",
        "            # Calculate total decrease duration (for wells with decreasing trends)\n",
        "            total_decrease_duration = None\n",
        "            if trend == 'Decrease' and time_at_MCL:\n",
        "                total_decrease_duration = (time_at_MCL - min(query['COLLECTION_DATE'])).days\n",
        "\n",
        "            df = len(x) - 2\n",
        "            ts = t.ppf(1 - 0.025, df)  # Two-sided t-critical value for 95% CI\n",
        "            ci_slope = ts * std_err\n",
        "\n",
        "            lower_date, upper_date = None, None\n",
        "            if time_at_MCL and isinstance(time_at_MCL, date):\n",
        "                lower_date, upper_date = self.adjust_date_based_on_confidence(time_at_MCL, slope, ci_slope, 365.25)\n",
        "\n",
        "            summary.append({\n",
        "                'well_name': well,\n",
        "                'slope': slope,\n",
        "                'p_value': p_value,  # Add p-value to summary\n",
        "                'trend': trend,\n",
        "                'time@MCL': time_at_MCL,\n",
        "                'lower_time@MCL': lower_date,\n",
        "                'upper_time@MCL': upper_date,\n",
        "                'total_decrease_duration': total_decrease_duration\n",
        "            })\n",
        "\n",
        "        summary_df = pd.DataFrame(summary)\n",
        "        return summary_df\n",
        "\n",
        "    def compile_results(self, well_analyte_pairs, well_name_column, analyte_name_column, MCL):\n",
        "        results = []\n",
        "        for well_name, analyte_name in well_analyte_pairs:\n",
        "            time_at_MCL = None\n",
        "            query = self.preprocess_data(well_name_column, analyte_name_column, well_name, analyte_name)\n",
        "            if query.empty:\n",
        "                continue\n",
        "\n",
        "            query = self.remove_outliers(query)\n",
        "            x = date2num(query['COLLECTION_DATE'])\n",
        "            y = query['RESULT_LOG']\n",
        "\n",
        "            if len(x) < 2:\n",
        "                continue\n",
        "\n",
        "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "            df = len(x) - 2\n",
        "            ts = t.ppf(1 - 0.025, df)\n",
        "\n",
        "            trend = \"Increase\" if slope > 0 else \"Decrease\" if slope < 0 else \"Neutral\"\n",
        "            log_MCL = np.log10(MCL) if MCL else None\n",
        "            earliest_date = min(query['COLLECTION_DATE'])\n",
        "            time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, earliest_date) if MCL else None\n",
        "\n",
        "            # Calculate total decrease duration (for wells with decreasing trends)\n",
        "            total_decrease_duration = None\n",
        "            if trend == 'Decrease' and time_at_MCL:\n",
        "                total_decrease_duration = (time_at_MCL - min(query['COLLECTION_DATE'])).days\n",
        "\n",
        "            lower_date, upper_date = None, None\n",
        "            if time_at_MCL and isinstance(time_at_MCL, date):\n",
        "                lower_date, upper_date = self.adjust_date_based_on_confidence(time_at_MCL, slope, ts * std_err, 365.25)\n",
        "\n",
        "            results.append({\n",
        "                'well_name': well_name,\n",
        "                'slope': slope,\n",
        "                'p_value': p_value,  # Include p-value in results\n",
        "                'trend': trend,\n",
        "                'time@MCL': time_at_MCL,\n",
        "                'lower_time@MCL': lower_date,\n",
        "                'upper_time@MCL': upper_date,\n",
        "                'total_decrease_duration': total_decrease_duration  # Add total decrease duration\n",
        "            })\n",
        "\n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_df.to_csv('time2MCL_results.csv', index=False)\n",
        "        return results_df\n",
        "\n",
        "    def plot_MCL(self, well_name_column, analyte_name_column, well_name, analyte_name, year_interval=20, save_dir='plot_MCL', MCL=None, extrapolate_until_year=2025):\n",
        "        plt.style.use(['science', 'no-latex'])\n",
        "\n",
        "        # Set the global font size parameters\n",
        "        plt.rcParams.update({\n",
        "            'font.size': 18,\n",
        "            'axes.titlesize': 18,\n",
        "            'axes.labelsize': 18,\n",
        "            'xtick.labelsize': 18,\n",
        "            'ytick.labelsize': 18,\n",
        "            'legend.fontsize': 18,\n",
        "            'figure.titlesize': 18\n",
        "        })\n",
        "\n",
        "        query = self.preprocess_data(well_name_column, analyte_name_column, well_name, analyte_name)\n",
        "        if query.empty:\n",
        "            print(f'No results found for {well_name} and {analyte_name}')\n",
        "            return\n",
        "\n",
        "        query = self.remove_outliers(query, result_column='RESULT_LOG')\n",
        "        x = date2num(query['COLLECTION_DATE'])\n",
        "        y = query['RESULT_LOG']\n",
        "\n",
        "        if len(x) < 2:\n",
        "            print(\"Not enough data for regression.\")\n",
        "            return\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "        df = len(x) - 2\n",
        "        ts = t.ppf(1 - 0.025, df)\n",
        "\n",
        "        log_MCL = np.log10(MCL) if MCL else None\n",
        "        time_at_MCL = self.predict_time_at_MCL(slope, intercept, log_MCL, min(query['COLLECTION_DATE'])) if MCL and slope < 0 else 'Not available'\n",
        "\n",
        "        lower_date, upper_date = None, None\n",
        "        if time_at_MCL and isinstance(time_at_MCL, date):\n",
        "            lower_date, upper_date = self.adjust_date_based_on_confidence(time_at_MCL, slope, ts * std_err, 365.25)\n",
        "\n",
        "        # Prepare for extrapolation\n",
        "        extrapolate_date = np.datetime64(f'{extrapolate_until_year}-12-31')\n",
        "        x_extrapolate = np.linspace(x.min(), date2num(extrapolate_date), 100)\n",
        "        y_extrapolate = slope * x_extrapolate + intercept\n",
        "\n",
        "        # Ensure all plots are of the same size\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.grid(False)\n",
        "\n",
        "        data_points = plt.scatter(query['COLLECTION_DATE'], y, color='blue', label='Data Points')\n",
        "        plt.xlim([num2date(x.min()), num2date(x_extrapolate.max())])\n",
        "        plt.ylim([0.0, max(y.max(), y_extrapolate.max())])\n",
        "\n",
        "        extrap_regression = plt.plot(num2date(x_extrapolate), y_extrapolate, 'r--', label=f'Extrapolated Regression')\n",
        "\n",
        "        if log_MCL is not None:\n",
        "            mcl_line = plt.axhline(y=log_MCL, color='green', linestyle='--', label=f'MCL (log10): {log_MCL:.2f}')\n",
        "\n",
        "        if lower_date and upper_date:\n",
        "            lower_intercept = intercept + slope * (date2num(lower_date) - date2num(time_at_MCL))\n",
        "            upper_intercept = intercept + slope * (date2num(upper_date) - date2num(time_at_MCL))\n",
        "            lower_ci = plt.plot(num2date(x_extrapolate), slope * x_extrapolate + lower_intercept, 'b--', label='Lower CI Line')\n",
        "            upper_ci = plt.plot(num2date(x_extrapolate), slope * x_extrapolate + upper_intercept, 'orange', label='Upper CI Line')\n",
        "\n",
        "        plt.xlabel('Collection Date')\n",
        "        plt.ylabel('Log-Concentration (pCi/mL)')\n",
        "        plt.title(f'{well_name} - {analyte_name}')\n",
        "\n",
        "        handles, labels = plt.gca().get_legend_handles_labels()\n",
        "        fig_legend = plt.legend(handles=handles, loc='upper center', bbox_to_anchor=(0.3, -0.2), fontsize=18, frameon=True, shadow=True)\n",
        "\n",
        "        # Add p-value to the result string\n",
        "        result_str = (\n",
        "            f\"Results\\n\"\n",
        "            f\"- Slope value: {slope:.2e}\\n\"\n",
        "            f\"- Time@MCL value: {time_at_MCL}\\n\"\n",
        "            f\"- Lower CI value: {lower_date}\\n\"\n",
        "            f\"- Upper CI value: {upper_date}\\n\"\n",
        "            f\"- p-value: {p_value:.4f}\"\n",
        "        )\n",
        "\n",
        "        plt.figtext(0.55, -0.2, result_str, fontsize=18, ha=\"left\", bbox=dict(facecolor='white', alpha=0.5))\n",
        "        plt.tight_layout(rect=[0, 0.31, 1, 0.95])\n",
        "        plt.gca().xaxis.set_major_locator(mdates.YearLocator(5))\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "        plt.gcf().autofmt_xdate()\n",
        "\n",
        "        plt.xlim([num2date(x.min()), num2date(x_extrapolate.max())])\n",
        "\n",
        "        figure_path = f'{save_dir}/{well_name}-{analyte_name}.png'\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        plt.savefig(figure_path)\n",
        "\n",
        "        return figure_path\n",
        "\n",
        "    def generate_analysis_dataframe(self, analyte_name_column, well_name_column, analyte_name, MCL):\n",
        "        analyte_summary_df = self.generate_analyte_summary(analyte_name_column, analyte_name, well_name_column, MCL)\n",
        "        analysis_data = []\n",
        "\n",
        "        for index, row in analyte_summary_df.iterrows():\n",
        "            if row['trend'] == 'Neutral':\n",
        "                continue\n",
        "\n",
        "            plot_path = self.plot_MCL(well_name_column, analyte_name_column, row['well_name'], analyte_name, year_interval=5, save_dir='plot_MCL', MCL=MCL)\n",
        "\n",
        "            analysis_data.append({\n",
        "                'analyte_name': analyte_name,\n",
        "                'well_name': row['well_name'],\n",
        "                'slope': row['slope'],\n",
        "                'p_value': row['p_value'],  # Add p-value to analysis dataframe\n",
        "                'trend': row['trend'],\n",
        "                'time@MCL': row['time@MCL'],\n",
        "                'lower_time@MCL': row['lower_time@MCL'],\n",
        "                'upper_time@MCL': row['upper_time@MCL'],\n",
        "                'total_decrease_duration': row['total_decrease_duration'],\n",
        "                'path_to_figure': plot_path\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(analysis_data)\n",
        "\n",
        "    def generate_decreasing_well_list(self, analyte_name_column, analyte_name, well_name_column, MCL):\n",
        "        analyte_summary_df = self.generate_analyte_summary(analyte_name_column, analyte_name, well_name_column, MCL)\n",
        "        decreasing_well_list = analyte_summary_df[analyte_summary_df['trend'] == 'Decrease']['well_name'].tolist()\n",
        "        return decreasing_well_list\n",
        "\n",
        "    def extract_location_data(self, well_name_column, decreasing_well_list):\n",
        "        location_data = self.data[self.data[well_name_column].isin(decreasing_well_list)]\n",
        "        return location_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlV_0Cy2ZB0K"
      },
      "outputs": [],
      "source": [
        "data_analyzer = DataAnalyzer('https://raw.githubusercontent.com/ALTEMIS-DOE/pylenm/master/notebooks/data/FASB_Data_thru_3Q2015_Reduced_Demo.csv')\n",
        "data_analyzer.print_column_names()  # This will print column names, helping you to identify the correct ones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOb4qjWIgn1h"
      },
      "source": [
        "# **Plot MCL for a specific well**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DjW54mrPRDW"
      },
      "outputs": [],
      "source": [
        "data_analyzer.plot_MCL(\n",
        "    well_name_column='STATION_ID',\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    well_name='FSP  2B',\n",
        "    analyte_name='STRONTIUM-90',\n",
        "    MCL=10,\n",
        "    extrapolate_until_year=2025\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht6HtEJWg8bW"
      },
      "source": [
        "# **Data Frame Table**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch0bHZHYhCi2"
      },
      "source": [
        "# ***Sr-90***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-kg_wJWM8YS"
      },
      "outputs": [],
      "source": [
        "analyte_summary_df_sr_90 = data_analyzer.generate_analyte_summary(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='STRONTIUM-90',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "print(analyte_summary_df_sr_90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbv1KNZ2hMOL"
      },
      "source": [
        "# ***I-129***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkwVLAD1Phlf"
      },
      "outputs": [],
      "source": [
        "analyte_summary_df_i_129 = data_analyzer.generate_analyte_summary(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='IODINE-129',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "print(analyte_summary_df_i_129)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ucizq6khY-a"
      },
      "source": [
        "# **Store Data Frame for a Specific Analyte**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrJ8VEHIkl7A"
      },
      "source": [
        "# ***Sr-90***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqIS5ND3kuPv"
      },
      "outputs": [],
      "source": [
        "sr_90_data_frame = data_analyzer.generate_analysis_dataframe(\n",
        "    'ANALYTE_NAME',\n",
        "    'STATION_ID',\n",
        "    'STRONTIUM-90',\n",
        "    10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qws3XeSJiBg0"
      },
      "source": [
        "# ***I-129***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VRV-4G_SkYN"
      },
      "outputs": [],
      "source": [
        "i_129_data_frame = data_analyzer.generate_analysis_dataframe(\n",
        "    'ANALYTE_NAME',\n",
        "    'STATION_ID',\n",
        "    'IODINE-129',\n",
        "    10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2N3tmzsllod"
      },
      "source": [
        "# **Report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOD4Ymjzl-qI"
      },
      "outputs": [],
      "source": [
        "def report_of_data_frame(df, original_data, analyte_name):\n",
        "    plt.style.use(['science', 'no-latex'])\n",
        "    plt.grid(False)\n",
        "\n",
        "    df['total_decrease_duration_years'] = df['total_decrease_duration'] / 365.25\n",
        "\n",
        "    median_duration = df['total_decrease_duration_years'].median()\n",
        "\n",
        "    counts, bins, patches = plt.hist(df['total_decrease_duration_years'], bins=20, edgecolor='b\n",
        "\n",
        "    for i in range(len(patches)):\n",
        "        if bins[i] < median_duration:\n",
        "            patches[i].set_facecolor('blue')\n",
        "        else:\n",
        "            patches[i].set_facecolor('green')\n",
        "\n",
        "    plt.axvline(median_duration, color='r', linestyle='--', label=f'Median: {median_duration:.2f} years')\n",
        "    plt.title('Histogram of Total Decrease Duration (Years)', fontsize=14)  # Increase the font size\n",
        "    plt.xlabel('Total Decrease Duration (Years)', fontsize=12)  # Increase the font size\n",
        "    plt.ylabel('Frequency', fontsize=12)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "    total_decrease_duration_years = df['total_decrease_duration_years'].sum()\n",
        "    print(f\"Overall Total Decrease Duration: {total_decrease_duration_years:.2f} years\")\n",
        "    trend_counts = df['trend'].value_counts()\n",
        "    print(f\"\\nTrend counts:\\n{trend_counts}\")\n",
        "    avg_decrease_duration_years = df[df['trend'] == 'Decrease']['total_decrease_duration_years'].mean()\n",
        "    print(f\"Average Decrease Duration for Decreasing Wells: {avg_decrease_duration_years:.2f} years\")\n",
        "    all_wells_count = original_data[original_data['ANALYTE_NAME'] == analyte_name]['STATION_ID'].nunique()\n",
        "    print(f\"\\nTotal number of wells in the original dataset for analyte '{analyte_name}': {all_wells_count}\")\n",
        "\n",
        "    decreasing_wells = df[df['trend'] == 'Decrease']\n",
        "    wells_less_than_10_years = decreasing_wells[decreasing_wells['total_decrease_duration_years'] < 10].shape[0]\n",
        "    percentage_wells_less_than_10_years = (wells_less_than_10_years / decreasing_wells.shape[0]) * 100\n",
        "    print(f\"\\nNumber of decreasing wells with time-to-MCL less than 10 years: {wells_less_than_10_years}\")\n",
        "    print(f\"Percentage of decreasing wells with time-to-MCL less than 10 years: {percentage_wells_less_than_10_years:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiGhT0VUs-HE"
      },
      "outputs": [],
      "source": [
        "report_of_data_frame(sr_90_data_frame, data_analyzer.data, 'STRONTIUM-90')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPBAOpITyTZu"
      },
      "outputs": [],
      "source": [
        "report_of_data_frame(i_129_data_frame, data_analyzer.data, 'IODINE-129')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_analytes_histogram(df1, df2, analyte1_name, analyte2_name):\n",
        "    plt.style.use(['science', 'no-latex'])\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 12))  # Changed layout to vertical\n",
        "\n",
        "    df1['total_decrease_duration_years'] = df1['total_decrease_duration'] / 365.25\n",
        "    df2['total_decrease_duration_years'] = df2['total_decrease_duration'] / 365.25\n",
        "\n",
        "    df1_filtered = df1['total_decrease_duration_years'].dropna()\n",
        "    df2_filtered = df2['total_decrease_duration_years'].dropna()\n",
        "\n",
        "    median1 = df1_filtered.median()\n",
        "    median2 = df2_filtered.median()\n",
        "\n",
        "    counts1, bins1, patches1 = axes[0].hist(df1_filtered, bins=20, edgecolor='black')\n",
        "    axes[0].set_title(f'(a)', fontsize=14)\n",
        "    axes[0].set_xlabel('Total Decrease Duration (Years)', fontsize=12)\n",
        "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "    for i in range(len(patches1)):\n",
        "        if bins1[i] < median1:\n",
        "            patches1[i].set_facecolor('blue')\n",
        "        else:\n",
        "            patches1[i].set_facecolor('green')\n",
        "    axes[0].axvline(median1, color='r', linestyle='--', label=f'Median: {median1:.2f} years')\n",
        "    axes[0].legend()\n",
        "\n",
        "    counts2, bins2, patches2 = axes[1].hist(df2_filtered, bins=20, edgecolor='black')\n",
        "    axes[1].set_title(f'(b)', fontsize=14)\n",
        "    axes[1].set_xlabel('Total Decrease Duration (Years)', fontsize=12)\n",
        "    axes[1].set_ylabel('Frequency', fontsize=12)\n",
        "    for i in range(len(patches2)):\n",
        "        if bins2[i] < median2:\n",
        "            patches2[i].set_facecolor('blue')\n",
        "        else:\n",
        "            patches2[i].set_facecolor('green')\n",
        "    axes[1].axvline(median2, color='r', linestyle='--', label=f'Median: {median2:.2f} years')\n",
        "    axes[1].legend()\n",
        "\n",
        "    def round_up_to_nearest(value, factor):\n",
        "        return np.ceil(value / factor) * factor\n",
        "\n",
        "    max_duration = max(bins1.max(), bins2.max())\n",
        "    max_frequency = max(counts1.max(), counts2.max())\n",
        "    max_duration_rounded = round_up_to_nearest(max_duration, 10)\n",
        "    max_frequency_rounded = round_up_to_nearest(max_frequency, 5) + 5  # Add 5 as buffer\n",
        "    axes[0].set_xlim(0, max_duration_rounded)\n",
        "    axes[1].set_xlim(0, max_duration_rounded)\n",
        "    axes[0].set_ylim(0, max_frequency_rounded)\n",
        "    axes[1].set_ylim(0, max_frequency_rounded)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0gxQ0ErsJB3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_analytes_histogram(sr_90_data_frame, i_129_data_frame, 'STRONTIUM-90', 'IODINE-129')"
      ],
      "metadata": {
        "id": "f2UytbtJJJ07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_log_time_to_MCL(df, analyte_name, font_size=14, title_font_size=18):\n",
        "    valid_mcl_data = df[df['total_decrease_duration'].notnull()]\n",
        "    valid_mcl_data = valid_mcl_data[valid_mcl_data['total_decrease_duration'] > 0]\n",
        "    valid_mcl_data['total_decrease_duration_years'] = valid_mcl_data['total_decrease_duration'] / 365.25\n",
        "    valid_mcl_data['log_time_to_MCL'] = np.log10(valid_mcl_data['total_decrease_duration_years'])\n",
        "\n",
        "    median_log_duration = valid_mcl_data['log_time_to_MCL'].median()\n",
        "\n",
        "    plt.style.use(['science', 'no-latex'])  # Ensure SciencePlots is used\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    counts, bins, patches = plt.hist(valid_mcl_data['log_time_to_MCL'], bins=20, edgecolor=None, alpha=0.7)\n",
        "\n",
        "    for i in range(len(patches)):\n",
        "        if bins[i] < median_log_duration:\n",
        "            patches[i].set_facecolor('blue')\n",
        "        else:\n",
        "            patches[i].set_facecolor('green')\n",
        "\n",
        "    plt.axvline(median_log_duration, color='r', linestyle='--', label=f'Median: {median_log_duration:.2f} (log-years)')\n",
        "    plt.title(f'Log(Time-to-MCL) Distribution for {analyte_name}', fontsize=title_font_size)\n",
        "    plt.xlabel('Log(Time-to-MCL in years)', fontsize=font_size)\n",
        "    plt.ylabel('Frequency', fontsize=font_size)\n",
        "    plt.xticks(fontsize=font_size)\n",
        "    plt.yticks(fontsize=font_size)\n",
        "    plt.grid(False)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "    return valid_mcl_data[['well_name', 'total_decrease_duration_years', 'log_time_to_MCL']]"
      ],
      "metadata": {
        "id": "6c7RpXLxvVCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time_to_mcl_sr_90 = plot_log_time_to_MCL(sr_90_data_frame, analyte_name='STRONTIUM-90', font_size=14, title_font_size=18)\n",
        "print(log_time_to_mcl_sr_90)"
      ],
      "metadata": {
        "id": "52H_B6Otvi7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time_to_mcl_i_129 = plot_log_time_to_MCL(i_129_data_frame, analyte_name='IODINE-129', font_size=14, title_font_size=18)\n",
        "print(log_time_to_mcl_i_129)"
      ],
      "metadata": {
        "id": "OBm4tJHAyG0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_log_time_to_MCL(df1, analyte1_name, df2, analyte2_name, font_size=14, title_font_size=18):\n",
        "    valid_mcl_data1 = df1[df1['total_decrease_duration'].notnull() & (df1['total_decrease_duration'] > 0)]\n",
        "    valid_mcl_data1['total_decrease_duration_years'] = valid_mcl_data1['total_decrease_duration'] / 365.25\n",
        "    valid_mcl_data2 = df2[df2['total_decrease_duration'].notnull() & (df2['total_decrease_duration'] > 0)]\n",
        "    valid_mcl_data2['total_decrease_duration_years'] = valid_mcl_data2['total_decrease_duration'] / 365.25\n",
        "\n",
        "    valid_mcl_data1['log_time_to_MCL'] = np.log10(valid_mcl_data1['total_decrease_duration_years'])\n",
        "    valid_mcl_data2['log_time_to_MCL'] = np.log10(valid_mcl_data2['total_decrease_duration_years'])\n",
        "\n",
        "    median_log_duration1 = valid_mcl_data1['log_time_to_MCL'].median()\n",
        "    median_log_duration2 = valid_mcl_data2['log_time_to_MCL'].median()\n",
        "    mean_log_duration1 = valid_mcl_data1['log_time_to_MCL'].mean()\n",
        "    mean_log_duration2 = valid_mcl_data2['log_time_to_MCL'].mean()\n",
        "\n",
        "    plt.style.use(['science', 'no-latex'])  # Ensure SciencePlots is used\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 12))  # Vertical alignment\n",
        "\n",
        "    counts1, bins1, patches1 = axes[0].hist(valid_mcl_data1['log_time_to_MCL'], bins=20, edgecolor=None, alpha=0.7)\n",
        "    counts2, bins2, patches2 = axes[1].hist(valid_mcl_data2['log_time_to_MCL'], bins=20, edgecolor=None, alpha=0.7)\n",
        "\n",
        "    for i in range(len(patches1)):\n",
        "        if bins1[i] < median_log_duration1:\n",
        "            patches1[i].set_facecolor('blue')\n",
        "        else:\n",
        "            patches1[i].set_facecolor('green')\n",
        "\n",
        "    for i in range(len(patches2)):\n",
        "        if bins2[i] < median_log_duration2:\n",
        "            patches2[i].set_facecolor('blue')\n",
        "        else:\n",
        "            patches2[i].set_facecolor('green')\n",
        "\n",
        "    axes[0].axvline(median_log_duration1, color='r', linestyle='--', label=f'Median: {median_log_duration1:.2f} (log-years)')\n",
        "    axes[1].axvline(median_log_duration2, color='r', linestyle='--', label=f'Median: {median_log_duration2:.2f} (log-years)')\n",
        "    axes[0].axvline(mean_log_duration1, color='skyblue', linestyle=':', label=f'Mean: {mean_log_duration1:.2f} (log-years)')\n",
        "    axes[1].axvline(mean_log_duration2, color='skyblue', linestyle=':', label=f'Mean: {mean_log_duration2:.2f} (log-years)')\n",
        "    axes[0].set_title(f'(a) {analyte1_name}', fontsize=title_font_size)\n",
        "    axes[1].set_title(f'(b) {analyte2_name}', fontsize=title_font_size)\n",
        "    axes[0].set_xlabel('Log(Time-to-MCL in years)', fontsize=font_size)\n",
        "    axes[1].set_xlabel('Log(Time-to-MCL in years)', fontsize=font_size)\n",
        "    axes[0].set_ylabel('Frequency', fontsize=font_size)\n",
        "    axes[1].set_ylabel('Frequency', fontsize=font_size)\n",
        "    axes[0].tick_params(axis='both', which='major', labelsize=font_size)\n",
        "    axes[1].tick_params(axis='both', which='major', labelsize=font_size)\n",
        "\n",
        "    def round_up_to_nearest(value, factor):\n",
        "        return np.ceil(value / factor) * factor\n",
        "\n",
        "    max_duration = max(bins1.max(), bins2.max())\n",
        "    max_frequency = max(counts1.max(), counts2.max())\n",
        "    max_duration_rounded = round_up_to_nearest(max_duration, 0.5)  # Adjust for log-scale bins\n",
        "    max_frequency_rounded = round_up_to_nearest(max_frequency, 5) + 2.5  # Add 2.5 as a buffer\n",
        "\n",
        "    axes[0].set_xlim(0, max_duration_rounded)\n",
        "    axes[1].set_xlim(0, max_duration_rounded)\n",
        "    axes[0].set_ylim(0, max_frequency_rounded)\n",
        "    axes[1].set_ylim(0, max_frequency_rounded)\n",
        "    axes[0].legend(fontsize=12)\n",
        "    axes[1].legend(fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pKg52z77zbVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time_to_mcl_comparative = compare_log_time_to_MCL(sr_90_data_frame, 'STRONTIUM-90', i_129_data_frame, 'IODINE-129', font_size=14, title_font_size=18)"
      ],
      "metadata": {
        "id": "RV9Y23t0zjWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_analyte_plots(df1, analyte1_name, df2, analyte2_name):\n",
        "    plt.style.use(['science', 'no-latex'])\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 12))  # 2x2 layout\n",
        "\n",
        "    df1['total_decrease_duration_years'] = df1['total_decrease_duration'] / 365.25\n",
        "    df2['total_decrease_duration_years'] = df2['total_decrease_duration'] / 365.25\n",
        "    df1_filtered = df1['total_decrease_duration_years'].dropna()\n",
        "    df2_filtered = df2['total_decrease_duration_years'].dropna()\n",
        "\n",
        "    median1 = df1_filtered.median()\n",
        "    median2 = df2_filtered.median()\n",
        "\n",
        "    counts1, bins1, patches1 = axes[0, 0].hist(df1_filtered, bins=20, edgecolor='black')\n",
        "    axes[0, 0].set_title(f'(a)', fontsize=14)\n",
        "    axes[0, 0].set_xlabel('Total Decrease Duration (Years)', fontsize=12)\n",
        "    axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
        "\n",
        "    for i in range(len(patches1)):\n",
        "        if bins1[i] < median1:\n",
        "            patches1[i].set_facecolor('blue')\n",
        "        else:\n",
        "            patches1[i].set_facecolor('green')\n",
        "\n",
        "    axes[0, 0].axvline(median1, color='r', linestyle='--', label=f'Median: {median1:.2f} years')\n",
        "    axes[0, 0].legend()\n",
        "    counts2, bins2, patches2 = axes[1, 0].hist(df2_filtered, bins=20, edgecolor='black')\n",
        "    axes[1, 0].set_title(f'(b)', fontsize=14)\n",
        "    axes[1, 0].set_xlabel('Total Decrease Duration (Years)', fontsize=12)\n",
        "    axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
        "\n",
        "    for i in range(len(patches2)):\n",
        "        if bins2[i] < median2:\n",
        "            patches2[i].set_facecolor('blue')\n",
        "        else:\n",
        "            patches2[i].set_facecolor('green')\n",
        "\n",
        "    axes[1, 0].axvline(median2, color='r', linestyle='--', label=f'Median: {median2:.2f} years')\n",
        "    axes[1, 0].legend()\n",
        "\n",
        "    valid_mcl_data1 = df1[df1['total_decrease_duration'].notnull() & (df1['total_decrease_duration'] > 0)]\n",
        "    valid_mcl_data1['total_decrease_duration_years'] = valid_mcl_data1['total_decrease_duration'] / 365.25\n",
        "    valid_mcl_data1['log_time_to_MCL'] = np.log10(valid_mcl_data1['total_decrease_duration_years'])\n",
        "    valid_mcl_data2 = df2[df2['total_decrease_duration'].notnull() & (df2['total_decrease_duration'] > 0)]\n",
        "    valid_mcl_data2['total_decrease_duration_years'] = valid_mcl_data2['total_decrease_duration'] / 365.25\n",
        "    valid_mcl_data2['log_time_to_MCL'] = np.log10(valid_mcl_data2['total_decrease_duration_years'])\n",
        "\n",
        "    median_log_duration1 = valid_mcl_data1['log_time_to_MCL'].median()\n",
        "    median_log_duration2 = valid_mcl_data2['log_time_to_MCL'].median()\n",
        "    mean_log_duration1 = valid_mcl_data1['log_time_to_MCL'].mean()\n",
        "    mean_log_duration2 = valid_mcl_data2['log_time_to_MCL'].mean()\n",
        "\n",
        "    counts1_log, bins1_log, patches1_log = axes[0, 1].hist(valid_mcl_data1['log_time_to_MCL'], bins=20, edgecolor=None, alpha=0.7)\n",
        "    axes[0, 1].set_title(f'(c)', fontsize=14)\n",
        "    axes[0, 1].set_xlabel('Log10(Time-to-MCL in years)', fontsize=12)\n",
        "    axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
        "\n",
        "    for i in range(len(patches1_log)):\n",
        "        if bins1_log[i] < median_log_duration1:\n",
        "            patches1_log[i].set_facecolor('blue')\n",
        "        else:\n",
        "            patches1_log[i].set_facecolor('green')\n",
        "\n",
        "    axes[0, 1].axvline(median_log_duration1, color='r', linestyle='--', label=f'Median: {median_log_duration1:.2f} (log-years)')\n",
        "    axes[0, 1].axvline(mean_log_duration1, color='brown', linestyle=':', label=f'Mean: {mean_log_duration1:.2f} (log-years)')\n",
        "    axes[0, 1].legend()\n",
        "    counts2_log, bins2_log, patches2_log = axes[1, 1].hist(valid_mcl_data2['log_time_to_MCL'], bins=20, edgecolor=None, alpha=0.7)\n",
        "    axes[1, 1].set_title(f'(d)', fontsize=14)\n",
        "    axes[1, 1].set_xlabel('Log10(Time-to-MCL in years)', fontsize=12)\n",
        "    axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
        "\n",
        "    for i in range(len(patches2_log)):\n",
        "        if bins2_log[i] < median_log_duration2:\n",
        "            patches2_log[i].set_facecolor('blue')\n",
        "        else:\n",
        "            patches2_log[i].set_facecolor('green')\n",
        "\n",
        "    axes[1, 1].axvline(median_log_duration2, color='r', linestyle='--', label=f'Median: {median_log_duration2:.2f} (log-years)')\n",
        "    axes[1, 1].axvline(mean_log_duration2, color='brown', linestyle=':', label=f'Mean: {mean_log_duration2:.2f} (log-years)')\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6iWvisPxtZKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_analyte_plots(sr_90_data_frame, 'STRONTIUM-90', i_129_data_frame, 'IODINE-129')"
      ],
      "metadata": {
        "id": "aXQU5kt6tavX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipcv_FDLzfJj"
      },
      "source": [
        "\n",
        "# **Mapping**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "import branca.colormap as cm\n",
        "from matplotlib import cm, colors as mcolors\n",
        "from folium.plugins import MeasureControl\n",
        "from geopy.distance import geodesic"
      ],
      "metadata": {
        "id": "C_r5UnEaqFQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "gGr_NB3ZcK3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_analyzer = DataAnalyzer('/content/gdrive/MyDrive/MIT Project - VAL @HW_CEE/FASB Well Construction Info.xlsx')"
      ],
      "metadata": {
        "id": "J71ZQU25c1Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_analyzer.print_column_names()"
      ],
      "metadata": {
        "id": "eJbdiT_a4SFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_real_distances(min_latitude, max_latitude, min_longitude, max_longitude):\n",
        "    # Calculate the distances in meters using geodesic distance\n",
        "    width = geodesic((min_latitude, min_longitude), (min_latitude, max_longitude)).meters\n",
        "    height = geodesic((min_latitude, min_longitude), (max_latitude, min_longitude)).meters\n",
        "    return width, height"
      ],
      "metadata": {
        "id": "YTogQKbz0bAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_map(df, duration_column, zoom_level_offset=10, satellite_mode=False, aquifer=None, min_latitude=None, max_latitude=None, min_longitude=None, max_longitude=None):\n",
        "    if aquifer:\n",
        "        df = df[df['aquifer'] == aquifer]\n",
        "\n",
        "    if df.empty:\n",
        "        print(f\"No wells found for the aquifer: {aquifer}\")\n",
        "        return None\n",
        "\n",
        "    if min_latitude is None:\n",
        "        min_latitude = df['latitude'].min()\n",
        "    if max_latitude is None:\n",
        "        max_latitude = df['latitude'].max()\n",
        "    if min_longitude is None:\n",
        "        min_longitude = df['longitude'].min()\n",
        "    if max_longitude is None:\n",
        "        max_longitude = df['longitude'].max()\n",
        "\n",
        "    center_latitude = (max_latitude + min_latitude) / 2\n",
        "    center_longitude = (max_longitude + min_longitude) / 2\n",
        "\n",
        "    max_delta = max(max_latitude - min_latitude, max_longitude - min_longitude)\n",
        "    zoom_level = zoom_level_offset - int(np.log2(max_delta))  # Use the provided offset\n",
        "\n",
        "    if satellite_mode:\n",
        "        m = folium.Map(location=[center_latitude, center_longitude], zoom_start=zoom_level, tiles=None)\n",
        "        folium.TileLayer(\n",
        "            tiles='https://{s}.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
        "            attr='Google',\n",
        "            name='Google Satellite',\n",
        "            max_zoom=20,\n",
        "            subdomains=['mt0', 'mt1', 'mt2', 'mt3']\n",
        "        ).add_to(m)\n",
        "    else:\n",
        "        m = folium.Map(location=[center_latitude, center_longitude], zoom_start=zoom_level)\n",
        "\n",
        "    conc_fg = folium.FeatureGroup(name=\"Concentrations\")\n",
        "    no_decline_fg = folium.FeatureGroup(name=\"No Decline\")\n",
        "\n",
        "    cmap_jet = cm.get_cmap('jet')\n",
        "    num_colors = 6  # Optional number of colors\n",
        "    color_list = [mcolors.rgb2hex(cmap_jet(i / num_colors)) for i in range(num_colors)]\n",
        "\n",
        "    valid_df = df[df[duration_column].notnull() & (df[duration_column] > 0)]\n",
        "    valid_df['duration_years'] = valid_df[duration_column] / 365.25\n",
        "    valid_df['log_duration_years'] = np.log(valid_df['duration_years'])\n",
        "\n",
        "    min_val_log = valid_df['log_duration_years'].min()\n",
        "    max_val_log = valid_df['log_duration_years'].max()\n",
        "    valid_df['normalized_log_duration'] = (valid_df['log_duration_years'] - min_val_log) / (max_val_log - min_val_log)\n",
        "\n",
        "    cmap = mcolors.LinearSegmentedColormap.from_list('custom_colormap', color_list)\n",
        "\n",
        "    for _, row in valid_df.iterrows():\n",
        "        coordinates = [row['latitude'], row['longitude']]\n",
        "        point_color = mcolors.rgb2hex(cmap(row['normalized_log_duration']))\n",
        "        popup_content = f\"\"\"\n",
        "        <div style='font-family: \"Arial\", sans-serif; font-size: 16px; max-width:800px;'>\n",
        "        <ul>\n",
        "        <li><b>Well Name:</b> {row['well_name']}</li>\n",
        "        <li><b>Time to MCL:</b> {row[duration_column]} days ({row['duration_years']:.2f} Years)</li>\n",
        "        <li><b>Aquifer:</b> {row['aquifer']}</li>\n",
        "        </ul></div>\n",
        "        \"\"\"\n",
        "\n",
        "        folium.Circle(\n",
        "            radius=15,\n",
        "            location=coordinates,\n",
        "            popup=folium.Popup(popup_content, max_width=800),\n",
        "            fill_color=point_color,\n",
        "            color='#000000',\n",
        "            fill_opacity=1,\n",
        "            stroke=1,\n",
        "            weight=1\n",
        "        ).add_to(conc_fg)\n",
        "\n",
        "    m.add_child(conc_fg)\n",
        "\n",
        "    no_decline_df = df[df[duration_column].isnull() | (df[duration_column] <= 0)]\n",
        "\n",
        "    for _, row in no_decline_df.iterrows():\n",
        "        coordinates = [row['latitude'], row['longitude']]\n",
        "        popup_content = f\"\"\"\n",
        "        <div style='font-family: \"Arial\", sans-serif; font-size: 16px; max-width:800px;'>\n",
        "        <ul>\n",
        "        <li><b>Well Name:</b> {row['well_name']}</li>\n",
        "        <li><b>Aquifer:</b> {row['aquifer']}</li>\n",
        "        <li><b>No Decline Observed</b></li>\n",
        "        </ul></div>\n",
        "        \"\"\"\n",
        "\n",
        "        folium.Circle(\n",
        "            radius=15,\n",
        "            location=coordinates,\n",
        "            popup=folium.Popup(popup_content, max_width=800),\n",
        "            fill_color='black',\n",
        "            color='#000000',\n",
        "            fill_opacity=1,\n",
        "            stroke=1,\n",
        "            weight=1\n",
        "        ).add_to(no_decline_fg)\n",
        "\n",
        "    m.add_child(no_decline_fg)\n",
        "\n",
        "    min_val = valid_df[duration_column].min()\n",
        "    max_val = valid_df[duration_column].max()\n",
        "    min_val_years = min_val / 365.25\n",
        "    max_val_years = max_val / 365.25\n",
        "\n",
        "    colormap_original = folium.LinearColormap(colors=color_list, vmin=min_val_years, vmax=max_val_years, caption=f\"{duration_column} (years)\")\n",
        "    colormap_original.add_to(m)\n",
        "\n",
        "    return m"
      ],
      "metadata": {
        "id": "IX8IVgEy1Ycz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sr-90**"
      ],
      "metadata": {
        "id": "wLNFZRfVD37G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decreasing_well_list = data_analyzer.generate_decreasing_well_list(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='STRONTIUM-90',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "\n",
        "location_data = location_analyzer.extract_location_data(\n",
        "    well_name_column='station_id',\n",
        "    decreasing_well_list=decreasing_well_list\n",
        ")\n",
        "\n",
        "merged_df_sr_90 = pd.merge(analyte_summary_df_sr_90, location_data, left_on='well_name', right_on='station_id', how='inner')\n",
        "\n",
        "print(merged_df_sr_90)"
      ],
      "metadata": {
        "id": "6bxNSS5RP0Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_sr_90, 'total_decrease_duration')"
      ],
      "metadata": {
        "id": "XsMKCIXE0_ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_sr_90, 'total_decrease_duration', zoom_level_offset=11)"
      ],
      "metadata": {
        "id": "qVrTu3Q6122W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_sr_90, 'total_decrease_duration', satellite_mode=True)"
      ],
      "metadata": {
        "id": "EcCKzELXUzDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_sr_90, 'total_decrease_duration', aquifer='UAZ_UTRAU', zoom_level_offset=9)"
      ],
      "metadata": {
        "id": "WWYFpIuE-ke8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_sr_90, 'total_decrease_duration', aquifer='LAZ_UTRAU', zoom_level_offset=8)"
      ],
      "metadata": {
        "id": "OLuaepwW_VJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **I-129**"
      ],
      "metadata": {
        "id": "HO0hJkD8lBdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decreasing_well_list = data_analyzer.generate_decreasing_well_list(\n",
        "    analyte_name_column='ANALYTE_NAME',\n",
        "    analyte_name='IODINE-129',\n",
        "    well_name_column='STATION_ID',\n",
        "    MCL=10\n",
        ")\n",
        "\n",
        "location_data = location_analyzer.extract_location_data(\n",
        "    well_name_column='station_id',\n",
        "    decreasing_well_list=decreasing_well_list\n",
        ")\n",
        "\n",
        "merged_df_i_129 = pd.merge(analyte_summary_df_i_129, location_data, left_on='well_name', right_on='station_id', how='inner')\n",
        "\n",
        "print(merged_df_i_129)"
      ],
      "metadata": {
        "id": "Ry-5iwJZYqeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_i_129, 'total_decrease_duration', zoom_level_offset=9)"
      ],
      "metadata": {
        "id": "_4ArcEyYqwyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_i_129, 'total_decrease_duration', satellite_mode=True)"
      ],
      "metadata": {
        "id": "TOekTzsGq3dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_i_129, 'total_decrease_duration', aquifer='UAZ_UTRAU')"
      ],
      "metadata": {
        "id": "sEWOE5cDAEUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_map(merged_df_i_129, 'total_decrease_duration', aquifer='LAZ_UTRAU')"
      ],
      "metadata": {
        "id": "KAczlJkIAJsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compare maps**"
      ],
      "metadata": {
        "id": "kN9F_JQa12-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_aquifer_maps(df_sr_90, df_i_129, aquifers, duration_column, slope_column, p_value_column, zoom_level_offset=10, satellite_mode=False):\n",
        "    maps = {}\n",
        "    combined_df = pd.concat([df_sr_90, df_i_129])\n",
        "\n",
        "    min_latitude = combined_df['latitude'].min()\n",
        "    max_latitude = combined_df['latitude'].max()\n",
        "    min_longitude = combined_df['longitude'].min()\n",
        "    max_longitude = combined_df['longitude'].max()\n",
        "\n",
        "    def create_map_for_analyte(df, analyte, aquifer):\n",
        "        df_filtered = df[df['aquifer'] == aquifer]\n",
        "\n",
        "        if df_filtered.empty:\n",
        "            print(f\"No wells found for analyte: {analyte} and aquifer: {aquifer}\")\n",
        "            return None\n",
        "\n",
        "        center_latitude = (min_latitude + max_latitude) / 2\n",
        "        center_longitude = (min_longitude + max_longitude) / 2\n",
        "\n",
        "        max_delta = max(max_latitude - min_latitude, max_longitude - min_longitude)\n",
        "        zoom_level = zoom_level_offset - int(np.log2(max_delta))  # Use the provided offset\n",
        "\n",
        "        if satellite_mode:\n",
        "            m = folium.Map(location=[center_latitude, center_longitude], zoom_start=zoom_level, tiles=None)\n",
        "            folium.TileLayer(\n",
        "                tiles='https://{s}.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
        "                attr='Google',\n",
        "                name='Google Satellite',\n",
        "                max_zoom=20,\n",
        "                subdomains=['mt0', 'mt1', 'mt2', 'mt3']\n",
        "            ).add_to(m)\n",
        "        else:\n",
        "            m = folium.Map(location=[center_latitude, center_longitude], zoom_start=zoom_level)\n",
        "\n",
        "        cmap = cm.get_cmap('jet', 6)  # 6 discrete colors\n",
        "        color_list = [mcolors.rgb2hex(cmap(i / 5)) for i in range(6)]  # Convert to hex colors\n",
        "\n",
        "        slope_min = df_filtered[slope_column].min()\n",
        "        slope_max = df_filtered[slope_column].max()\n",
        "\n",
        "        df_filtered['normalized_slope'] = (df_filtered[slope_column] - slope_min) / (slope_max - slope_min)\n",
        "\n",
        "        for _, row in df_filtered.iterrows():\n",
        "            coordinates = [row['latitude'], row['longitude']]\n",
        "\n",
        "            if row[p_value_column] <= 0.05:\n",
        "                point_color = mcolors.rgb2hex(cmap(row['normalized_slope']))\n",
        "            else:\n",
        "                point_color = \"#000000\"  # Black for wells with no significant trend\n",
        "\n",
        "            popup_content = f\"\"\"\n",
        "            <div style='font-family: \"Arial\", sans-serif; font-size: 16px; max-width:800px;'>\n",
        "            <ul>\n",
        "            <li><b>Well Name:</b> {row['well_name']}</li>\n",
        "            <li><b>Slope:</b> {row[slope_column]:.4f}</li>\n",
        "            <li><b>P-value:</b> {row[p_value_column]:.4f}</li>\n",
        "            <li><b>Time to MCL:</b> {row[duration_column]} days</li>\n",
        "            <li><b>Aquifer:</b> {row['aquifer']}</li>\n",
        "            </ul></div>\n",
        "            \"\"\"\n",
        "\n",
        "            folium.Circle(\n",
        "                radius=15,  # You can adjust the radius if needed\n",
        "                location=coordinates,\n",
        "                popup=folium.Popup(popup_content, max_width=800),\n",
        "                fill_color=point_color,\n",
        "                color='#000000',\n",
        "                fill_opacity=1,\n",
        "                stroke=True,\n",
        "                weight=1\n",
        "            ).add_to(m)\n",
        "\n",
        "        colormap = folium.LinearColormap(colors=color_list, vmin=slope_min, vmax=slope_max, caption='Slope')\n",
        "        colormap.add_to(m)\n",
        "\n",
        "        return m\n",
        "\n",
        "    for aquifer in aquifers:\n",
        "        sr_90_map = create_map_for_analyte(df_sr_90, 'Sr-90', aquifer)\n",
        "        if sr_90_map:\n",
        "            maps[f\"Sr-90_{aquifer}\"] = sr_90_map\n",
        "\n",
        "        i_129_map = create_map_for_analyte(df_i_129, 'I-129', aquifer)\n",
        "        if i_129_map:\n",
        "            maps[f\"I-129_{aquifer}\"] = i_129_map\n",
        "\n",
        "    return maps"
      ],
      "metadata": {
        "id": "Be7oUZmN148V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aquifers = ['UAZ_UTRAU', 'LAZ_UTRAU']\n",
        "maps = generate_aquifer_maps(merged_df_i_129, merged_df_i_129, aquifers, 'total_decrease_duration', 'slope', 'p_value')"
      ],
      "metadata": {
        "id": "ppfbxG4g2V4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maps['Sr-90_UAZ_UTRAU']"
      ],
      "metadata": {
        "id": "wyqyXyYk2-Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maps['Sr-90_LAZ_UTRAU']"
      ],
      "metadata": {
        "id": "n21IKKbl3T-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maps['I-129_UAZ_UTRAU']"
      ],
      "metadata": {
        "id": "Z1-8ZHrx37Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maps['I-129_LAZ_UTRAU']"
      ],
      "metadata": {
        "id": "Lgg5MFrW3__5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "39aaf84606e21579959f93fb3c45e9e3d37c51d84ce27b2546e2ee0530e04c07"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}